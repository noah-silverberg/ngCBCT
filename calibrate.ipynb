{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52765262",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "This initial block sets up the environment, defines file paths, and specifies the validation scans and model versions to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.paths import Directories, Files\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "if torch.cuda.is_available():\n",
    "    CUDA_DEVICE = torch.device(\"cuda:0\")\n",
    "    print(f\"CUDA is available. Using device: {CUDA_DEVICE}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your PyTorch installation. Using CPU instead.\")\n",
    "    CUDA_DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "PHASE = '7'\n",
    "DATA_VERSION = '13'\n",
    "\n",
    "\n",
    "# Base directory\n",
    "WORK_ROOT = \"D:/NoahSilverberg/ngCBCT\"\n",
    "\n",
    "# NSG_CBCT Path where the raw matlab data is stored\n",
    "NSG_CBCT_PATH = \"D:/MitchellYu/NSG_CBCT\"\n",
    "\n",
    "# Directory with all files specific to this phase/data version\n",
    "PHASE_DATAVER_DIR = os.path.join(\n",
    "    WORK_ROOT, f\"phase{PHASE}\", f\"DS{DATA_VERSION}\"\n",
    ")\n",
    "\n",
    "DIRECTORIES = Directories(\n",
    "    # mat_projections_dir=os.path.join(NSG_CBCT_PATH, \"data/prj/HF/mat\"),\n",
    "    # pt_projections_dir=os.path.join(WORK_ROOT, \"prj_pt\"),\n",
    "    # projections_aggregate_dir=os.path.join(PHASE_DATAVER_DIR, \"aggregates\", \"projections\"),\n",
    "    # projections_model_dir=os.path.join(PHASE_DATAVER_DIR, \"models\", \"projections\"),\n",
    "    # projections_results_dir=os.path.join(PHASE_DATAVER_DIR, \"results\", \"projections\"),\n",
    "    # projections_gated_dir=os.path.join(WORK_ROOT, \"gated\", \"prj_mat\"),\n",
    "    reconstructions_dir=os.path.join(PHASE_DATAVER_DIR, \"reconstructions\"),\n",
    "    reconstructions_gated_dir=os.path.join(WORK_ROOT, \"gated\", \"fdk_recon\"),\n",
    "    # reconstructions_dir=os.path.join(\"H:\\\\\", \"Public\", \"Noah\", \"reconstructions\"),\n",
    "    # reconstructions_gated_dir=os.path.join(\"H:\\\\\", \"Public\", \"Noah\", \"gated\", \"fdk_recon\"),\n",
    "    # images_aggregate_dir=os.path.join(PHASE_DATAVER_DIR, \"aggregates\", \"images\"),\n",
    "    # images_model_dir=os.path.join(PHASE_DATAVER_DIR, \"models\", \"images\"),\n",
    "    # images_results_dir=os.path.join(PHASE_DATAVER_DIR, \"results\", \"images\"),\n",
    ")\n",
    "\n",
    "FILES = Files(DIRECTORIES)\n",
    "\n",
    "# VAL_SCANS = [('02', '01'), ('02', '02'), ('16', '01'), ('16', '02'), ('22', '01'), ('22', '02')]\n",
    "VAL_SCANS = [('08', '01'), ('10', '01'), ('14', '01'), ('14', '02'), ('15', '01'), ('20', '01')]\n",
    "\n",
    "SCAN_TYPE = 'HF'\n",
    "MODEL_VERSIONS = ['MK7_MCDROPOUT']\n",
    "PASSTHROUGH_COUNT = 50 # use None for multiple models -- only use this for one model for MC dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82802fe6",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "This block gathers the file paths for the model's reconstructions (50 passes for MC Dropout) and the corresponding ground truth reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4132e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_paths_dict = {}\n",
    "recon_names_dict = {}\n",
    "gt_paths_dict = {}\n",
    "\n",
    "for patient, scan in VAL_SCANS:\n",
    "    print(f\"Processing patient {patient}, scan {scan}\")\n",
    "    recon_paths = []\n",
    "    recon_names = []\n",
    "    if PASSTHROUGH_COUNT is None:\n",
    "        for model_version in MODEL_VERSIONS:\n",
    "            recon_path = FILES.get_recon_filepath(model_version, patient, scan, SCAN_TYPE, gated=False)\n",
    "            recon_paths.append(recon_path)\n",
    "            recon_names.append(model_version)\n",
    "    else:\n",
    "        for i in range(PASSTHROUGH_COUNT):\n",
    "            recon_path = FILES.get_recon_filepath(MODEL_VERSIONS[0], patient, scan, SCAN_TYPE, gated=False, passthrough_num=i)\n",
    "            recon_paths.append(recon_path)\n",
    "            recon_names.append(f\"Passthrough {i+1}\")\n",
    "\n",
    "    gt_path = FILES.get_recon_filepath('fdk', patient, scan, SCAN_TYPE, gated=True)\n",
    "    gt_paths_dict[(patient, scan)] = gt_path\n",
    "\n",
    "    recon_paths_dict[(patient, scan)] = recon_paths\n",
    "    recon_names_dict[(patient, scan)] = recon_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a6ce9",
   "metadata": {},
   "source": [
    "## Analysis Loop\n",
    "The main loop iterates through each scan. For each scan, it loads all 50 reconstruction passes, calculates the mean (prediction) and standard deviation (uncertainty), and then performs several analyses to evaluate the quality of the uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now go through each patient and scan, load the reconstructions\n",
    "# calclate the mean and std (pixel-wise) and the error from GT\n",
    "for (patient, scan), recon_paths in recon_paths_dict.items():\n",
    "    print(f\"\\nProcessing reconstructions for patient {patient}, scan {scan}\")\n",
    "    \n",
    "    # Load ground truth\n",
    "    gt_path = gt_paths_dict[(patient, scan)]\n",
    "    gt_recon = torch.load(gt_path).cpu().numpy()\n",
    "    \n",
    "    # Initialize lists to hold reconstructions\n",
    "    reconstructions = []\n",
    "    \n",
    "    for recon_path in recon_paths:\n",
    "        recon = torch.load(recon_path).cpu().numpy()\n",
    "        reconstructions.append(recon)\n",
    "\n",
    "    print(f\"Loaded {len(reconstructions)} reconstructions for patient {patient}, scan {scan}\")\n",
    "    \n",
    "    # Convert to numpy array for easier manipulation\n",
    "    reconstructions = np.array(reconstructions)\n",
    "    \n",
    "    # --- Basic Calculations ---\n",
    "    # Calculate mean and std across the first axis (across models or passthroughs)\n",
    "    mean_recon = np.mean(reconstructions, axis=0)\n",
    "    std_recon = np.std(reconstructions, axis=0)\n",
    "    \n",
    "    # Calculate error from ground truth\n",
    "    error = mean_recon - gt_recon\n",
    "    \n",
    "    # To avoid memory issues, let's work with flattened arrays from here on\n",
    "    # and select a random subset of the data for some plots to keep them readable.\n",
    "    # We'll also only consider voxels inside a radius of 225px from the center (in the last 2 dims)\n",
    "    # (note scans are 200x512x512)\n",
    "    # Efficiently create a circular mask in the last two dimensions\n",
    "    z, y, x = gt_recon.shape\n",
    "    yy, xx = np.ogrid[:y, :x]\n",
    "    center = np.array([y // 2, x // 2])\n",
    "    radius = 225\n",
    "    dist_from_center = np.sqrt((yy - center[0])**2 + (xx - center[1])**2)\n",
    "    slice_mask = dist_from_center <= radius\n",
    "    mask = np.broadcast_to(slice_mask, gt_recon.shape)\n",
    "    \n",
    "    flat_std = std_recon[mask].flatten()\n",
    "    flat_error = error[mask].flatten()\n",
    "    flat_abs_error = np.abs(flat_error)\n",
    "    flat_squared_error = flat_error**2\n",
    "\n",
    "    # Clean up memory\n",
    "    del reconstructions, mean_recon, std_recon, error, gt_recon\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Analysis 1: Standardized Error Histogram (Your Original Analysis) ---\n",
    "    standardized_error = flat_error / (flat_std + 1e-8)  # Adding a small value to avoid division by zero\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(standardized_error, bins=100, color='gray', alpha=0.7, range=(-5, 5))\n",
    "    plt.title(f'Standardized Error Histogram for Patient {patient}, Scan {scan}')\n",
    "    plt.xlabel('Standardized Error (Error / Predicted Std Dev)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate and print percent with abs standardized error < 1, < 2, < 3\n",
    "    percent_within_1 = np.mean(np.abs(standardized_error) < 1) * 100\n",
    "    percent_within_2 = np.mean(np.abs(standardized_error) < 2) * 100\n",
    "    percent_within_3 = np.mean(np.abs(standardized_error) < 3) * 100\n",
    "    print(f\"Patient {patient}, Scan {scan} - Coverage Statistics:\")\n",
    "    print(f\"  Percent with abs standardized error < 1: {percent_within_1:.2f}% (Expected for Gaussian: ~68%)\")\n",
    "    print(f\"  Percent with abs standardized error < 2: {percent_within_2:.2f}% (Expected for Gaussian: ~95%)\")\n",
    "    print(f\"  Percent with abs standardized error < 3: {percent_within_3:.2f}% (Expected for Gaussian: ~99.7%)\")\n",
    "\n",
    "    # --- Analysis 2: Predicted Uncertainty vs. Actual Error ---\n",
    "    # This plot directly visualizes the correlation. For a good uncertainty estimate,\n",
    "    # we expect to see that as the predicted standard deviation increases (x-axis),\n",
    "    # the absolute error also tends to increase (y-axis).\n",
    "    # A 2D histogram is used because a scatter plot would be too dense.\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.hist2d(flat_std, flat_abs_error, bins=50, cmap='inferno')\n",
    "    plt.colorbar(label='Voxel Count')\n",
    "    plt.title(f'Predicted Uncertainty vs. Actual Error for Patient {patient}, Scan {scan}')\n",
    "    plt.xlabel('Predicted Standard Deviation (Uncertainty)')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Analysis 3: Calibration Plot (Reliability Diagram) ---\n",
    "    # This plot assesses how well the *magnitude* of the predicted uncertainty corresponds\n",
    "    # to the *magnitude* of the actual error. Voxels are binned by their predicted uncertainty.\n",
    "    # For each bin, we plot the average predicted uncertainty against the actual error (RMSE).\n",
    "    # A perfectly calibrated model would fall on the y=x line.\n",
    "    num_bins = 20\n",
    "    bin_limits = np.linspace(np.min(flat_std), np.max(flat_std), num_bins + 1)\n",
    "    rmse_per_bin = np.zeros(num_bins)\n",
    "    mean_std_per_bin = np.zeros(num_bins)\n",
    "    \n",
    "    for i in range(num_bins):\n",
    "        lower_bound = bin_limits[i]\n",
    "        upper_bound = bin_limits[i+1]\n",
    "        mask_bin = (flat_std >= lower_bound) & (flat_std < upper_bound)\n",
    "        if np.sum(mask_bin) > 0:\n",
    "            rmse_per_bin[i] = np.sqrt(np.mean(flat_squared_error[mask_bin]))\n",
    "            mean_std_per_bin[i] = np.mean(flat_std[mask_bin])\n",
    "    \n",
    "    # Filter out empty bins\n",
    "    valid_bins = mean_std_per_bin > 0\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(mean_std_per_bin[valid_bins], rmse_per_bin[valid_bins], 'o-', label='Model Calibration', color='royalblue')\n",
    "    # Plot the ideal y=x line for perfect calibration\n",
    "    lims = [min(np.min(mean_std_per_bin[valid_bins]), np.min(rmse_per_bin[valid_bins])), max(np.max(mean_std_per_bin[valid_bins]), np.max(rmse_per_bin[valid_bins]))]\n",
    "    plt.plot(lims, lims, 'k--', label='Perfect Calibration (y=x)')\n",
    "    plt.title(f'Calibration Plot for Patient {patient}, Scan {scan}')\n",
    "    plt.xlabel('Average Predicted Standard Deviation (per bin)')\n",
    "    plt.ylabel('Root Mean Squared Error (per bin)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Analysis 4: Sparsification Plot ---\n",
    "    # This plot shows the utility of the uncertainty. It answers: \"If I remove the most\n",
    "    # uncertain voxels, how quickly does my overall error decrease?\" A good uncertainty\n",
    "    # estimate will identify the voxels with the highest error first, causing the MSE curve\n",
    "    # to drop rapidly.\n",
    "    sorted_indices = np.argsort(flat_std)\n",
    "    sorted_squared_error = flat_squared_error[sorted_indices]\n",
    "    \n",
    "    fractions_removed = np.linspace(0, 0.5, 51) # Remove up to 50% of voxels\n",
    "    mse_remaining = np.zeros_like(fractions_removed)\n",
    "    \n",
    "    for i, frac in enumerate(fractions_removed):\n",
    "        num_to_keep = int((1 - frac) * len(sorted_squared_error))\n",
    "        # We remove the *most* uncertain, which are at the end of the sorted list\n",
    "        mse_remaining[i] = np.mean(sorted_squared_error[:num_to_keep])\n",
    "        \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fractions_removed * 100, mse_remaining, 'o-', color='crimson')\n",
    "    plt.title(f'Sparsification Plot for Patient {patient}, Scan {scan}')\n",
    "    plt.xlabel('Percent of Most Uncertain Voxels Removed (%)')\n",
    "    plt.ylabel('Mean Squared Error (on remaining voxels)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "    \n",
    "    # Clean up memory before next loop iteration\n",
    "    del flat_std, flat_error, flat_abs_error, flat_squared_error, standardized_error\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
