{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85cca8e",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7075ce10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:DEBUG mode is enabled. Detailed logs will be shown.\n"
     ]
    }
   ],
   "source": [
    "from pipeline.config import CUDA_DEVICE, DEBUG, SCANS, DATA_DIR, RESULT_DIR, data_version, PD_training_app, PD_epochs, PD_network_name, PD_model_name, PD_batch_size, PD_optimizer, PD_num_workers, ID_training_app, ID_epochs, ID_network_name, ID_model_name, ID_batch_size, ID_optimizer, ID_num_workers\n",
    "from pipeline.proj import load_projection_mat, reformat_sinogram, interpolate_projections, pad_and_reshape, divide_sinogram\n",
    "from pipeline.aggregate_prj import aggregate_saved_projections\n",
    "# from .aggregate_ct import aggregate_saved_volumes\n",
    "from pipeline.launcher import run_app\n",
    "from pipeline.apply_model import apply_model_to_projections, load_model\n",
    "# from .infer3d import inference_3d\n",
    "from pipeline.utils import ensure_dir\n",
    "import torch\n",
    "import scipy.io\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Show info messages if DEBUG mode is enabled\n",
    "if DEBUG:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logging.info(\"DEBUG mode is enabled. Detailed logs will be shown.\")\n",
    "else:\n",
    "    logging.basicConfig(level=logging.WARNING)\n",
    "    logging.warning(\"DEBUG mode is disabled. Only warnings and errors will be shown.\")\n",
    "\n",
    "# TODO run FDK via: FFrecon_reconFDK(input_mat, output_mat); in file \"FFrecon_fullFDK.m\"\n",
    "# TODO add DEBUG mode that prints out helpful info like shapes, etc. (and be sure to set logging level so 'info' shows up too)\n",
    "# TODO add some kind of logging for the hyperparameters used in the each run\n",
    "# TODO go through the training code and make sure it is consistent with new pipeline\n",
    "# TODO save everything as numpy arrays instead of torch tensors, and then convert to torch tensors when needed\n",
    "# TODO add more options for training like learning rate, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b49a2c",
   "metadata": {},
   "source": [
    "# 1. Data Preparation: projection interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccc147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing patient 01, scan 01, type HF, sample TRAIN\n",
      "INFO:root:Loaded odd_index shape: (440,)\n",
      "INFO:root:Loaded angles shape: torch.Size([908])\n",
      "INFO:root:Loaded projection shape: torch.Size([510, 382, 908])\n",
      "INFO:root:Reformatted projection shape: torch.Size([908, 382, 510])\n",
      "INFO:root:Interpolated ngCBCT projection shape: torch.Size([908, 382, 510])\n",
      "INFO:root:Combined gCBCT shape: torch.Size([764, 1, 512, 512])\n",
      "INFO:root:Combined ngCBCT shape: torch.Size([764, 1, 512, 512])\n",
      "INFO:root:Saving projections...\n",
      "INFO:root:Done with patient 01, scan 01, type HF, sample TRAIN\n",
      "\n",
      "INFO:root:Processing patient 02, scan 01, type FF, sample TRAIN\n",
      "INFO:root:Loaded odd_index shape: (259,)\n",
      "INFO:root:Loaded angles shape: torch.Size([496])\n",
      "INFO:root:Loaded projection shape: torch.Size([510, 382, 496])\n",
      "INFO:root:Reformatted projection shape: torch.Size([496, 382, 510])\n",
      "INFO:root:Interpolated ngCBCT projection shape: torch.Size([496, 382, 510])\n",
      "INFO:root:Combined gCBCT shape: torch.Size([764, 1, 256, 512])\n",
      "INFO:root:Combined ngCBCT shape: torch.Size([764, 1, 256, 512])\n",
      "INFO:root:Saving projections...\n",
      "INFO:root:Done with patient 02, scan 01, type FF, sample TRAIN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for patient, scan, scan_type, sample in SCANS:\n",
    "    # Load the projection data from the matlab files\n",
    "    odd_index, angles, prj = load_projection_mat(patient, scan, scan_type)\n",
    "\n",
    "    # Log shapes of loaded data\n",
    "    logging.info(f'Processing patient {patient}, scan {scan}, type {scan_type}, sample {sample}')\n",
    "    logging.info(f'Loaded odd_index shape: {odd_index.shape}')\n",
    "    logging.info(f'Loaded angles shape: {angles.shape}')\n",
    "    logging.info(f'Loaded projection shape: {prj.shape}')\n",
    "\n",
    "    # Flip and permute to get it in the right format\n",
    "    prj_gcbct, angles1 = reformat_sinogram(prj, angles)\n",
    "\n",
    "    # Log shapes after reformatting\n",
    "    logging.info(f'Reformatted projection shape: {prj_gcbct.shape}')\n",
    "\n",
    "    # Simulate ngCBCT projections\n",
    "    prj_ngcbct_li = interpolate_projections(prj_gcbct, odd_index)\n",
    "\n",
    "    # Log shapes after interpolation\n",
    "    logging.info(f'Interpolated ngCBCT projection shape: {prj_ngcbct_li.shape}')\n",
    "\n",
    "    # Split the projections into two halves so they are good dimensions for the CNN\n",
    "    combined_gcbct = divide_sinogram(pad_and_reshape(prj_gcbct), v_dim=512 if scan_type == \"HF\" else 256)\n",
    "    combined_ngcbct = divide_sinogram(pad_and_reshape(prj_ngcbct_li), v_dim=512 if scan_type == \"HF\" else 256)\n",
    "\n",
    "    # Log shapes after dividing sinograms\n",
    "    logging.info(f'Combined gCBCT shape: {combined_gcbct.shape}')\n",
    "    logging.info(f'Combined ngCBCT shape: {combined_ngcbct.shape}')\n",
    "\n",
    "    # Ensure the output directories exist\n",
    "    g_dir = os.path.join(DATA_DIR, 'gated')\n",
    "    ng_dir = os.path.join(DATA_DIR, 'ng')\n",
    "    ensure_dir(g_dir)\n",
    "    ensure_dir(ng_dir)\n",
    "\n",
    "    logging.info(f'Saving projections...')\n",
    "    \n",
    "    # Save the projections\n",
    "    torch.save(combined_gcbct, os.path.join(g_dir, f'{scan_type}_p{patient}_{scan}_{sample}.pt')) # e.g., HF_p01_01_TRAIN.pt\n",
    "    torch.save(combined_ngcbct, os.path.join(ng_dir, f'{scan_type}_p{patient}_{scan}_{sample}.pt'))\n",
    "\n",
    "    logging.info(f'Done with patient {patient}, scan {scan}, type {scan_type}, sample {sample}\\n')\n",
    "\n",
    "logging.info(\"All projections saved successfully.\")\n",
    "logging.info(\"Gated projections saved in: %s\", g_dir)\n",
    "logging.info(\"Non-gated projections saved in: %s\", ng_dir)\n",
    "\n",
    "# Free up memory\n",
    "del odd_index, angles, prj, prj_gcbct, angles1, prj_ngcbct_li, combined_gcbct, combined_ngcbct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e695f",
   "metadata": {},
   "source": [
    "# 2. Aggregate projections for train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188d3403",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scan_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFF\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAIN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVALIDATION\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEST\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m----> 8\u001b[0m         prj_gcbct, prj_ngcbct \u001b[38;5;241m=\u001b[39m \u001b[43maggregate_saved_projections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscan_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(prj_gcbct, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(proj_agg_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscan_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_gated.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;66;03m# e.g., HF_TRAIN_gated.pt\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(prj_ngcbct, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(proj_agg_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscan_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ng.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/MSK-Research/ngCBCT/pipeline/aggregate_prj.py:29\u001b[0m, in \u001b[0;36maggregate_saved_projections\u001b[0;34m(scan_type, sample)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# TODO we need some way to choose which datasets we actually want...\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Create ground truth dataset, and concatenate all scans into one tensor\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# along the H dimension (i.e., the dimension where we already stacked them before saving -- see \"divide_sinogram\" in proj.py)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m truth_set \u001b[38;5;241m=\u001b[39m PrjSet(g_dir)\n\u001b[0;32m---> 29\u001b[0m prj_gcbct \u001b[38;5;241m=\u001b[39m \u001b[43mtruth_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(truth_set)):\n\u001b[1;32m     31\u001b[0m     prj_gcbct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((prj_gcbct, truth_set[idx]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/MSK-Research/ngCBCT/pipeline/dsets.py:252\u001b[0m, in \u001b[0;36mPrjSet.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 252\u001b[0m     train_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_images\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Directory for aggregated data saving\n",
    "proj_agg_dir = os.path.join(DATA_DIR, \"agg\", \"projections\")\n",
    "ensure_dir(proj_agg_dir)\n",
    "\n",
    "# Aggregate and save projection data sets\n",
    "for scan_type in ['HF', 'FF']:\n",
    "    for sample in ['TRAIN', 'VALIDATION', 'TEST']:\n",
    "        prj_gcbct, prj_ngcbct = aggregate_saved_projections(scan_type, sample)\n",
    "        torch.save(prj_gcbct, os.path.join(proj_agg_dir, f\"{scan_type}_{sample}_gated.pt\")) # e.g., HF_TRAIN_gated.pt\n",
    "        torch.save(prj_ngcbct, os.path.join(proj_agg_dir, f\"{scan_type}_{sample}_ng.pt\"))\n",
    "\n",
    "# Free up memory\n",
    "del prj_gcbct, prj_ngcbct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae3e79",
   "metadata": {},
   "source": [
    "# 3. Training PD CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daead9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_app(PD_training_app, [f'--epoch={PD_epochs}', f'--network={PD_network_name}', f'--model_name={PD_model_name}', f'--data_ver={data_version}', f'--optimizer={PD_optimizer}', '--shuffle=True', f'--DEBUG={DEBUG}', f'--batch_size={PD_batch_size}'], f'--num_workers={PD_num_workers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef2c2c",
   "metadata": {},
   "source": [
    "# 4. Apply PD model to all nonstop-gated sinograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28452019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained PD model onto the GPU\n",
    "PD_model = load_model(PD_network_name, PD_model_name, device=torch.device(CUDA_DEVICE))\n",
    "\n",
    "for patient, scan, scan_type, sample in SCANS:\n",
    "    # Get the matlab dicts for the ground truth and CNN projections\n",
    "    g_mat, cnn_mat = apply_model_to_projections(patient, scan, scan_type, sample, PD_model)\n",
    "\n",
    "    # Save the ground truth and CNN projections\n",
    "    scipy.io.savemat(os.path.join(RESULT_DIR, f'{scan_type}_p{patient}_{scan}_{sample}_gated.mat'), g_mat) # e.g., HF_p01_01_TRAIN_gated.mat\n",
    "    scipy.io.savemat(os.path.join(RESULT_DIR, f'{scan_type}_p{patient}_{scan}_{sample}_ng.mat'), cnn_mat)\n",
    "\n",
    "# Free up memory\n",
    "del PD_model, g_mat, cnn_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feaa971",
   "metadata": {},
   "source": [
    "# 5. TODO: FDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4029008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09b672",
   "metadata": {},
   "source": [
    "# 6. Aggregate CT volumes for train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for aggregated data saving\n",
    "vol_agg_dir = os.path.join(DATA_DIR, \"agg\", \"volumes\")\n",
    "ensure_dir(vol_agg_dir)\n",
    "\n",
    "# Aggregate and save volume data sets\n",
    "for scan_type in ['HF', 'FF']:\n",
    "    for sample in ['train', 'validation', 'test']:\n",
    "        vol_gcbct, vol_ngcbct = aggregate_saved_volumes(scan_type, sample)\n",
    "        torch.save(vol_gcbct, os.path.join(vol_agg_dir, f\"{scan_type}_{sample}_gated.pt\"))\n",
    "        torch.save(vol_ngcbct, os.path.join(vol_agg_dir, f\"{scan_type}_{sample}_ng.pt\"))\n",
    "\n",
    "# Free up memory\n",
    "del vol_gcbct, vol_ngcbct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e933115",
   "metadata": {},
   "source": [
    "# 7. Train ID CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b35e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f74da1",
   "metadata": {},
   "source": [
    "# 8. Inference on test scans for full 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = inference_3d(patient_id, scan_id, 'HF', data_version, model_name, 'tumor_location_panc.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
