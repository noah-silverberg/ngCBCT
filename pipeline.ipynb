{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4229da",
   "metadata": {},
   "source": [
    "# Setup Logging & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f3a1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a15cef",
   "metadata": {},
   "source": [
    "### Setting up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8c4bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pipeline:DEBUG mode is enabled. Detailed logs will be shown.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"pipeline\")\n",
    "\n",
    "# Show info messages if DEBUG mode is enabled\n",
    "if DEBUG:\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.debug(\"DEBUG mode is enabled. Detailed logs will be shown.\")\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.info(\"DEBUG mode is disabled. Only essential logs will be shown.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0476a6",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd091fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# We set up CUDA first to ensure it is configured correctly\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "CUDA_DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7075ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.proj import load_projection_mat, reformat_sinogram, interpolate_projections, pad_and_reshape, divide_sinogram\n",
    "from pipeline.aggregate_prj import aggregate_saved_projections\n",
    "# from .aggregate_ct import aggregate_saved_volumes\n",
    "from pipeline.apply_model import apply_model_to_projections, load_model\n",
    "# from .infer3d import inference_3d\n",
    "from pipeline.utils import ensure_dir, read_scans_agg_file\n",
    "import torch\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yaml\n",
    "import importlib\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    logger.error(\"CUDA is not available. Please check your PyTorch installation. Using CPU instead...this will be slow.\")\n",
    "    CUDA_DEVICE = \"cpu\"\n",
    "\n",
    "# TODO run FDK via: FFrecon_reconFDK(input_mat, output_mat); in file \"FFrecon_fullFDK.m\"\n",
    "# TODO add input verification?\n",
    "# TODO add a way for the user to add more data (since currently if there is any, it will skip the processing entirely)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d9f9e",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18df6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scans to convert to PyTorch tensors\n",
    "# Put None if you don't have any scans to convert\n",
    "# See the README for how to write this file correctly\n",
    "# NOTE: This will throw an error if the scan has already been converted\n",
    "#       If you would like to re-convert a scan,\n",
    "#       you can delete the file manually\n",
    "# SCANS_CONVERT = 'scans_convert_to_pt.txt'\n",
    "SCANS_CONVERT = None\n",
    "\n",
    "# Phase of the project (all data, models, etc. will be saved under this phase)\n",
    "PHASE = \"7\"\n",
    "\n",
    "# If this data version already exists in this phase, it will be loaded\n",
    "# Otherwise it will be created using whatever the most updated data creation script is\n",
    "DATA_VERSION = '13'\n",
    "\n",
    "# Scans to use for training, val, and testing\n",
    "# Set this to None if you don't want to do any aggregation\n",
    "# See the README for how to write this file correctly\n",
    "# NOTE: This will throw an error if there are already aggregated scans\n",
    "#       (even if they are not the same as the ones in this file)\n",
    "#       If you would like to re-aggregate,\n",
    "#       you can delete the file manually or change the data version\n",
    "# SCANS_AGG = 'scans_to_agg.txt'\n",
    "SCANS_AGG = None\n",
    "\n",
    "# List of yaml files that contain configurations for the pipeline\n",
    "# Each file should contain the paramters for a specific model/ensemble\n",
    "CONFIG_FILES = [\n",
    "    # \"config_01epoch.yaml\",\n",
    "    \"config_20epoch.yaml\",\n",
    "]\n",
    "\n",
    "# Base directory\n",
    "WORK_ROOT = \"D:/NoahSilverberg/ngCBCT\"\n",
    "\n",
    "# NSG_CBCT Path where the raw matlab data is stored\n",
    "NSG_CBCT_PATH = \"D:/MitchellYu/NSG_CBCT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa6e84",
   "metadata": {},
   "source": [
    "### Some immediate variable definitions and setting changes based on the configuration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38798306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pipeline:All directories are set up successfully.\n"
     ]
    }
   ],
   "source": [
    "# Directories derived from bases\n",
    "PHASE_DATAVER_DIR = os.path.join(\n",
    "    WORK_ROOT, f\"phase{PHASE}\", f\"DS{DATA_VERSION}\"\n",
    ")  # everything should go inside this directory\n",
    "MODEL_DIR = os.path.join(PHASE_DATAVER_DIR, \"model\")  # for trained models\n",
    "RESULT_DIR = os.path.join(PHASE_DATAVER_DIR, \"result\")  # for outputs of CNN\n",
    "AGG_DIR = os.path.join(\n",
    "    PHASE_DATAVER_DIR, \"agg\"\n",
    ")  # for aggregated data (for PD and ID training)\n",
    "\n",
    "# Make the folders if they don't already exist\n",
    "ensure_dir(PHASE_DATAVER_DIR)\n",
    "ensure_dir(MODEL_DIR)\n",
    "ensure_dir(RESULT_DIR)\n",
    "ensure_dir(AGG_DIR)\n",
    "\n",
    "logger.debug(\"All directories are set up successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b49a2c",
   "metadata": {},
   "source": [
    "# Data Preparation: projection interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ccc147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:No scans to convert. Skipping projection data processing.\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output directories exist\n",
    "g_dir = os.path.join(WORK_ROOT, 'data_pt', 'prj', 'gated')\n",
    "ng_dir = os.path.join(WORK_ROOT, 'data_pt', 'prj', 'ng')\n",
    "ensure_dir(g_dir)\n",
    "ensure_dir(ng_dir)\n",
    "\n",
    "if SCANS_CONVERT is not None:\n",
    "    # Read the scans to convert file\n",
    "    with open(SCANS_CONVERT, \"r\") as f:\n",
    "        SCANS_CONVERT = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            patient, scan, scan_type = line.split()\n",
    "            SCANS_CONVERT.append((patient, scan, scan_type))\n",
    "\n",
    "    logger.debug(f\"Loaded scan list for conversion: {SCANS_CONVERT}\")\n",
    "\n",
    "    logger.info(\"Starting to process projection data...\")\n",
    "\n",
    "    for patient, scan, scan_type in SCANS_CONVERT:\n",
    "        # Load the projection data from the matlab files\n",
    "        odd_index, angles, prj = load_projection_mat(patient, scan, scan_type, NSG_CBCT_PATH)\n",
    "\n",
    "        # Log shapes of loaded data\n",
    "        logger.debug(f'Processing patient {patient}, scan {scan}, type {scan_type}')\n",
    "        logger.debug(f'Loaded odd_index shape: {odd_index.shape}')\n",
    "        logger.debug(f'Loaded angles shape: {angles.shape}')\n",
    "        logger.debug(f'Loaded projection shape: {prj.shape}')\n",
    "\n",
    "        # Flip and permute to get it in the right format\n",
    "        prj_gcbct, angles1 = reformat_sinogram(prj, angles)\n",
    "\n",
    "        # Log shapes after reformatting\n",
    "        logger.debug(f'Reformatted projection shape: {prj_gcbct.shape}')\n",
    "\n",
    "        # Simulate ngCBCT projections\n",
    "        prj_ngcbct_li = interpolate_projections(prj_gcbct, odd_index)\n",
    "\n",
    "        # Log shapes after interpolation\n",
    "        logger.debug(f'Interpolated ngCBCT projection shape: {prj_ngcbct_li.shape}')\n",
    "\n",
    "        # Split the projections into two halves so they are good dimensions for the CNN\n",
    "        combined_gcbct = divide_sinogram(pad_and_reshape(prj_gcbct), v_dim=512 if scan_type == \"HF\" else 256)\n",
    "        combined_ngcbct = divide_sinogram(pad_and_reshape(prj_ngcbct_li), v_dim=512 if scan_type == \"HF\" else 256)\n",
    "\n",
    "        # Log shapes after dividing sinograms\n",
    "        logger.debug(f'Combined gCBCT shape: {combined_gcbct.shape}')\n",
    "        logger.debug(f'Combined ngCBCT shape: {combined_ngcbct.shape}')\n",
    "\n",
    "        logger.debug(f'Saving projections...')\n",
    "        \n",
    "        # NOTE: These need to have the same name since later we will aggregate them, and we just sort by the name\n",
    "        g_path = os.path.join(g_dir, f'{scan_type}_p{patient}_{scan}.pt')\n",
    "        ng_path = os.path.join(ng_dir, f'{scan_type}_p{patient}_{scan}.pt')\n",
    "\n",
    "        # Make sure the files do not already exist\n",
    "        if os.path.exists(g_path) or os.path.exists(ng_path):\n",
    "            raise FileExistsError(f\"Projection files already exist for patient {patient}, scan {scan}, type {scan_type}. Please delete them before re-running.\")\n",
    "        \n",
    "        # Save the projections\n",
    "        torch.save(combined_gcbct, g_path) # e.g., HF_p01_01.pt\n",
    "        torch.save(combined_ngcbct, ng_path)\n",
    "\n",
    "        logger.debug(f'Done with patient {patient}, scan {scan}, type {scan_type}\\n')\n",
    "\n",
    "    logger.info(\"All projections saved successfully.\")\n",
    "    logger.info(\"Gated projections saved in: %s\", g_dir)\n",
    "    logger.info(\"Nonstop-gated projections saved in: %s\", ng_dir)\n",
    "\n",
    "    # Free up memory\n",
    "    del odd_index, angles, prj, prj_gcbct, angles1, prj_ngcbct_li, combined_gcbct, combined_ngcbct\n",
    "else:\n",
    "    logger.info(\"No scans to convert. Skipping projection data processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23247d",
   "metadata": {},
   "source": [
    "### DEBUG: Sample projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b21297",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG and SCANS_CONVERT is not None:\n",
    "    # Pick the first HF scan and first FF scan\n",
    "    hf_scan = None\n",
    "    ff_scan = None\n",
    "    for patient, scan, scan_type in SCANS_CONVERT:\n",
    "        if scan_type == \"HF\":\n",
    "            hf_scan = (patient, scan, scan_type)\n",
    "            break\n",
    "    for patient, scan, scan_type in SCANS_CONVERT:\n",
    "        if scan_type == \"FF\":\n",
    "            ff_scan = (patient, scan, scan_type)\n",
    "            break\n",
    "\n",
    "    # Display the first HF scan\n",
    "    # Show the gated and nonstop-gated on subplots\n",
    "    if hf_scan:\n",
    "        hf_patient, hf_scan_num, hf_scan_type = hf_scan\n",
    "        hf_gated_prj = torch.load(os.path.join(g_dir, f'{hf_scan_type}_p{hf_patient}_{hf_scan_num}.pt'))\n",
    "        hf_ng_prj = torch.load(os.path.join(ng_dir, f'{hf_scan_type}_p{hf_patient}_{hf_scan_num}.pt'))\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(hf_gated_prj[0, 0, :, :].cpu().numpy(), cmap='gray')\n",
    "        plt.title(f'Gated Projection - {hf_scan_type} p{hf_patient}_{hf_scan_num}')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(hf_ng_prj[0, 0, :, :].cpu().numpy(), cmap='gray')\n",
    "        plt.title(f'Nonstop-Gated Projection - {hf_scan_type} p{hf_patient}_{hf_scan_num}')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Free up memory\n",
    "        del hf_gated_prj, hf_ng_prj\n",
    "\n",
    "    # Repeat for FF scan\n",
    "    if ff_scan:\n",
    "        ff_patient, ff_scan_num, ff_scan_type = ff_scan\n",
    "        ff_gated_prj = torch.load(os.path.join(g_dir, f'{ff_scan_type}_p{ff_patient}_{ff_scan_num}.pt'))\n",
    "        ff_ng_prj = torch.load(os.path.join(ng_dir, f'{ff_scan_type}_p{ff_patient}_{ff_scan_num}.pt'))\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(ff_gated_prj[0, 0, :, :].cpu().numpy(), cmap='gray')\n",
    "        plt.title(f'Gated Projection - {ff_scan_type} p{ff_patient}_{ff_scan_num}')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(ff_ng_prj[0, 0, :, :].cpu().numpy(), cmap='gray')\n",
    "        plt.title(f'Nonstop-Gated Projection - {ff_scan_type} p{ff_patient}_{ff_scan_num}')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Free up memory\n",
    "        del ff_gated_prj, ff_ng_prj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e695f",
   "metadata": {},
   "source": [
    "# Aggregate projections for train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "188d3403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:No scans to aggregate. Skipping projection data aggregation.\n"
     ]
    }
   ],
   "source": [
    "if SCANS_AGG is not None:\n",
    "    SCANS_AGG, scan_type = read_scans_agg_file(SCANS_AGG)\n",
    "    logger.debug(f\"Loaded scan list for aggregation: {SCANS_AGG}\")\n",
    "\n",
    "    # Only aggregate projections if they don't already exist\n",
    "    if len([f for f in os.listdir(AGG_DIR) if f.startswith(\"PROJ\")]) > 0:\n",
    "        raise FileExistsError(f\"Aggregated projection data for phase {PHASE} data version {DATA_VERSION} already exists in {AGG_DIR}. Please delete the existing files or change the data version to re-aggregate.\")\n",
    "    else:\n",
    "        logger.info(\"Starting to aggregate projection data...\")\n",
    "        pt_prj_dir = os.path.join(WORK_ROOT, 'data_pt', 'prj')\n",
    "        # Aggregate and save projection data sets\n",
    "        for sample in ['TRAIN', 'VALIDATION', 'TEST']:\n",
    "            if len(SCANS_AGG[sample]):\n",
    "                prj_ngcbct = aggregate_saved_projections(scan_type, sample, pt_prj_dir, SCANS_AGG, truth=False)\n",
    "                np.save(os.path.join(AGG_DIR, f\"PROJ_ng_{scan_type}_{sample}.npy\"), prj_ngcbct.numpy()) # e.g., PROJ_ng_HF_TRAIN.npy\n",
    "                del prj_ngcbct\n",
    "                logger.debug(\"Done with nonstop-gated...\")\n",
    "                prj_gcbct = aggregate_saved_projections(scan_type, sample, pt_prj_dir, SCANS_AGG, truth=True)\n",
    "                np.save(os.path.join(AGG_DIR, f\"PROJ_gated_{scan_type}_{sample}.npy\"), prj_gcbct.numpy())\n",
    "                del prj_gcbct\n",
    "                logger.debug(\"Done with gated...\")\n",
    "\n",
    "                logger.debug(f\"Aggregated projections saved for {scan_type} {sample}.\\n\")\n",
    "            else:\n",
    "                logger.debug(f\"No scans to aggregate for {scan_type} {sample}. Skipping aggregation.\")\n",
    "\n",
    "        # Free up memory\n",
    "        del prj_gcbct, prj_ngcbct\n",
    "\n",
    "    logger.info(\"Projection data aggregation completed successfully.\")\n",
    "    logger.info(\"Aggregated projection data saved in: %s\", AGG_DIR)\n",
    "else:\n",
    "    logger.info(\"No scans to aggregate. Skipping projection data aggregation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae3e79",
   "metadata": {},
   "source": [
    "# Training PD CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daead9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pipeline:Loaded configuration from config_20epoch.yaml\n",
      "DEBUG:pipeline:Using CUDA; 1 devices.\n",
      "DEBUG:pipeline:Loaded class TrainingApp from module train_app_MK6_numpy\n",
      "INFO:pipeline:Going to try training the 1-th model with configuration from config_20epoch.yaml...\n",
      "DEBUG:pipeline:Starting TrainingApp, {'training': True, 'training_app': 'train_app_MK6_numpy.TrainingApp', 'epochs': 20, 'learning_rate': [0.001, 0.005], 'network_name': 'IResNet', 'model_version': 'MK7', 'batch_size': 8, 'optimizer': 'NAdam', 'num_workers': 0, 'shuffle': True, 'grad_clip': True, 'grad_max': 0.01, 'betas_NAdam': (0.9, 0.999), 'momentum_decay_NAdam': 0.0004, 'momentum_SGD': 0.99, 'weight_decay_SGD': 1e-08, 'checkpoint_save_step': 5, 'tensor_board': False, 'tensor_board_comment': '', 'train_during_inference': False, 'ensemble_size': 1, 'scan_type': 'HF', 'augment': None, 'input_type': None, 'data_version': '13', 'domain': 'PROJ', 'MODEL_DIR': 'D:/NoahSilverberg/ngCBCT\\\\phase7\\\\DS13\\\\model', 'AGG_DIR': 'D:/NoahSilverberg/ngCBCT\\\\phase7\\\\DS13\\\\agg'}\n",
      "DEBUG:pipeline:TRAIN images path: D:/NoahSilverberg/ngCBCT\\phase7\\DS13\\agg\\PROJ_ng_HF_TRAIN.npy\n",
      "DEBUG:pipeline:TRAIN ground truth images path: D:/NoahSilverberg/ngCBCT\\phase7\\DS13\\agg\\PROJ_gated_HF_TRAIN.npy\n",
      "DEBUG:pipeline:TRAIN dataset loaded with 32088 samples, each with shape torch.Size([1, 512, 512]).\n",
      "DEBUG:pipeline:TRAIN dataloader initialized with 4011 batches of size 8, with 0 workers, shuffle=True, and pin_memory=True.\n",
      "DEBUG:pipeline:VALIDATION images path: D:/NoahSilverberg/ngCBCT\\phase7\\DS13\\agg\\PROJ_ng_HF_VALIDATION.npy\n",
      "DEBUG:pipeline:VALIDATION ground truth images path: D:/NoahSilverberg/ngCBCT\\phase7\\DS13\\agg\\PROJ_gated_HF_VALIDATION.npy\n",
      "DEBUG:pipeline:VALIDATION dataset loaded with 4584 samples, each with shape torch.Size([1, 512, 512]).\n",
      "DEBUG:pipeline:VALIDATION dataloader initialized with 573 batches of size 8, with 0 workers, shuffle=True, and pin_memory=True.\n",
      "DEBUG:pipeline:Optimizer: NAdam with learning rate 0.001, betas (0.9, 0.999), momentum_decay 0.0004\n",
      "INFO:pipeline:TRAINING SETTINGS:\n",
      "INFO:pipeline:{'training': True, 'training_app': 'train_app_MK6_numpy.TrainingApp', 'epochs': 20, 'learning_rate': [0.001, 0.005], 'network_name': 'IResNet', 'model_version': 'MK7', 'batch_size': 8, 'optimizer': 'NAdam', 'num_workers': 0, 'shuffle': True, 'grad_clip': True, 'grad_max': 0.01, 'betas_NAdam': (0.9, 0.999), 'momentum_decay_NAdam': 0.0004, 'momentum_SGD': 0.99, 'weight_decay_SGD': 1e-08, 'checkpoint_save_step': 5, 'tensor_board': False, 'tensor_board_comment': '', 'train_during_inference': False, 'ensemble_size': 1, 'scan_type': 'HF', 'augment': None, 'input_type': None, 'data_version': '13', 'domain': 'PROJ', 'MODEL_DIR': 'D:/NoahSilverberg/ngCBCT\\\\phase7\\\\DS13\\\\model', 'AGG_DIR': 'D:/NoahSilverberg/ngCBCT\\\\phase7\\\\DS13\\\\agg'}\n",
      "INFO:pipeline:STARTING TRAINING...\n",
      "DEBUG:pipeline:LEARNING RATE HERE IS 0.001\n",
      "c:\\ProgramData\\Anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "DEBUG:pipeline:Learning rate updated to 0.005 at epoch 1.\n",
      "DEBUG:pipeline:Model set to training mode for training.\n",
      "Epoch 1 Training:   2%|▏         | 76/4011 [00:59<51:34,  1.27it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoing to try training the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-th model with configuration from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-th model.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m instance, cfg\n",
      "File \u001b[1;32md:\\NoahSilverberg\\ngCBCT\\pipeline\\train_app_MK6_numpy.py:314\u001b[0m, in \u001b[0;36mTrainingApp.main\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    311\u001b[0m     epoch_total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m train_inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# Save avg training statistics\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m epoch_avg_train_loss \u001b[38;5;241m=\u001b[39m epoch_total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dl\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m    315\u001b[0m avg_train_loss_values\u001b[38;5;241m.\u001b[39mappend(epoch_avg_train_loss)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# Store loss in TensorBoard if enabled\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for config_file in CONFIG_FILES:\n",
    "    # Load the yaml configuration file\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    logger.debug(f\"Loaded configuration from {config_file}\")\n",
    "\n",
    "    # Skip this config if the user has set PD_training to False\n",
    "    if not config['PD_settings']['training']:\n",
    "        logger.info(f\"Skipping PD training for {config_file} as PD training is set to False.\")\n",
    "        continue\n",
    "\n",
    "    # Get the training application\n",
    "    module_name, class_name = config['PD_settings']['training_app'].rsplit('.', 1)\n",
    "    module = importlib.import_module(\"pipeline.\" + module_name)\n",
    "    cls = getattr(module, class_name)\n",
    "\n",
    "    logger.debug(f\"Loaded class {class_name} from module {module_name}\")\n",
    "\n",
    "    # Get the model version (for naming purposes)\n",
    "    model_version = config['PD_settings']['model_version']\n",
    "\n",
    "    # Get the ensemble size, and loop through it\n",
    "    ensemble_size = config['PD_settings']['ensemble_size']\n",
    "    for i in range(ensemble_size):\n",
    "        # If we are training an ensemble, we add an identifier to the model version\n",
    "        if ensemble_size > 1:\n",
    "            # Deepcopy config so we don't affect the original\n",
    "            cfg = copy.deepcopy(config)\n",
    "            cfg['PD_settings']['model_version'] = f\"{model_version}_{i+1:02}\" # e.g., \"v1_01\"\n",
    "        else:\n",
    "            cfg = config\n",
    "\n",
    "        # Add the data version to the configuration\n",
    "        cfg['PD_settings']['data_version'] = DATA_VERSION\n",
    "\n",
    "        # Instantiate with the loaded configuration\n",
    "        instance = cls(cfg, \"PROJ\", DEBUG, MODEL_DIR, AGG_DIR)\n",
    "\n",
    "        logger.info(f\"Going to try training the {i + 1}-th model with configuration from {config_file}...\")\n",
    "\n",
    "        # Run the training\n",
    "        instance.main()\n",
    "\n",
    "        logger.info(f\"Finished training the {i + 1}-th model.\\n\")\n",
    "\n",
    "        del instance, cfg\n",
    "        gc.collect()\n",
    "\n",
    "    # Free up memory\n",
    "    del module, cls, config, module_name, class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef2c2c",
   "metadata": {},
   "source": [
    "# Apply PD model to all nonstop-gated sinograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28452019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ng_dir = os.path.join(WORK_ROOT, 'data_pt', 'prj', 'ng')\n",
    "# g_dir = os.path.join(WORK_ROOT, 'data_pt', 'prj', 'gated')\n",
    "ng_dir = os.path.join('G:', 'data_pt', 'prj', 'ng') # TODO change back\n",
    "g_dir = os.path.join('G:', 'data_pt', 'prj', 'gated')\n",
    "\n",
    "# Loop through the configurations again\n",
    "for config_file in CONFIG_FILES:\n",
    "    # Load the yaml configuration file\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    logger.debug(f\"Loaded configuration from {config_file}\")\n",
    "\n",
    "    # Get the ensemble size, and loop through it\n",
    "    ensemble_size = config['PD_settings']['ensemble_size']\n",
    "    for i in range(ensemble_size):\n",
    "        model_version = config['PD_settings']['model_version']\n",
    "\n",
    "        # If we are training an ensemble, we add an identifier to the model version\n",
    "        if ensemble_size > 1:\n",
    "            model_version = f\"{model_version}_{i+1:02}\"\n",
    "\n",
    "        scan_type = config['PD_settings']['scan_type']\n",
    "\n",
    "        # Load the trained PD model onto the GPU\n",
    "        PD_model = load_model(config['PD_settings']['network_name'], model_version, torch.device(CUDA_DEVICE), MODEL_DIR, DATA_VERSION, 'PROJ', scan_type)\n",
    "\n",
    "        for file in tqdm(os.listdir(ng_dir), desc=f\"Applying model {model_version} to projections\"):\n",
    "            scan_type, patient, scan = file.split('_')\n",
    "            patient = patient[1:]  # Remove the 'p' from the patient number)\n",
    "            scan = scan.split('.')[0]  # Remove the file extension\n",
    "\n",
    "            # Get the matlab dicts for the ground truth and CNN projections\n",
    "            g_mat, cnn_mat = apply_model_to_projections(patient, scan, scan_type, PD_model, g_dir, ng_dir, CUDA_DEVICE, NSG_CBCT_PATH)\n",
    "\n",
    "            # save_dir = os.path.join(RESULT_DIR, model_version)\n",
    "            save_dir = os.path.join('G:', 'result', model_version) # TODO change back\n",
    "            ensure_dir(save_dir)  # Ensure the directory exists\n",
    "\n",
    "            # Save the ground truth and CNN projections\n",
    "            scipy.io.savemat(os.path.join(save_dir, f'PROJ_gated_{scan_type}_p{patient}_{scan}.mat'), g_mat) # e.g., PROJ_gated_HF_p01_01.mat\n",
    "            scipy.io.savemat(os.path.join(save_dir, f'PROJ_ng_{scan_type}_p{patient}_{scan}.mat'), cnn_mat)\n",
    "\n",
    "            logger.debug(f\"Saved projections for {scan_type} p{patient}_{scan}.\")\n",
    "\n",
    "        # Free up memory\n",
    "        del PD_model, g_mat, cnn_mat\n",
    "\n",
    "logger.info(\"All models applied to projections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feaa971",
   "metadata": {},
   "source": [
    "# 5. TODO: FDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tigre\n",
    "from tigre.utilities import sample_loader\n",
    "from tigre.utilities import CTnoise\n",
    "import tigre.algorithms as algs\n",
    "import tigre.utilities.gpu as gpu\n",
    "from pipeline.FDK_half.FDK_half import FDKHalf\n",
    "gpuids = gpu.getGpuIds()\n",
    "gpuids.devices = [0]\n",
    "\n",
    "geo = tigre.geometry()\n",
    "# VARIABLE                                   DESCRIPTION                    UNITS\n",
    "# -------------------------------------------------------------------------------------\n",
    "# Distances\n",
    "geo.DSD = 1500  # Distance Source Detector      (mm)\n",
    "geo.DSO = 1000  # Distance Source Origin        (mm)\n",
    "# Detector parameters\n",
    "PixelSize = 0.388  # in mm\n",
    "rebin = 2  # we did 2x2 rebinning to make 0.776x0.776 detector bins\n",
    "# number of pixels              (px)\n",
    "# geo.nDetector = np.array(prj.shape[1], prj.shape[0])\n",
    "geo.nDetector = np.array([382, 510])\n",
    "# size of each pixel            (mm)\n",
    "geo.dDetector = PixelSize * rebin * np.array([1, 1])\n",
    "# total size of the detector    (mm)\n",
    "geo.sDetector = geo.nDetector * geo.dDetector\n",
    "# Image parameters\n",
    "geo.nVoxel = np.array([200, 512, 512])  # number of voxels              (vx)\n",
    "geo.dVoxel = np.array([1.0, 1.0, 1.0])  # size of each voxel            (mm)\n",
    "geo.sVoxel = geo.nVoxel * geo.dVoxel  # total size of the image       (mm)\n",
    "\n",
    "# Offsets\n",
    "geo.offOrigin = np.array([0, 0, 0])  # Offset of image from origin   (mm)\n",
    "geo.offDetector = np.array([0, 160])  # Offset of Detector            (mm)\n",
    "# These two can be also defined\n",
    "# per angle\n",
    "\n",
    "# Auxiliary\n",
    "geo.accuracy = 0.5  # Variable to define accuracy of\n",
    "# 'interpolated' projection\n",
    "# It defines the amoutn of\n",
    "# samples per voxel.\n",
    "# Recommended <=0.5             (vx/sample)\n",
    "\n",
    "# Optional Parameters\n",
    "# There is no need to define these unless you actually need them in your\n",
    "# reconstruction\n",
    "\n",
    "\n",
    "geo.COR = 0  # y direction displacement for\n",
    "# centre of rotation\n",
    "# correction                   (mm)\n",
    "# This can also be defined per\n",
    "# angle\n",
    "\n",
    "geo.rotDetector = np.array([0, 0, 0])  # Rotation of the detector, by\n",
    "# X,Y and Z axis respectively. (rad)\n",
    "# This can also be defined per\n",
    "# angle\n",
    "\n",
    "geo.mode = \"cone\"  # Or 'parallel'. Geometry type.\n",
    "\n",
    "mat = scipy.io.loadmat(os.path.join(RESULT_DIR, 'MK6', 'PROJ_gated_HF_p02_01.mat'))\n",
    "fdk_half = FDKHalf()(mat['prj'], geo, mat['angles'].flatten(), filter='hann', parker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d785531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdk_half.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c640acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fdk_half[100, :, :].T, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c49b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat(os.path.join(RESULT_DIR, 'MK6', 'PROJ_ng_HF_p02_01.mat'))\n",
    "fdk_half = FDKHalf()(mat['prj'], geo, mat['angles'].flatten(), filter='hann', parker=True)\n",
    "plt.imshow(fdk_half[100, :, :].T, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09b672",
   "metadata": {},
   "source": [
    "# 6. Aggregate CT volumes for train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for aggregated data saving\n",
    "vol_agg_dir = os.path.join(DATA_DIR, \"agg\", \"volumes\")\n",
    "ensure_dir(vol_agg_dir)\n",
    "\n",
    "# Aggregate and save volume data sets\n",
    "for scan_type in ['HF', 'FF']:\n",
    "    for sample in ['train', 'validation', 'test']:\n",
    "        vol_gcbct, vol_ngcbct = aggregate_saved_volumes(scan_type, sample)\n",
    "        torch.save(vol_gcbct, os.path.join(vol_agg_dir, f\"{scan_type}_{sample}_gated.pt\"))\n",
    "        torch.save(vol_ngcbct, os.path.join(vol_agg_dir, f\"{scan_type}_{sample}_ng.pt\"))\n",
    "\n",
    "# Free up memory\n",
    "del vol_gcbct, vol_ngcbct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e933115",
   "metadata": {},
   "source": [
    "# 7. Train ID CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b35e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f74da1",
   "metadata": {},
   "source": [
    "# 8. Inference on test scans for full 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = inference_3d(patient_id, scan_id, 'HF', data_version, model_name, 'tumor_location_panc.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
