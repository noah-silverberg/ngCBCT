{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b5c8d3",
   "metadata": {},
   "source": [
    "### Setup/Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0320969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from pipeline.paths import Directories, Files\n",
    "from pipeline.utils import read_scans_agg_file\n",
    "import scipy.stats\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec7764",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eaa8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASE = \"7\"\n",
    "DATA_VERSION = \"13\"\n",
    "SPLIT_TO_ANALYZE = 'VALIDATION'  # Options: 'TRAIN', 'VALIDATION', 'TEST'\n",
    "\n",
    "MODELS_TO_ANALYZE = [\n",
    "    {\n",
    "        'name': 'BBB pi=0.75 mu=0.0 sigma1=1e-1 sigma2=1e-2 beta=1e-2 (10)',\n",
    "        'type': 'stochastic',\n",
    "        'domain': 'IMAG',\n",
    "        'model_version_root': 'MK7_BBB_pi0.75_mu_0.0_sigma1_1e-1_sigma2_1e-3_beta_1e-1',\n",
    "        'count': 10,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Path to the .pt file containing tumor locations.\n",
    "# This is a 5D tensor [patient, scan, (x, y, z)]\n",
    "WORK_ROOT = \"D:/NoahSilverberg/ngCBCT\"\n",
    "TUMOR_LOCATIONS_FILE = 'D:/NoahSilverberg/ngCBCT/3D_recon/tumor_location.pt'\n",
    "\n",
    "# --- Advanced Config ---\n",
    "SCANS_AGG_FILE = 'scans_to_agg.txt'\n",
    "SSIM_KWARGS = {\"K1\": 0.03, \"K2\": 0.06, \"win_size\": 15}\n",
    "SSIM_KWARGS_ = {\"k1\": 0.03, \"k2\": 0.06, \"kernel_size\": 15}\n",
    "\n",
    "# --- Setup ---\n",
    "# Create Directories and Files objects\n",
    "phase_dataver_dir = os.path.join(WORK_ROOT, f\"phase{PHASE}\", f\"DS{DATA_VERSION}\")\n",
    "DIRECTORIES = Directories(\n",
    "    # projections_results_dir=os.path.join(phase_dataver_dir, \"results\", \"projections\"),\n",
    "    # projections_gated_dir=os.path.join(WORK_ROOT, \"gated\", \"prj_mat\"),\n",
    "    # reconstructions_dir=os.path.join(phase_dataver_dir, \"reconstructions\"),\n",
    "    reconstructions_gated_dir=os.path.join(WORK_ROOT, \"gated\", \"fdk_recon\"),\n",
    "    images_results_dir=os.path.join('H:\\Public/Noah/phase7/DS13', \"results\", \"images\"),\n",
    ")\n",
    "FILES = Files(DIRECTORIES)\n",
    "\n",
    "# Load the list of scans\n",
    "scans_agg, scan_type_agg = read_scans_agg_file(SCANS_AGG_FILE)\n",
    "analysis_scans = scans_agg[SPLIT_TO_ANALYZE]\n",
    "\n",
    "# Load tumor locations\n",
    "tumor_locations = torch.load(TUMOR_LOCATIONS_FILE, weights_only=False)\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE} named '{torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}'\")\n",
    "\n",
    "print(f\"\\nConfiguration loaded.\")\n",
    "print(f\"Analyzing {len(analysis_scans)} scans from the '{SPLIT_TO_ANALYZE}' split.\")\n",
    "print(f\"Found {len(MODELS_TO_ANALYZE)} model(s) to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9357ce85",
   "metadata": {},
   "source": [
    "## Function definitions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114d02c",
   "metadata": {},
   "source": [
    "### Data loading/prep functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b635426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth(files_obj: Files, scan_info, domain, slice_idx=None):\n",
    "    \"\"\"\n",
    "    Loads the ground truth data for a given scan and domain.\n",
    "    \"\"\"\n",
    "    patient, scan, scan_type = scan_info\n",
    "    \n",
    "    if domain == 'PROJ':\n",
    "        gt_path = files_obj.get_projections_results_filepath('fdk', patient, scan, scan_type, gated=True)\n",
    "        data = torch.from_numpy(scipy.io.loadmat(gt_path)['prj']).detach().permute(1, 0, 2)\n",
    "    elif domain == 'FDK':\n",
    "        gt_path = files_obj.get_recon_filepath(\"fdk\", patient, scan, scan_type, gated=True, ensure_exists=False)\n",
    "        data = torch.load(gt_path).detach()\n",
    "        data = data[20:-20, :, :]\n",
    "        data = 25. * torch.clip(data, min=0.0, max=0.04)\n",
    "    elif domain == 'IMAG':\n",
    "        # The ground truth for the IMAG domain is the FDK of the gated projection\n",
    "        gt_path = files_obj.get_recon_filepath(\"fdk\", patient, scan, scan_type, gated=True, ensure_exists=False)\n",
    "        data = torch.load(gt_path).detach()\n",
    "        data = data[20:-20, :, :]\n",
    "        data = 25. * torch.clip(data, min=0.0, max=0.04)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown domain: {domain}\")\n",
    "\n",
    "    if slice_idx is not None and data.ndim == 3:\n",
    "        return data[slice_idx]\n",
    "    return data\n",
    "\n",
    "def load_predictions(files_obj: Files, model_config, scan_info, slice_idx=None):\n",
    "    \"\"\"\n",
    "    Loads all predictions for a given model, scan, and domain.\n",
    "    \"\"\"\n",
    "    patient, scan, scan_type = scan_info\n",
    "    domain = model_config['domain']\n",
    "    root = model_config['model_version_root']\n",
    "    count = model_config['count']\n",
    "    model_type = model_config['type']\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"Loading {count} predictions for {model_config['name']}...\")\n",
    "    \n",
    "    for i in tqdm(range(count), desc=\"Loading predictions\", leave=False):\n",
    "        passthrough_num = None\n",
    "        model_version = root\n",
    "\n",
    "        if model_type == 'ensemble':\n",
    "            model_version = f\"{root}_{i+1:02d}\"\n",
    "        elif model_type == 'stochastic':\n",
    "            passthrough_num = i\n",
    "\n",
    "        if domain == 'PROJ':\n",
    "            pred_path = files_obj.get_projections_results_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            pred = torch.from_numpy(scipy.io.loadmat(pred_path)['prj']).detach().permute(1, 0, 2)\n",
    "        elif domain == 'FDK':\n",
    "            pred_path = files_obj.get_recon_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            pred = torch.load(pred_path).detach()\n",
    "            pred = pred[20:-20, :, :]\n",
    "            pred = 25. * torch.clip(pred, min=0.0, max=0.04)\n",
    "        elif domain == 'IMAG':\n",
    "            # This assumes the results are saved with the ID model version name\n",
    "            pred_path = files_obj.get_images_results_filepath(model_version, patient, scan, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            pred = torch.load(pred_path).detach()\n",
    "            pred = torch.squeeze(pred, dim=1)\n",
    "            pred = torch.permute(pred, (0, 2, 1))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown domain: {domain}\")\n",
    "            \n",
    "        predictions.append(pred)\n",
    "\n",
    "    predictions_tensor = torch.stack(predictions)\n",
    "    \n",
    "    if slice_idx is not None and predictions_tensor.ndim == 4:\n",
    "        return predictions_tensor[:, slice_idx, :, :]\n",
    "        \n",
    "    return predictions_tensor\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3cbb74",
   "metadata": {},
   "source": [
    "### Metric calculation functions: Image quality and pre-calibration uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723009ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ause_sparsification(uncertainty, errors):\n",
    "    \"\"\"\n",
    "    Calculates the Area Under the Sparsification Error curve (AUSE) efficiently.\n",
    "    \"\"\"\n",
    "    uncertainty_flat = uncertainty.flatten()\n",
    "    errors_flat = errors.flatten()\n",
    "    \n",
    "    # Normalize by overall MAE so the curve starts at 1\n",
    "    overall_mae = np.mean(errors_flat)\n",
    "    \n",
    "    def get_sparsification_curve_fast(sorted_errs):\n",
    "        n_pixels = len(sorted_errs)\n",
    "        cumulative_errors = np.cumsum(sorted_errs)\n",
    "        total_error_sum = cumulative_errors[-1]\n",
    "        sum_errors_removed = np.insert(cumulative_errors[:-1], 0, 0)\n",
    "        sum_errors_remaining = total_error_sum - sum_errors_removed\n",
    "        n_remaining = np.arange(n_pixels, 0, -1)\n",
    "        curve = sum_errors_remaining / n_remaining\n",
    "        if overall_mae > 0:\n",
    "            curve = curve / overall_mae # Normalize the curve\n",
    "        return curve\n",
    "\n",
    "    # Move arrays to GPU using torch for sorting\n",
    "    uncertainty_tensor = torch.from_numpy(uncertainty_flat).cuda()\n",
    "    errors_tensor = torch.from_numpy(errors_flat).cuda()\n",
    "\n",
    "    # Model curve (sorted by uncertainty)\n",
    "    model_sorted_indices = torch.argsort(uncertainty_tensor, descending=True)\n",
    "    model_sorted_errors = errors_tensor[model_sorted_indices].cpu().numpy()\n",
    "    model_curve = get_sparsification_curve_fast(model_sorted_errors)\n",
    "\n",
    "    # Oracle curve (sorted by error)\n",
    "    oracle_sorted_errors = torch.sort(errors_tensor, descending=True)[0].cpu().numpy()\n",
    "    oracle_curve = get_sparsification_curve_fast(oracle_sorted_errors)\n",
    "    \n",
    "    # The AUSE is the area between the two normalized curves\n",
    "    ause = np.mean(np.abs(model_curve - oracle_curve))\n",
    "    return ause\n",
    "\n",
    "def calculate_spearman_correlation(uncertainty, errors, device):\n",
    "    \"\"\"\n",
    "    Calculates the Spearman's Rank Correlation Coefficient between\n",
    "    the uncertainty and the absolute error.\n",
    "    \"\"\"\n",
    "    uncertainty_flat = torch.from_numpy(uncertainty.flatten()).to(device)\n",
    "    errors_flat = torch.from_numpy(errors.flatten()).to(device)\n",
    "    \n",
    "    # obtain ranks\n",
    "    xr = torch.argsort(torch.argsort(uncertainty_flat)).float()\n",
    "    yr = torch.argsort(torch.argsort(errors_flat)).float()\n",
    "\n",
    "    # demean\n",
    "    xr = xr - xr.mean()\n",
    "    yr = yr - yr.mean()\n",
    "\n",
    "    # compute covariance and norms\n",
    "    cov = (xr * yr).sum() / (xr.numel() - 1)\n",
    "    rho = cov / (xr.std(unbiased=True) * yr.std(unbiased=True))\n",
    "    \n",
    "    return rho.item()\n",
    "\n",
    "import torchmetrics\n",
    "import torchmetrics.image\n",
    "\n",
    "def calculate_volume_metrics_2_pass(files_obj, model_config, scan_info, gt_volume, device):\n",
    "    \"\"\"\n",
    "    Calculates stats and metrics using a two-pass algorithm for variance for improved stability.\n",
    "    Also includes a robust PSNR calculation that handles infinite values.\n",
    "    Pass 1: Calculate the mean of all predictions.\n",
    "    Pass 2: Calculate variance and other metrics using the pre-calculated mean.\n",
    "    \"\"\"\n",
    "    n_samples = model_config['count']\n",
    "    gt_volume = gt_volume.to(device) # Ensure GT is on the correct device\n",
    "\n",
    "    def prediction_generator():\n",
    "        # This generator yields tensors directly on the GPU\n",
    "        patient, scan, scan_type = scan_info\n",
    "        domain = model_config['domain']\n",
    "        root = model_config['model_version_root']\n",
    "        model_type = model_config['type']\n",
    "        for i in range(n_samples):\n",
    "            passthrough_num = None\n",
    "            model_version = root\n",
    "            if model_type == 'ensemble': model_version = f\"{root}_{i+1:02d}\"\n",
    "            elif model_type == 'stochastic': passthrough_num = i\n",
    "\n",
    "            if domain == 'PROJ':\n",
    "                pred_path = files_obj.get_projections_results_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                pred = torch.from_numpy(scipy.io.loadmat(pred_path)['prj']).detach().permute(1, 0, 2)\n",
    "            elif domain == 'FDK':\n",
    "                pred_path = files_obj.get_recon_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                pred = torch.load(pred_path).detach()\n",
    "                pred = pred[20:-20, :, :]\n",
    "                pred = 25. * torch.clip(pred, min=0.0, max=0.04)\n",
    "            elif domain == 'IMAG':\n",
    "                pred_path = files_obj.get_images_results_filepath(model_version, patient, scan, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                pred = torch.load(pred_path).detach()\n",
    "                pred = torch.squeeze(pred, dim=1)\n",
    "                pred = torch.permute(pred, (0, 2, 1))\n",
    "            yield pred.to(device)\n",
    "\n",
    "    # --- Pass 1: Calculate Mean ---\n",
    "    print(\"Pass 1: Calculating mean prediction...\")\n",
    "    mean_volume = torch.zeros_like(gt_volume)\n",
    "    # Using a simple sum and divide for the mean\n",
    "    for pred_volume in tqdm(prediction_generator(), total=n_samples, desc=\"Pass 1/2 (Mean)\", leave=False):\n",
    "        mean_volume += pred_volume\n",
    "    mean_volume /= n_samples\n",
    "\n",
    "    # --- Pass 2: Calculate Variance and Metrics ---\n",
    "    print(\"Pass 2: Calculating variance and metrics...\")\n",
    "    sum_sq_diff_volume = torch.zeros_like(gt_volume)\n",
    "    sample_avg_ssims, sample_avg_psnrs, sample_avg_mses, sample_avg_maes = [], [], [], []\n",
    "\n",
    "    # Initialize metrics on the specified device\n",
    "    data_range = gt_volume.max() - gt_volume.min()\n",
    "    ssim_metric = torchmetrics.image.StructuralSimilarityIndexMeasure(data_range=data_range, **SSIM_KWARGS_).to(device)\n",
    "\n",
    "    # PSNR metric that returns per-slice results to handle 'inf'\n",
    "    psnr_metric = torchmetrics.image.PeakSignalNoiseRatio(data_range=data_range, reduction='none').to(device)\n",
    "\n",
    "    for pred_volume in tqdm(prediction_generator(), total=n_samples, desc=\"Pass 2/2 (Var & Metrics)\", leave=False):\n",
    "        # Variance calculation\n",
    "        diff = pred_volume - mean_volume\n",
    "        sum_sq_diff_volume += diff * diff\n",
    "\n",
    "        # Per-sample metrics\n",
    "        if gt_volume.ndim > 2:\n",
    "            pred_vol_batch = pred_volume.unsqueeze(1)\n",
    "            gt_vol_batch = gt_volume.unsqueeze(1)\n",
    "            \n",
    "            sample_avg_ssims.append(ssim_metric(pred_vol_batch, gt_vol_batch).item())\n",
    "            sample_avg_mses.append(torch.mean((gt_volume - pred_volume)**2).item())\n",
    "            sample_avg_maes.append(torch.mean(torch.abs(gt_volume - pred_volume)).item())\n",
    "\n",
    "            # --- Robust PSNR Calculation ---\n",
    "            psnr_per_slice = psnr_metric(pred_vol_batch, gt_vol_batch)\n",
    "            finite_psnrs = psnr_per_slice[torch.isfinite(psnr_per_slice)] # Filter out inf values\n",
    "            if finite_psnrs.numel() > 0:\n",
    "                sample_avg_psnrs.append(torch.mean(finite_psnrs).item())\n",
    "            else:\n",
    "                # For debugging, print abs difference between GT and prediction\n",
    "                abs_diff = torch.sum(torch.abs(gt_volume - pred_volume))\n",
    "                print(f\"Absolute difference between GT and prediction: {abs_diff.item()}\")\n",
    "                # Raise error\n",
    "                raise ValueError(\"All slices in the prediction are perfect matches, leading to infinite PSNR values.\")\n",
    "\n",
    "    # Finalize variance and uncertainty\n",
    "    if n_samples > 1:\n",
    "        # Using n_samples for population standard deviation, as in the original code.\n",
    "        # For sample standard deviation, use (n_samples - 1).\n",
    "        variance_volume_map = sum_sq_diff_volume / n_samples\n",
    "        uncertainty_volume_map = torch.sqrt(variance_volume_map)\n",
    "    else:\n",
    "        uncertainty_volume_map = torch.zeros_like(mean_volume)\n",
    "\n",
    "    # --- Calculate metrics for the mean prediction ---\n",
    "    metrics = {}\n",
    "    if gt_volume.ndim > 2:\n",
    "        mean_vol_batch = mean_volume.unsqueeze(1)\n",
    "        gt_vol_batch = gt_volume.unsqueeze(1)\n",
    "        \n",
    "        metrics['mean_ssim'] = ssim_metric(mean_vol_batch, gt_vol_batch).item()\n",
    "        metrics['mean_mse'] = torch.mean((gt_volume - mean_volume)**2).item()\n",
    "        metrics['mean_mae'] = torch.mean(torch.abs(gt_volume - mean_volume)).item()\n",
    "\n",
    "        # Robust PSNR for the mean prediction\n",
    "        mean_psnr_per_slice = psnr_metric(mean_vol_batch, gt_vol_batch)\n",
    "        finite_mean_psnrs = mean_psnr_per_slice[torch.isfinite(mean_psnr_per_slice)]\n",
    "        if finite_mean_psnrs.numel() > 0:\n",
    "            metrics['mean_psnr'] = torch.mean(finite_mean_psnrs).item()\n",
    "        else:\n",
    "            # For debugging, print abs difference between GT and mean prediction\n",
    "            abs_diff = torch.sum(torch.abs(gt_volume - mean_volume))\n",
    "            print(f\"Absolute difference between GT and mean prediction: {abs_diff.item()}\")\n",
    "            # Raise error\n",
    "            raise ValueError(\"All slices in the mean prediction are perfect matches, leading to infinite PSNR values.\")\n",
    "\n",
    "    # --- Aggregate per-sample metrics ---\n",
    "    metrics['sample_avg_ssim'] = np.mean(sample_avg_ssims) if sample_avg_ssims else 0\n",
    "    metrics['sample_avg_psnr'] = np.mean(sample_avg_psnrs) if sample_avg_psnrs else 0\n",
    "    metrics['sample_avg_mse'] = np.mean(sample_avg_mses) if sample_avg_mses else 0\n",
    "    metrics['sample_avg_mae'] = np.mean(sample_avg_maes) if sample_avg_maes else 0\n",
    "\n",
    "    return metrics, mean_volume, uncertainty_volume_map\n",
    "\n",
    "print(\"Metric calculation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc53009",
   "metadata": {},
   "source": [
    "### Uncertainty calibration and post-calibration metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import isotonic_regression\n",
    "\n",
    "# --- Calibration Methods ---\n",
    "\n",
    "def calculate_platt_scaler(errors, std_devs):\n",
    "    \"\"\"\n",
    "    Calculates the optimal scaling factor 'T' for variance scaling.\n",
    "    This factor is found by minimizing the NLL on a validation set.\n",
    "    The optimal T is sqrt(mean(squared_error / variance)).\n",
    "\n",
    "    Args:\n",
    "        errors (np.ndarray): The absolute errors (ground_truth - prediction).\n",
    "        std_devs (np.ndarray): The predicted standard deviations.\n",
    "\n",
    "    Returns:\n",
    "        float: The scaling factor T.\n",
    "    \"\"\"\n",
    "    errors_flat = errors.flatten()\n",
    "    std_devs_flat = std_devs.flatten()\n",
    "\n",
    "    # To avoid division by zero, add a small epsilon to the variance\n",
    "    variances_flat = std_devs_flat**2 + 1e-6\n",
    "\n",
    "    # Calculate T^2 = mean(error^2 / variance)\n",
    "    t_squared = np.mean(errors_flat**2 / variances_flat)\n",
    "\n",
    "    return np.sqrt(t_squared)\n",
    "\n",
    "\n",
    "def train_isotonic_regression(predicted_std, observed_errors, device):\n",
    "    \"\"\"\n",
    "    Trains an Isotonic Regression model using scipy.optimize.isotonic_regression.\n",
    "    \"\"\"\n",
    "    pred_std_flat = predicted_std.flatten()\n",
    "    obs_err_flat = observed_errors.flatten()\n",
    "\n",
    "    # Sort by predicted standard deviation\n",
    "    print(\"Sorting erorrs...\")\n",
    "    sort_indices = torch.argsort(torch.from_numpy(pred_std_flat).to(device)).cpu().numpy()\n",
    "    sorted_pred_std = pred_std_flat[sort_indices]\n",
    "    sorted_obs_err_sq = obs_err_flat[sort_indices]**2\n",
    "\n",
    "    # Apply scipy's isotonic regression\n",
    "    print(\"Applying isotonic regression...\")\n",
    "    calibrated_variances = isotonic_regression(sorted_obs_err_sq).x\n",
    "    calibrated_std = np.sqrt(calibrated_variances)\n",
    "\n",
    "    # Create an interpolation function to map new predictions\n",
    "    unique_pred_std, unique_indices = np.unique(sorted_pred_std, return_index=True)\n",
    "    unique_calib_std = calibrated_std[unique_indices]\n",
    "    \n",
    "    # check that the unique pred std are sorted\n",
    "    if not np.all(np.diff(unique_pred_std) >= 0):\n",
    "        raise ValueError(\"Predicted standard deviations are not sorted. Ensure the input is sorted before applying isotonic regression.\")\n",
    "\n",
    "    print(\"Creating interpolation model...\")\n",
    "    iso_model = interp1d(unique_pred_std, unique_calib_std, kind='linear', bounds_error=False, \n",
    "                         fill_value=(unique_calib_std[0], unique_calib_std[-1]), assume_sorted=True)\n",
    "\n",
    "    return iso_model\n",
    "\n",
    "\n",
    "# --- New Evaluation Metrics ---\n",
    "\n",
    "def calculate_nll(ground_truth, mean_pred, uncertainty_map):\n",
    "    \"\"\"\n",
    "    Calculates the Negative Log-Likelihood (NLL) for a Gaussian prediction.\n",
    "    \"\"\"\n",
    "    gt_flat = ground_truth.flatten()\n",
    "    pred_flat = mean_pred.flatten()\n",
    "    uncert_flat = uncertainty_map.flatten()\n",
    "\n",
    "    # Add a small epsilon to variance to prevent log(0) or division by zero\n",
    "    variance = uncert_flat**2 + 1e-9\n",
    "    \n",
    "    # NLL formula for a Gaussian distribution\n",
    "    nll_values = 0.5 * (np.log(2 * np.pi * variance) + (gt_flat - pred_flat)**2 / variance)\n",
    "    \n",
    "    return np.mean(nll_values)\n",
    "\n",
    "\n",
    "def calculate_all_eces(ground_truth, mean_pred, uncertainty_map, n_bins=20):\n",
    "    \"\"\"\n",
    "    Calculates variations of the Expected Calibration Error (ECE) for regression.\n",
    "    \"\"\"\n",
    "    gt_flat = ground_truth.flatten()\n",
    "    pred_flat = mean_pred.flatten()\n",
    "    uncert_flat = uncertainty_map.flatten() + 1e-9 # Avoid zero std dev\n",
    "\n",
    "    # Get the predicted CDF value for each ground truth point\n",
    "    pred_cdfs = scipy.stats.norm.cdf(gt_flat, loc=pred_flat, scale=uncert_flat)\n",
    "    \n",
    "    expected_confidence_levels = np.linspace(0, 1, n_bins)\n",
    "    observed_frequencies = np.array([np.mean(pred_cdfs <= p_j) for p_j in expected_confidence_levels])\n",
    "\n",
    "    # --- Calculate bin weights (proportional to number of points in each bin) ---\n",
    "    bin_weights = np.zeros(n_bins)\n",
    "    for i in range(1, n_bins):\n",
    "        lower_bound = expected_confidence_levels[i-1]\n",
    "        upper_bound = expected_confidence_levels[i]\n",
    "        points_in_bin = (pred_cdfs > lower_bound) & (pred_cdfs <= upper_bound)\n",
    "        bin_weights[i] = np.mean(points_in_bin)\n",
    "    \n",
    "    if np.sum(bin_weights) > 0:\n",
    "        bin_weights /= np.sum(bin_weights)\n",
    "    \n",
    "    # --- Calculate ECE variants ---\n",
    "    abs_diff = np.abs(expected_confidence_levels - observed_frequencies)\n",
    "    sq_diff = (expected_confidence_levels - observed_frequencies)**2\n",
    "\n",
    "    ece_weighted_abs = np.sum(bin_weights * abs_diff)\n",
    "    ece_weighted_sq = np.sum(bin_weights * sq_diff)\n",
    "    ece_unweighted_abs = np.mean(abs_diff)\n",
    "    ece_unweighted_sq = np.mean(sq_diff)\n",
    "    \n",
    "    return {\n",
    "        'ece_weighted_abs': ece_weighted_abs,\n",
    "        'ece_weighted_sq': ece_weighted_sq,\n",
    "        'ece_unweighted_abs': ece_unweighted_abs,\n",
    "        'ece_unweighted_sq': ece_unweighted_sq,\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_ence(ground_truth, mean_pred, uncertainty_map, device, n_bins=20):\n",
    "    \"\"\"\n",
    "    Calculates the Expected Normalized Calibration Error (ENCE) using quantile-based binning\n",
    "    as described in Levi et al., 2020.\n",
    "    \"\"\"\n",
    "    gt_flat = ground_truth.flatten()\n",
    "    pred_flat = mean_pred.flatten()\n",
    "    uncert_flat = uncertainty_map.flatten()\n",
    "    \n",
    "    # Ensure there are enough unique values for binning\n",
    "    if len(np.unique(uncert_flat)) < n_bins:\n",
    "        print(f\"Warning: Number of unique uncertainties is less than n_bins. ENCE may be unreliable.\")\n",
    "\n",
    "    # Get the indices that would sort the uncertainties\n",
    "    sorted_indices = torch.argsort(torch.from_numpy(uncert_flat).to(device)).cpu().numpy()\n",
    "    \n",
    "    # Split the sorted indices into N bins of equal size.\n",
    "    # np.array_split handles cases where the total number of points is not divisible by n_bins.\n",
    "    binned_indices = np.array_split(sorted_indices, n_bins)\n",
    "\n",
    "    ence_sum = 0\n",
    "    \n",
    "    # Loop through each bin of indices\n",
    "    for bin_idx_list in binned_indices:\n",
    "        # Skip empty bins, though this is unlikely with quantile binning\n",
    "        if len(bin_idx_list) > 0:\n",
    "            # Root Mean Variance (RMV) in bin\n",
    "            rmv_j = np.sqrt(np.mean(uncert_flat[bin_idx_list]**2)) + 1e-9\n",
    "            \n",
    "            # Root Mean Squared Error (RMSE) in bin\n",
    "            rmse_j = np.sqrt(np.mean((gt_flat[bin_idx_list] - pred_flat[bin_idx_list])**2))\n",
    "\n",
    "            # Add the normalized error for this bin to the sum\n",
    "            ence_sum += np.abs(rmv_j - rmse_j) / rmv_j\n",
    "\n",
    "    return ence_sum / n_bins\n",
    "\n",
    "\n",
    "def calculate_mpiw(uncertainty_map, confidence_levels=[0.68, 0.95]):\n",
    "    \"\"\"\n",
    "    Calculates the Mean Prediction Interval Width (MPIW) for given confidence levels.\n",
    "    \"\"\"\n",
    "    uncert_flat = uncertainty_map.flatten()\n",
    "    results = {}\n",
    "    for level in confidence_levels:\n",
    "        # Get the z-score for the confidence level (e.g., 1.96 for 95%)\n",
    "        z_score = scipy.stats.norm.ppf(1 - (1 - level) / 2)\n",
    "        \n",
    "        # Width of the prediction interval\n",
    "        widths = 2 * z_score * uncert_flat\n",
    "        \n",
    "        # Store the mean width\n",
    "        results[f'mpiw_{int(level*100)}'] = np.mean(widths)\n",
    "        \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Calibration and advanced uncertainty metric functions are defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85d476",
   "metadata": {},
   "source": [
    "### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b12fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_comparison(mean_pred, ground_truth, uncertainty_map, model_name, scan_name, slice_idx, tumor_coords_xy=None):\n",
    "    \"\"\"Plots the GT, mean prediction, absolute error, and uncertainty map.\"\"\"\n",
    "    error_map = np.abs(ground_truth - mean_pred)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    fig.suptitle(f'{model_name} - {scan_name} - Slice {slice_idx} (Mean vs. GT)', fontsize=16)\n",
    "\n",
    "    im1 = axes[0].imshow(ground_truth, cmap='gray')\n",
    "    axes[0].set_title('Ground Truth')\n",
    "    axes[0].axis('off')\n",
    "    fig.colorbar(im1, ax=axes[0])\n",
    "\n",
    "    if tumor_coords_xy:\n",
    "        x, y = tumor_coords_xy\n",
    "        for i in range(4):\n",
    "            axes[i].annotate('', xy=(x, y), xytext=(x - 30, y - 30),\n",
    "                            arrowprops=dict(facecolor='red', edgecolor='red', shrink=0.05, width=1, headwidth=5, headlength=5))\n",
    "\n",
    "    im2 = axes[1].imshow(mean_pred, cmap='gray')\n",
    "    axes[1].set_title('Mean Prediction')\n",
    "    axes[1].axis('off')\n",
    "    fig.colorbar(im2, ax=axes[1])\n",
    "\n",
    "    im3 = axes[2].imshow(error_map, cmap='magma')\n",
    "    axes[2].set_title('Absolute Error Map')\n",
    "    axes[2].axis('off')\n",
    "    fig.colorbar(im3, ax=axes[2])\n",
    "\n",
    "    im4 = axes[3].imshow(uncertainty_map, cmap='viridis')\n",
    "    axes[3].set_title('Uncertainty (Std Dev)')\n",
    "    axes[3].axis('off')\n",
    "    fig.colorbar(im4, ax=axes[3])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ssim_map(ssim_map, model_name, scan_name):\n",
    "    \"\"\"Plots the SSIM map.\"\"\"\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(ssim_map, cmap='viridis', vmin=0, vmax=1)\n",
    "    plt.title(f'SSIM Map - {model_name} - {scan_name}')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_calibration_curve(ground_truth, mean_pred, uncertainty_map, model_name, scan_name, n_levels=20):\n",
    "    \"\"\"\n",
    "    Plots the calibration curve with a marginal histogram below the x-axis\n",
    "    showing the distribution of the predicted CDF values.\n",
    "    \"\"\"\n",
    "    gt_flat = ground_truth.flatten()\n",
    "    pred_flat = mean_pred.flatten()\n",
    "    uncert_flat = uncertainty_map.flatten()\n",
    "\n",
    "    # Calculate the predicted CDF value for each point\n",
    "    pred_cdfs = scipy.stats.norm.cdf(gt_flat, loc=pred_flat, scale=uncert_flat)\n",
    "    \n",
    "    # --- Create figure with two subplots, sharing the x-axis ---\n",
    "    fig, (ax_cal, ax_hist) = plt.subplots(\n",
    "        2, 1,\n",
    "        figsize=(8, 8),\n",
    "        sharex=True,\n",
    "        gridspec_kw={'height_ratios': [3, 1]} # Main plot is 3x taller\n",
    "    )\n",
    "    \n",
    "    # --- Main Calibration Plot (top) ---\n",
    "    expected_confidence_levels = np.linspace(0, 1, n_levels)\n",
    "    observed_frequencies = np.array([np.mean(pred_cdfs <= p_j) for p_j in expected_confidence_levels])\n",
    "\n",
    "    ax_cal.plot([0, 1], [0, 1], '--', color='grey', label='Perfectly Calibrated')\n",
    "    ax_cal.plot(expected_confidence_levels, observed_frequencies, '-o', label='Model Calibration')\n",
    "    ax_cal.set_ylabel('Observed Confidence Level')\n",
    "    ax_cal.set_title(f'Calibration Plot - {model_name} - {scan_name}')\n",
    "    ax_cal.legend()\n",
    "    ax_cal.grid(True, linestyle=':')\n",
    "\n",
    "    # --- Marginal Histogram (bottom) ---\n",
    "    ax_hist.hist(pred_cdfs, bins=50, range=(0,1), density=True, color='steelblue', alpha=0.8)\n",
    "    ax_hist.set_xlabel('Expected Confidence Level (Predicted CDF)')\n",
    "    ax_hist.set_ylabel('Density')\n",
    "    ax_hist.set_yscale('log')\n",
    "    # ax_hist.set_yticks([]) # Hide y-ticks for clarity\n",
    "\n",
    "    # Final adjustments\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_sparsification_curve(uncertainty, errors, model_name, scan_name, device):\n",
    "    \"\"\"\n",
    "    Plots the model and oracle sparsification curves used for AUSE calculation.\n",
    "    \"\"\"\n",
    "    uncertainty_flat = uncertainty.flatten()\n",
    "    errors_flat = errors.flatten()\n",
    "    \n",
    "    def get_sparsification_curve_fast(sorted_errs, overall_mae):\n",
    "        n_pixels = len(sorted_errs)\n",
    "        cumulative_errors = np.cumsum(sorted_errs)\n",
    "        total_error_sum = cumulative_errors[-1]\n",
    "        sum_errors_removed = np.insert(cumulative_errors[:-1], 0, 0)\n",
    "        sum_errors_remaining = total_error_sum - sum_errors_removed\n",
    "        n_remaining = np.arange(n_pixels, 0, -1)\n",
    "        curve = sum_errors_remaining / n_remaining\n",
    "        if overall_mae > 0:\n",
    "            curve = curve / overall_mae\n",
    "        return curve\n",
    "    \n",
    "    overall_mae = np.mean(errors_flat)\n",
    "\n",
    "    # Model curve (sorted by uncertainty)\n",
    "    model_sorted_indices = torch.argsort(torch.from_numpy(uncertainty_flat).to(device), descending=True).cpu().numpy()\n",
    "    model_sorted_errors = errors_flat[model_sorted_indices]\n",
    "    model_curve = get_sparsification_curve_fast(model_sorted_errors, overall_mae)\n",
    "\n",
    "    # Oracle curve (sorted by error)\n",
    "    oracle_sorted_errors = torch.sort(torch.from_numpy(errors_flat).to(device), descending=True).values.cpu().numpy()\n",
    "    oracle_curve = get_sparsification_curve_fast(oracle_sorted_errors, overall_mae)\n",
    "    \n",
    "    # X-axis: fraction of pixels removed\n",
    "    fraction_removed = np.linspace(0, 1, len(model_curve))\n",
    "\n",
    "    # Downsample to 1000 points\n",
    "    if len(fraction_removed) > 1000:\n",
    "        step = len(fraction_removed) // 1000\n",
    "        fraction_removed = fraction_removed[::step]\n",
    "        model_curve = model_curve[::step]\n",
    "        oracle_curve = oracle_curve[::step]\n",
    "    \n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(fraction_removed, model_curve, label='Model (Sort by Uncertainty)')\n",
    "    plt.plot(fraction_removed, oracle_curve, '--', label='Oracle (Sort by Error)')\n",
    "    plt.xlabel('Fraction of Pixels Removed')\n",
    "    plt.ylabel('Mean Absolute Error of Remaining Pixels')\n",
    "    plt.title(f'Sparsification Curve - {model_name} - {scan_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':')\n",
    "    plt.show()\n",
    "\n",
    "def plot_samples_comparison(ground_truth, mean_pred, samples, model_name, scan_name, slice_idx, tumor_coords_xy=None):\n",
    "    \"\"\"\n",
    "    Plots the ground truth, mean prediction, and a few individual sample predictions.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth (np.ndarray): The 2D ground truth slice.\n",
    "        mean_pred (np.ndarray): The 2D mean prediction slice.\n",
    "        samples (list of np.ndarray): A list of 2D sample prediction slices.\n",
    "        model_name (str): The name of the model for the title.\n",
    "        scan_name (str): The name of the scan for the title.\n",
    "        slice_idx (int): The index of the slice for the title.\n",
    "        tumor_coords_xy (tuple, optional): (x, y) coordinates for the tumor arrow.\n",
    "    \"\"\"\n",
    "    num_samples = len(samples)\n",
    "    # Total columns = 1 for GT + 1 for Mean + N for samples\n",
    "    num_cols = 2 + num_samples\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_cols, figsize=(4 * num_cols, 4.5), constrained_layout=True)\n",
    "    fig.suptitle(f'{model_name} - {scan_name} - Slice {slice_idx} (GT, Mean, and Samples)', fontsize=16)\n",
    "\n",
    "    # Determine a consistent grayscale range based on the ground truth and mean\n",
    "    vmin = min(ground_truth.min(), mean_pred.min())\n",
    "    vmax = max(ground_truth.max(), mean_pred.max())\n",
    "\n",
    "    # --- Plot Ground Truth ---\n",
    "    axes[0].imshow(ground_truth, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    axes[0].set_title('Ground Truth')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # --- Plot Mean Prediction ---\n",
    "    axes[1].imshow(mean_pred, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    axes[1].set_title('Mean Prediction')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # --- Plot Samples ---\n",
    "    for i in range(num_samples):\n",
    "        ax = axes[i + 2]\n",
    "        im = ax.imshow(samples[i], cmap='gray', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f'Sample {i+1}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # --- Add Tumor Arrow ---\n",
    "    if tumor_coords_xy:\n",
    "        x, y = tumor_coords_xy\n",
    "        for ax in axes:\n",
    "            ax.annotate('', xy=(x, y), xytext=(x - 30, y - 30),\n",
    "                        arrowprops=dict(facecolor='red', edgecolor='red', shrink=0.05, \n",
    "                                        width=1, headwidth=5, headlength=5))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_worst_samples_comparison(ground_truth, mean_pred, worst_samples_data, model_name, scan_name, slice_idx, tumor_coords_xy=None):\n",
    "    \"\"\"\n",
    "    Plots the ground truth, mean prediction, and the worst-performing sample predictions.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth (np.ndarray): The 2D ground truth slice.\n",
    "        mean_pred (np.ndarray): The 2D mean prediction slice.\n",
    "        worst_samples_data (list): A list of tuples, where each tuple is \n",
    "                                   (loss, sample_slice_numpy, sample_index).\n",
    "        model_name (str): The name of the model for the title.\n",
    "        scan_name (str): The name of the scan for the title.\n",
    "        slice_idx (int): The index of the slice for the title.\n",
    "        tumor_coords_xy (tuple, optional): (x, y) coordinates for the tumor arrow.\n",
    "    \"\"\"\n",
    "    num_samples = len(worst_samples_data)\n",
    "    num_cols = 2 + num_samples\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_cols, figsize=(4 * num_cols, 5), constrained_layout=True)\n",
    "    fig.suptitle(f'{model_name} - {scan_name} - Slice {slice_idx} (Top {num_samples} Worst Samples by SmoothL1Loss)', fontsize=16)\n",
    "\n",
    "    # Determine a consistent grayscale range\n",
    "    vmin = min(ground_truth.min(), mean_pred.min())\n",
    "    vmax = max(ground_truth.max(), mean_pred.max())\n",
    "\n",
    "    # --- Plot Ground Truth ---\n",
    "    axes[0].imshow(ground_truth, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    axes[0].set_title('Ground Truth')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # --- Plot Mean Prediction ---\n",
    "    axes[1].imshow(mean_pred, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    axes[1].set_title('Mean Prediction')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # --- Plot Worst Samples ---\n",
    "    for i in range(num_samples):\n",
    "        loss, sample_slice, sample_idx = worst_samples_data[i]\n",
    "        ax = axes[i + 2]\n",
    "        im = ax.imshow(sample_slice, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "        # Add the loss and original sample number to the title\n",
    "        ax.set_title(f'Sample #{sample_idx}\\nLoss: {loss:.4f}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # --- Add Tumor Arrow ---\n",
    "    if tumor_coords_xy:\n",
    "        x, y = tumor_coords_xy\n",
    "        for ax in axes:\n",
    "            ax.annotate('', xy=(x, y), xytext=(x - 30, y - 30),\n",
    "                        arrowprops=dict(facecolor='red', edgecolor='red', shrink=0.05, \n",
    "                                        width=1, headwidth=5, headlength=5))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_combined_calibration_curves(\n",
    "    ground_truth, mean_pred, \n",
    "    platt_uncertainty_map, iso_uncertainty_map, \n",
    "    model_name, scan_name, n_bins=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots ECE and ENCE calibration curves on a single figure,\n",
    "    comparing STD Scaling and Isotonic Regression directly.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    fig.suptitle(f'Calibration Comparison for {model_name} on {scan_name}', fontsize=16)\n",
    "\n",
    "    # --- ECE Subplot (ax1) ---\n",
    "    ece_calibrations = {'STD Scaling': platt_uncertainty_map, 'Isotonic': iso_uncertainty_map}\n",
    "    ece_values = {}\n",
    "\n",
    "    for name, uncert_map in ece_calibrations.items():\n",
    "        gt_flat, pred_flat, uncert_flat = ground_truth.flatten(), mean_pred.flatten(), uncert_map.flatten() + 1e-9\n",
    "        pred_cdfs = scipy.stats.norm.cdf(gt_flat, loc=pred_flat, scale=uncert_flat)\n",
    "        \n",
    "        expected_confidence = np.linspace(0, 1, n_bins + 1)\n",
    "        observed_confidence = np.array([np.mean(pred_cdfs <= p_j) for p_j in expected_confidence])\n",
    "        \n",
    "        ax1.plot(expected_confidence, observed_confidence, '-o', label=name, alpha=0.8)\n",
    "        \n",
    "        ece_metric = calculate_all_eces(ground_truth, mean_pred, uncert_map, n_bins=n_bins)\n",
    "        ece_values[name] = ece_metric['ece_unweighted_abs']\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], '--', color='grey', label='Perfect')\n",
    "    ax1.set_title(f'ECE Plot | STD ECE: {ece_values[\"STD Scaling\"]:.4f} | Isotonic ECE: {ece_values[\"Isotonic\"]:.4f}')\n",
    "    ax1.set_xlabel('Expected Confidence Level')\n",
    "    ax1.set_ylabel('Observed Confidence Level')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle=':')\n",
    "    ax1.axis('equal')\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "\n",
    "    # --- ENCE Subplot (ax2) ---\n",
    "    ence_calibrations = {'STD Scaling': platt_uncertainty_map, 'Isotonic': iso_uncertainty_map}\n",
    "    ence_values = {}\n",
    "    all_points = []\n",
    "\n",
    "    for name, uncert_map in ence_calibrations.items():\n",
    "        gt_flat, pred_flat, uncert_flat = ground_truth.flatten(), mean_pred.flatten(), uncert_map.flatten()\n",
    "        \n",
    "        sorted_indices = torch.argsort(torch.from_numpy(uncert_flat).to(DEVICE)).cpu().numpy()\n",
    "        binned_indices = np.array_split(sorted_indices, n_bins)\n",
    "        \n",
    "        bin_rmv, bin_rmse = [], []\n",
    "        for bin_idx_list in binned_indices:\n",
    "            if len(bin_idx_list) > 0:\n",
    "                rmv_j = np.sqrt(np.mean(uncert_flat[bin_idx_list]**2))\n",
    "                rmse_j = np.sqrt(np.mean((gt_flat[bin_idx_list] - pred_flat[bin_idx_list])**2))\n",
    "                bin_rmv.append(rmv_j)\n",
    "                bin_rmse.append(rmse_j)\n",
    "        \n",
    "        ax2.plot(bin_rmv, bin_rmse, '-o', label=name, alpha=0.8, zorder=3)\n",
    "        all_points.extend(bin_rmv)\n",
    "        all_points.extend(bin_rmse)\n",
    "        \n",
    "        ence_values[name] = calculate_ence(ground_truth, mean_pred, uncert_map, DEVICE, n_bins=n_bins)\n",
    "\n",
    "    if all_points:\n",
    "      max_val = np.max(all_points) * 1.1\n",
    "      ax2.plot([0, max_val], [0, max_val], '--', color='grey', label='Perfect')\n",
    "      ax2.set_xlim(left=0, right=max_val)\n",
    "      ax2.set_ylim(bottom=0, top=max_val)\n",
    "\n",
    "    ax2.set_title(f'ENCE Plot (RMSE vs. RMV) | STD ENCE: {ence_values[\"STD Scaling\"]:.4f} | Isotonic ENCE: {ence_values[\"Isotonic\"]:.4f}')\n",
    "    ax2.set_xlabel('Root Mean Variance (RMV) per Bin')\n",
    "    ax2.set_ylabel('Root Mean Squared Error (RMSE) per Bin')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle=':')\n",
    "    ax2.axis('equal')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df3b1db",
   "metadata": {},
   "source": [
    "## Pre-calibration processing & results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ffbbf",
   "metadata": {},
   "source": [
    "NOTES FOR BBB:\n",
    "pi=0.5 has been bad for all (hallucinations)\n",
    "pi=0.75, sigma1=1e-1, sigma2=1e-3, beta=1e-2 was good\n",
    "pi=0.25, sigma1=1e-1, sigma2=1e-3, beta=1e-2 was good\n",
    "pi=0.75, sigma1=1e-1, sigma2=1e-3, beta=1e-3 had hallucinations\n",
    "pi=0.75, beta=1e-2, and sigma1=5e-1 or sigma2=1e-2 hallucinated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c49df",
   "metadata": {},
   "source": [
    "### Main loop: Pre-calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f091a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list will store dictionaries of results for each scan and model\n",
    "all_results = []\n",
    "\n",
    "for model_config in MODELS_TO_ANALYZE:\n",
    "    model_name = model_config['name']\n",
    "    domain = model_config['domain']\n",
    "    \n",
    "    scan_results = []\n",
    "\n",
    "    for scan_info in tqdm(analysis_scans, desc=f\"Analyzing Model: {model_name}\"):\n",
    "        patient, scan, _ = scan_info\n",
    "        scan_name = f\"p{patient}_{scan}\"\n",
    "        \n",
    "        # --- Determine Domain and Plotting Slice ---\n",
    "        is_visual_domain = domain in ['FDK', 'IMAG']\n",
    "        plot_slice_idx = None\n",
    "        tumor_xy = None\n",
    "        \n",
    "        if is_visual_domain:\n",
    "            if 'tumor_locations' in locals() and tumor_locations is not None:\n",
    "                try:\n",
    "                    loc = tumor_locations[int(patient), int(scan)]\n",
    "                    tumor_xy = (loc[1].item(), loc[0].item())\n",
    "                    plot_slice_idx = int(loc[2].item()) - 20\n",
    "                except (IndexError, TypeError):\n",
    "                    raise Exception(f\"Could not find tumor location for {scan_name}.\")\n",
    "            else:\n",
    "                raise Exception(f\"Could not find tumor location for {scan_name}.\")\n",
    "        \n",
    "        # --- Data Loading (Ground Truth Only) ---\n",
    "        gt_volume = load_ground_truth(FILES, scan_info, domain, slice_idx=None)\n",
    "        gt_volume_np = gt_volume.cpu().numpy()\n",
    "        \n",
    "        # --- Iterative Metric Calculation ---\n",
    "        print(\"Calculating metrics iteratively...\")\n",
    "        iq_metrics, mean_pred_vol, uncertainty_map_vol = calculate_volume_metrics_2_pass(\n",
    "            FILES, model_config, scan_info, gt_volume.to(DEVICE), DEVICE\n",
    "        )\n",
    "        mean_pred_vol = mean_pred_vol.cpu().numpy()\n",
    "        uncertainty_map_vol = uncertainty_map_vol.cpu().numpy()\n",
    "\n",
    "        \n",
    "        # --- Uncertainty Metric Calculation ---\n",
    "        errors_vol = np.abs(gt_volume_np - mean_pred_vol)\n",
    "        print(\"Calculating AUSE...\")\n",
    "        ause_val = calculate_ause_sparsification(uncertainty_map_vol, errors_vol)\n",
    "        # print(\"Calculating ECE...\")\n",
    "        # ece_val = calculate_ece(gt_volume_np, mean_pred_vol, uncertainty_map_vol)\n",
    "        print(\"Calculating Spearman's correlation...\")\n",
    "        spearman_val = calculate_spearman_correlation(uncertainty_map_vol, errors_vol, DEVICE)\n",
    "        \n",
    "        # --- Store Results ---\n",
    "        scan_result = {\n",
    "            'model_name': model_name,\n",
    "            'scan_name': scan_name,\n",
    "            **iq_metrics,\n",
    "            'ause': ause_val,\n",
    "            # 'ece': ece_val,\n",
    "            'spearman_corr': spearman_val,\n",
    "        }\n",
    "        scan_results.append(scan_result)\n",
    "        \n",
    "        # --- Visualization ---\n",
    "        print(f\"\\n--- Results for {model_name} on {scan_name} ---\")\n",
    "        \n",
    "        # plot_calibration_curve(gt_volume_np, mean_pred_vol, uncertainty_map_vol, model_name, scan_name)\n",
    "        # plot_sparsification_curve(uncertainty_map_vol, errors_vol, model_name, scan_name, DEVICE)\n",
    "        \n",
    "        if is_visual_domain:\n",
    "            gt_slice_np = gt_volume_np[plot_slice_idx]\n",
    "            mean_pred_slice = mean_pred_vol[plot_slice_idx]\n",
    "            uncertainty_map_slice = uncertainty_map_vol[plot_slice_idx]\n",
    "            _, ssim_map_val = ssim(gt_slice_np, mean_pred_slice, \n",
    "                                  data_range=(np.max(gt_slice_np) - np.min(gt_slice_np)), \n",
    "                                  full=True, **SSIM_KWARGS)\n",
    "            \n",
    "            plot_mean_comparison(mean_pred_slice, gt_slice_np, uncertainty_map_slice, \n",
    "                                 model_name, scan_name, plot_slice_idx, tumor_coords_xy=tumor_xy)\n",
    "            # plot_ssim_map(ssim_map_val, model_name, scan_name)\n",
    "\n",
    "            # --- Load and plot a few samples for visual comparison ---\n",
    "            print(\"Loading and plotting samples...\")\n",
    "            SAMPLES_TO_PLOT = 3\n",
    "            sample_slices_for_plotting = []\n",
    "            \n",
    "            # Determine the number of samples to load (can't be more than what's available)\n",
    "            num_to_load = min(SAMPLES_TO_PLOT, model_config['count'])\n",
    "\n",
    "            for i in range(num_to_load):\n",
    "                passthrough_num = None\n",
    "                model_version = model_config['model_version_root']\n",
    "                model_type = model_config['type']\n",
    "\n",
    "                if model_type == 'ensemble':\n",
    "                    model_version = f\"{model_config['model_version_root']}_{i+1:02d}\"\n",
    "                elif model_type == 'stochastic':\n",
    "                    passthrough_num = i\n",
    "                \n",
    "                # This loading logic is copied from your metrics function\n",
    "                pred = None\n",
    "                if domain == 'FDK':\n",
    "                    pred_path = FILES.get_recon_filepath(model_version, patient, scan, scan_type_agg, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                    pred = torch.load(pred_path).detach()\n",
    "                    pred = pred[20:-20, :, :]\n",
    "                    pred = 25. * torch.clip(pred, min=0.0, max=0.04)\n",
    "                elif domain == 'IMAG':\n",
    "                    pred_path = FILES.get_images_results_filepath(model_version, patient, scan, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                    pred = torch.load(pred_path).detach()\n",
    "                    pred = torch.squeeze(pred, dim=1)\n",
    "                    pred = torch.permute(pred, (0, 2, 1))\n",
    "                \n",
    "                # Get the specific slice and convert to a numpy array for plotting\n",
    "                if pred is not None:\n",
    "                    sample_slice = pred[plot_slice_idx].cpu().numpy()\n",
    "                    sample_slices_for_plotting.append(sample_slice)\n",
    "\n",
    "            # Call the new plotting function\n",
    "            if sample_slices_for_plotting:\n",
    "                plot_samples_comparison(\n",
    "                    ground_truth=gt_slice_np,\n",
    "                    mean_pred=mean_pred_slice,\n",
    "                    samples=sample_slices_for_plotting,\n",
    "                    model_name=model_name,\n",
    "                    scan_name=scan_name,\n",
    "                    slice_idx=plot_slice_idx,\n",
    "                    tumor_coords_xy=tumor_xy\n",
    "                )\n",
    "\n",
    "            # print(\"Finding and plotting worst samples by Smooth L1 Loss...\")\n",
    "            # WORST_SAMPLES_TO_PLOT = 5\n",
    "\n",
    "            # # List to store tuples of (loss, sample_slice_numpy, sample_index)\n",
    "            # worst_samples_data = []\n",
    "            # # Get the ground truth slice as a tensor on the correct device\n",
    "            # gt_slice_tensor = gt_volume[plot_slice_idx].to(DEVICE)\n",
    "\n",
    "            # # Loop through all available samples to find the worst ones\n",
    "            # for i in tqdm(range(model_config['count']), desc=\"Finding Worst Samples\", leave=False):\n",
    "            #     passthrough_num = None\n",
    "            #     model_version = model_config['model_version_root']\n",
    "            #     model_type = model_config['type']\n",
    "\n",
    "            #     if model_type == 'ensemble':\n",
    "            #         model_version = f\"{model_config['model_version_root']}_{i+1:02d}\"\n",
    "            #     elif model_type == 'stochastic':\n",
    "            #         passthrough_num = i\n",
    "                \n",
    "            #     # Load the prediction volume as a tensor on the GPU\n",
    "            #     pred_vol_tensor = None\n",
    "            #     if domain == 'FDK':\n",
    "            #         pred_path = FILES.get_recon_filepath(model_version, patient, scan, scan_type_agg, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            #         pred_vol_tensor = torch.load(pred_path, map_location=DEVICE).detach()\n",
    "            #         pred_vol_tensor = pred_vol_tensor[20:-20, :, :]\n",
    "            #         pred_vol_tensor = 25. * torch.clip(pred_vol_tensor, min=0.0, max=0.04)\n",
    "            #     elif domain == 'IMAG':\n",
    "            #         pred_path = FILES.get_images_results_filepath(model_version, patient, scan, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            #         pred_vol_tensor = torch.load(pred_path, map_location=DEVICE).detach()\n",
    "            #         pred_vol_tensor = torch.squeeze(pred_vol_tensor, dim=1)\n",
    "            #         pred_vol_tensor = torch.permute(pred_vol_tensor, (0, 2, 1))\n",
    "\n",
    "            #     if pred_vol_tensor is not None:\n",
    "            #         pred_slice_tensor = pred_vol_tensor[plot_slice_idx]\n",
    "                    \n",
    "            #         # Calculate Smooth L1 Loss for the current slice\n",
    "            #         import torch.nn.functional as F\n",
    "            #         loss = F.smooth_l1_loss(pred_slice_tensor, gt_slice_tensor, reduction='mean').item()\n",
    "\n",
    "            #         # Keep track of the top 5 worst samples (highest loss)\n",
    "            #         if len(worst_samples_data) < WORST_SAMPLES_TO_PLOT:\n",
    "            #             worst_samples_data.append((loss, pred_slice_tensor.cpu().numpy(), i))\n",
    "            #         else:\n",
    "            #             # Find the sample with the minimum loss currently in our list\n",
    "            #             min_loss_in_list = min(worst_samples_data, key=lambda x: x[0])\n",
    "            #             if loss > min_loss_in_list[0]:\n",
    "            #                 # If current sample is worse, replace the \"best of the worst\"\n",
    "            #                 worst_samples_data.remove(min_loss_in_list)\n",
    "            #                 worst_samples_data.append((loss, pred_slice_tensor.cpu().numpy(), i))\n",
    "\n",
    "            # # Sort the final list from worst to best for plotting\n",
    "            # worst_samples_data.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            # # Call the new plotting function with the results\n",
    "            # if worst_samples_data:\n",
    "            #     plot_worst_samples_comparison(\n",
    "            #         ground_truth=gt_slice_np,\n",
    "            #         mean_pred=mean_pred_slice,\n",
    "            #         worst_samples_data=worst_samples_data,\n",
    "            #         model_name=model_name,\n",
    "            #         scan_name=scan_name,\n",
    "            #         slice_idx=plot_slice_idx,\n",
    "            #         tumor_coords_xy=tumor_xy\n",
    "            #     )\n",
    "\n",
    "        # --- Clean up memory ---\n",
    "        del gt_volume, gt_volume_np, mean_pred_vol, uncertainty_map_vol, errors_vol\n",
    "        gc.collect()\n",
    "        \n",
    "    all_results.extend(scan_results)\n",
    "\n",
    "# Convert results to a pandas DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\n\\nâœ… Analysis complete for all models and scans.\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb60e045",
   "metadata": {},
   "source": [
    "### Summary/Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff11ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_csv('BBB_results_val.csv')\n",
    "\n",
    "# Only aggregate numeric columns\n",
    "numeric_cols = results_df.select_dtypes(include=[np.number]).columns\n",
    "summary = results_df.groupby('model_name')[numeric_cols].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# save results_df and summary to CSV files\n",
    "results_df.to_csv('BBB_results_val.csv', index=False)\n",
    "summary.to_csv('BBB_summary_val.csv', index=False)\n",
    "\n",
    "# Prepare summary_display with formatted mean Â± std for each metric\n",
    "summary_display = pd.DataFrame()\n",
    "summary_display['model_name'] = summary['model_name']\n",
    "for col in numeric_cols:\n",
    "    mean_col = (col, 'mean')\n",
    "    std_col = (col, 'std')\n",
    "    summary_display[col] = summary[mean_col].map('{:.4f}'.format) + ' Â± ' + summary[std_col].map('{:.4f}'.format)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(\"               Model Comparison Summary\")\n",
    "print(\"=======================================================\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(summary_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad6420",
   "metadata": {},
   "source": [
    "### Compare results across models/ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa95f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the trends of image quality and uncrtainty metrics over ensemble size\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.read_csv('MCdropout_30_summary_val.csv')\n",
    "\n",
    "# Now plot the trends of image quality and uncertainty metrics over ensemble size\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Note that columns come in pairs: mean and std like this \"mean_ssim\", \"mean_ssim.1\" and the second one is the std\n",
    "# and we also need to skip the first row after the headers\n",
    "columns = results_df.columns[1:]  # Skip the first column which is 'model_name'\n",
    "# delete the first row after the headers\n",
    "results_df = results_df.iloc[1:]  # Skip the first row after the headers\n",
    "ensemble_sizes = results_df['model_name'].str.extract(r'\\((\\d+)\\)').astype(int).values.flatten()\n",
    "# Plot each metric on a separate plot\n",
    "# Group columns by metric type\n",
    "ssim_cols = [col for col in columns if 'ssim' in col and not col.endswith('.1')]\n",
    "psnr_cols = [col for col in columns if 'psnr' in col and not col.endswith('.1')]\n",
    "mae_cols = [col for col in columns if 'mae' in col and not col.endswith('.1')]\n",
    "mse_cols = [col for col in columns if 'mse' in col and not col.endswith('.1')]\n",
    "ause_cols = [col for col in columns if 'ause' in col and not col.endswith('.1')]\n",
    "corr_cols = [col for col in columns if 'corr' in col and not col.endswith('.1')]\n",
    "\n",
    "def plot_metrics(metric_cols, title):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in metric_cols:\n",
    "        metric_mean = results_df[col].astype(float)\n",
    "        metric_std = results_df[f\"{col}.1\"].astype(float)\n",
    "        plt.errorbar(ensemble_sizes, metric_mean, yerr=metric_std, label=col, fmt='-o', capsize=5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Ensemble Size')\n",
    "    plt.xticks(ensemble_sizes)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot grouped metrics\n",
    "if ssim_cols:\n",
    "    plot_metrics(ssim_cols, 'SSIM Metrics Over Ensemble Size')\n",
    "if psnr_cols:\n",
    "    plot_metrics(psnr_cols, 'PSNR Metrics Over Ensemble Size')\n",
    "if mae_cols:\n",
    "    plot_metrics(mae_cols, 'MAE Metrics Over Ensemble Size')\n",
    "if mse_cols:\n",
    "    plot_metrics(mse_cols, 'MSE Metrics Over Ensemble Size')\n",
    "if ause_cols:\n",
    "    plot_metrics(ause_cols, 'AUSE Metrics Over Ensemble Size')\n",
    "if corr_cols:\n",
    "    plot_metrics(corr_cols, 'Correlation Metrics Over Ensemble Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32642ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of CSVs to compare\n",
    "csv_files = [\n",
    "    # 'MCdropout_15_summary_val.csv',\n",
    "    'MCdropout_30_summary_val.csv',\n",
    "    # 'MCdropout_50_summary_val.csv',\n",
    "    'ensemble_summary_val.csv',\n",
    "    # 'MCdropout_30_summary_test.csv',\n",
    "    # 'ensemble_summary_test.csv',\n",
    "    # Add more CSV file paths here\n",
    "]\n",
    "\n",
    "results_dfs = [pd.read_csv(csv) for csv in csv_files]\n",
    "labels = [csv.split('.')[0] for csv in csv_files]\n",
    "colors = plt.cm.tab10.colors  # Up to 10 distinct colors\n",
    "\n",
    "def get_metric_cols(columns, metric):\n",
    "    return [col for col in columns if metric in col and not col.endswith('.1')]\n",
    "\n",
    "metrics = ['ssim', 'psnr', 'mae', 'mse', 'ause', 'corr']\n",
    "metric_titles = {\n",
    "    'ssim': 'SSIM Metrics Over Ensemble Size',\n",
    "    'psnr': 'PSNR Metrics Over Ensemble Size',\n",
    "    'mae': 'MAE Metrics Over Ensemble Size',\n",
    "    'mse': 'MSE Metrics Over Ensemble Size',\n",
    "    'ause': 'AUSE Metrics Over Ensemble Size',\n",
    "    'corr': 'Correlation Metrics Over Ensemble Size'\n",
    "}\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # Find the minimum max ensemble size across all CSVs\n",
    "    max_x = min([\n",
    "        results_df.iloc[1:]['model_name'].str.extract(r'\\((\\d+)\\)').astype(int).values.flatten().max()\n",
    "        for results_df in results_dfs\n",
    "    ])\n",
    "    for i, results_df in enumerate(results_dfs):\n",
    "        columns = results_df.columns[1:]\n",
    "        results_df = results_df.iloc[1:]\n",
    "        ensemble_sizes = results_df['model_name'].str.extract(r'\\((\\d+)\\)').astype(int).values.flatten()\n",
    "        metric_cols = get_metric_cols(columns, metric)\n",
    "        for j, col in enumerate(metric_cols):\n",
    "            metric_mean = results_df[col].astype(float).values\n",
    "            # Sort by ensemble size\n",
    "            sorted_idx = np.argsort(ensemble_sizes)\n",
    "            sorted_ensemble_sizes = ensemble_sizes[sorted_idx]\n",
    "            sorted_metric_mean = metric_mean[sorted_idx]\n",
    "            linestyle = '-' if j == 0 else '--'\n",
    "            label = f\"{labels[i]}: {col}\"\n",
    "            # Only plot up to max_x\n",
    "            mask = sorted_ensemble_sizes <= max_x\n",
    "            plt.plot(\n",
    "                sorted_ensemble_sizes[mask], sorted_metric_mean[mask],\n",
    "                label=label,\n",
    "                color=colors[i % len(colors)], linestyle=linestyle, marker='o'\n",
    "            )\n",
    "    plt.title(metric_titles[metric])\n",
    "    plt.xlabel('Ensemble Size')\n",
    "    plt.xlim(left=None, right=max_x)\n",
    "    plt.xticks(np.arange(sorted_ensemble_sizes.min(), max_x + 1))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287da0be",
   "metadata": {},
   "source": [
    "## Post-calibration processing & results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calibration Analysis (Train on Validation, Evaluate on Test)\n",
    "# This cell performs a separate analysis focused on uncertainty calibration.\n",
    "# 1. It trains calibration models (Platt, Isotonic) for each model on the VALIDATION data.\n",
    "# 2. It then evaluates the uncalibrated and calibrated uncertainties on the TEST data.\n",
    "\n",
    "# This list will store dictionaries of post-calibration results\n",
    "calibration_results = []\n",
    "\n",
    "# Load the scan lists for validation and testing, assuming they don't change\n",
    "all_scans, _ = read_scans_agg_file(SCANS_AGG_FILE)\n",
    "validation_scans = all_scans['VALIDATION']\n",
    "test_scans = all_scans['TEST']\n",
    "\n",
    "for model_config in MODELS_TO_ANALYZE:\n",
    "    model_name = model_config['name']\n",
    "    domain = model_config['domain']\n",
    "    \n",
    "    # ======================================================================\n",
    "    # 1. CALIBRATION TRAINING on the VALIDATION set\n",
    "    # ======================================================================\n",
    "    print(f\"\\n--- Training calibration for model: {model_name} ---\")\n",
    "    \n",
    "    all_val_errors = []\n",
    "    all_val_uncertainties = []\n",
    "\n",
    "    for scan_info in tqdm(validation_scans, desc=f\"Gathering validation data for {model_name}\", leave=False):\n",
    "        # Load GT and calculate uncalibrated predictions for this validation scan\n",
    "        gt_volume = load_ground_truth(FILES, scan_info, domain)\n",
    "        # The 'calculate_volume_metrics_2_pass' is efficient for getting mean and std dev\n",
    "        _, mean_pred_vol, uncertainty_map_vol = calculate_volume_metrics_2_pass(\n",
    "            FILES, model_config, scan_info, gt_volume.to(DEVICE), DEVICE\n",
    "        )\n",
    "        \n",
    "        # Move results to CPU and convert to numpy for calibration training\n",
    "        gt_volume_np = gt_volume.cpu().numpy()\n",
    "        del gt_volume\n",
    "        mean_pred_vol_np = mean_pred_vol.cpu().numpy()\n",
    "        del mean_pred_vol\n",
    "        uncertainty_map_vol_np = uncertainty_map_vol.cpu().numpy()\n",
    "        del uncertainty_map_vol\n",
    "\n",
    "        errors_vol = np.abs(gt_volume_np - mean_pred_vol_np)\n",
    "        \n",
    "        all_val_errors.append(errors_vol.flatten())\n",
    "        all_val_uncertainties.append(uncertainty_map_vol_np.flatten())\n",
    "\n",
    "    # Consolidate all validation data into single arrays\n",
    "    val_errors_full = np.concatenate(all_val_errors)\n",
    "    val_uncertainties_full = np.concatenate(all_val_uncertainties)\n",
    "    \n",
    "    # Train the calibration models using the full validation dataset\n",
    "    print(\"Calculating Platt Scaler...\")\n",
    "    platt_scaler = calculate_platt_scaler(val_errors_full, val_uncertainties_full)\n",
    "    print(f\"Calibration complete for {model_name}. Platt Scaler T={platt_scaler:.4f}\")\n",
    "    print(\"Training Isotonic Regression model...\")\n",
    "    isotonic_model = train_isotonic_regression(val_uncertainties_full, val_errors_full, DEVICE)\n",
    "    print(\"Done.\")\n",
    "        \n",
    "    # Clean up memory from the training phase\n",
    "    del all_val_errors, all_val_uncertainties, val_errors_full, val_uncertainties_full\n",
    "    gc.collect()\n",
    "\n",
    "    # ======================================================================\n",
    "    # 2. FINAL EVALUATION on the TEST set\n",
    "    # ======================================================================\n",
    "    print(f\"\\n--- Evaluating model: {model_name} on the TEST set ---\")\n",
    "    \n",
    "    for scan_info in tqdm(test_scans, desc=f\"Analyzing Test Scans for {model_name}\"):\n",
    "        patient, scan, _ = scan_info\n",
    "        scan_name = f\"p{patient}_{scan}\"\n",
    "        \n",
    "        # --- Data Loading & Base Uncalibrated Calculation ---\n",
    "        gt_volume = load_ground_truth(FILES, scan_info, domain)\n",
    "        gt_volume_np = gt_volume.cpu().numpy()\n",
    "        \n",
    "        _, mean_pred_vol, uncal_uncertainty_map = calculate_volume_metrics_2_pass(\n",
    "            FILES, model_config, scan_info, gt_volume.to(DEVICE), DEVICE\n",
    "        )\n",
    "        del gt_volume\n",
    "        \n",
    "        mean_pred_vol_np = mean_pred_vol.cpu().numpy()\n",
    "        del mean_pred_vol\n",
    "        uncal_uncertainty_map_np = uncal_uncertainty_map.cpu().numpy()\n",
    "        del uncal_uncertainty_map\n",
    "        \n",
    "        # --- Apply Calibrations ---\n",
    "        print(\"Applying calibrations...\")\n",
    "        platt_uncertainty_map_np = uncal_uncertainty_map_np * platt_scaler\n",
    "        iso_uncertainty_map_np = isotonic_model(uncal_uncertainty_map_np)\n",
    "\n",
    "        # --- Store results for this scan ---\n",
    "        scan_result = {'model_name': model_name, 'scan_name': scan_name}\n",
    "        \n",
    "        # --- Calculate and Store All Metrics (Uncalibrated, Platt, Isotonic) ---\n",
    "        calibrations = {\n",
    "            'platt': platt_uncertainty_map_np,\n",
    "            'iso': iso_uncertainty_map_np\n",
    "        }\n",
    "\n",
    "        # We need the error map for AUSE calculation\n",
    "        errors_vol = np.abs(gt_volume_np - mean_pred_vol_np)\n",
    "        \n",
    "        print(\"Evaluating metrics for each calibration...\")\n",
    "        for cal_name, uncertainty_map in calibrations.items():\n",
    "            print(\"Calculating metrics for calibration:\", cal_name)\n",
    "\n",
    "            print(\"Calculating AUSE...\")\n",
    "            ause_val = calculate_ause_sparsification(uncertainty_map, errors_vol)\n",
    "            scan_result[f'ause_{cal_name}'] = ause_val\n",
    "\n",
    "            print(\"Calculating ECE...\")\n",
    "            for n_bins in [10, 20, 50]:\n",
    "                eces = calculate_all_eces(gt_volume_np, mean_pred_vol_np, uncertainty_map, n_bins)\n",
    "                for key, val in eces.items():\n",
    "                    scan_result[f'{key}_{n_bins}bins_{cal_name}'] = val\n",
    "\n",
    "            print(\"Calculating ENCE...\")         \n",
    "            for n_bins in [10, 20, 50]:\n",
    "                scan_result[f'ence_{n_bins}bins_{cal_name}'] = calculate_ence(gt_volume_np, mean_pred_vol_np, uncertainty_map, DEVICE, n_bins)\n",
    "            \n",
    "            print(\"Calculating NLL...\")\n",
    "            scan_result[f'nll_{cal_name}'] = calculate_nll(gt_volume_np, mean_pred_vol_np, uncertainty_map)\n",
    "            \n",
    "            print(\"Calculating MPIW...\")\n",
    "            mpiws = calculate_mpiw(uncertainty_map, confidence_levels=[0.68, 0.95])\n",
    "            for key, val in mpiws.items():\n",
    "                scan_result[f'{key}_{cal_name}'] = val\n",
    "            print(\"Done with metrics for\", cal_name)\n",
    "\n",
    "        calibration_results.append(scan_result)\n",
    "\n",
    "        # Platt Scaling Plots\n",
    "        print(f\"--- Generating combined calibration plots for {model_name} on {scan_name} ---\")\n",
    "        plot_combined_calibration_curves(\n",
    "            ground_truth=gt_volume_np,\n",
    "            mean_pred=mean_pred_vol_np,\n",
    "            platt_uncertainty_map=platt_uncertainty_map_np,\n",
    "            iso_uncertainty_map=iso_uncertainty_map_np,\n",
    "            model_name=model_name,\n",
    "            scan_name=scan_name,\n",
    "            n_bins=20\n",
    "        )\n",
    "        \n",
    "        # --- Clean up memory ---\n",
    "        del gt_volume_np, mean_pred_vol_np, uncal_uncertainty_map_np, errors_vol\n",
    "        del platt_uncertainty_map_np, iso_uncertainty_map_np\n",
    "        gc.collect()\n",
    "\n",
    "# Convert results to a pandas DataFrame for easier analysis\n",
    "calibration_results_df = pd.DataFrame(calibration_results)\n",
    "\n",
    "print(\"\\n\\nâœ… Calibration analysis complete.\")\n",
    "calibration_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276442bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aggregate Results Summary\n",
    "\n",
    "# Select only the numeric columns for aggregation\n",
    "numeric_cols = calibration_results_df.select_dtypes(include=[np.number]).columns\n",
    "summary = calibration_results_df.groupby('model_name')[numeric_cols].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Save results_df and summary to CSV files\n",
    "calibration_results_df.to_csv('MCdropout_30_results_calibration.csv', index=False)\n",
    "summary.to_csv('MCdropout_30_summary_calibration.csv', index=False)\n",
    "\n",
    "# Prepare a display DataFrame with formatted 'mean Â± std' strings\n",
    "summary_display = pd.DataFrame()\n",
    "summary_display['model_name'] = summary['model_name']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    mean_col = (col, 'mean')\n",
    "    std_col = (col, 'std')\n",
    "    # Format the string, handling potential NaN values in std dev\n",
    "    summary_display[col] = summary[mean_col].map('{:.4f}'.format) + ' Â± ' + summary[std_col].map('{:.4f}'.format)\n",
    "\n",
    "print(\"\\\\n\\\\n=======================================================\")\n",
    "print(\"               Model Comparison Summary\")\n",
    "print(\"=======================================================\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(summary_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf98081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Combine multiple summary DataFrames from different CSVs\n",
    "import pandas as pd\n",
    "\n",
    "# List of summary CSV files to compare\n",
    "summary_csv_files = [\n",
    "    'MCdropout_30_summary_calibration.csv',\n",
    "    'ensemble_summary_calibration.csv',\n",
    "    # Add more summary CSV file paths here\n",
    "]\n",
    "columns_to_include = [\n",
    "    # 'ece_weighted_abs_20bins_platt',\n",
    "    'ece_unweighted_abs_20bins_platt',\n",
    "    'ence_20bins_platt',\n",
    "    'nll_platt',\n",
    "    # 'mpiw_68_platt',\n",
    "    'mpiw_95_platt',\n",
    "    # 'ece_weighted_abs_20bins_iso',\n",
    "    'ece_unweighted_abs_20bins_iso',\n",
    "    'ence_20bins_iso',\n",
    "    'nll_iso',\n",
    "    # 'mpiw_68_iso',\n",
    "    'mpiw_95_iso',\n",
    "]\n",
    "\n",
    "# Load each summary CSV as a DataFrame and append to a list\n",
    "summaries = [pd.read_csv(csv) for csv in summary_csv_files]\n",
    "\n",
    "for i in range(len(summaries)):\n",
    "    # Discard the first non-header row\n",
    "    summaries[i] = summaries[i].iloc[1:]\n",
    "\n",
    "    # Convert all columns to numeric (except 'model_name')\n",
    "    for col in summaries[i].columns:\n",
    "        if col != 'model_name':\n",
    "            summaries[i][col] = pd.to_numeric(summaries[i][col], errors='raise')\n",
    "\n",
    "# Concatenate all summary DataFrames row-wise\n",
    "if summaries:\n",
    "    combined_summary = pd.concat(summaries, ignore_index=True)\n",
    "    print(\"\\n\\n=======================================================\")\n",
    "    print(\"         Combined Model Calibration Summaries\")\n",
    "    print(\"=======================================================\")\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    # Display the dataframe with values formatted as 'mean Â± std'\n",
    "    for col in columns_to_include:\n",
    "        mean_col = col\n",
    "        std_col = col + '.1'\n",
    "        combined_summary[col] = combined_summary[mean_col].map('{:.4f}'.format) + ' Â± ' + combined_summary[std_col].map('{:.4f}'.format)\n",
    "    \n",
    "    # Keep only the relevant columns\n",
    "    combined_summary = combined_summary[['model_name'] + columns_to_include]\n",
    "    display(combined_summary)\n",
    "else:\n",
    "    print(\"No summary CSV files provided.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
