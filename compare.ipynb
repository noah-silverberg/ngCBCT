{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b5c8d3",
   "metadata": {},
   "source": [
    "### Setup/Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0320969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from pipeline.paths import Directories, Files\n",
    "from pipeline.utils import read_scans_agg_file\n",
    "import scipy.stats\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec7764",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eaa8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASE = \"7\"\n",
    "DATA_VERSION = \"14\"\n",
    "SPLIT_TO_ANALYZE = 'VALIDATION'  # Options: 'TRAIN', 'VALIDATION', 'TEST'\n",
    "\n",
    "MODELS_TO_ANALYZE = [\n",
    "    {\n",
    "        'name': 'MC Dropout 15%',\n",
    "        'type': 'stochastic',\n",
    "        'domain': 'IMAG',\n",
    "        'model_version_root': 'MK7_MCDROPOUT_15_pct',\n",
    "        'count': 50,\n",
    "    },\n",
    "    {\n",
    "        'name': 'MC Dropout 30%',\n",
    "        'type': 'stochastic',\n",
    "        'domain': 'IMAG',\n",
    "        'model_version_root': 'MK7_MCDROPOUT_30_pct',\n",
    "        'count': 50,\n",
    "    },\n",
    "    {\n",
    "        'name': 'MC Dropout 50%',\n",
    "        'type': 'stochastic',\n",
    "        'domain': 'IMAG',\n",
    "        'model_version_root': 'MK7_MCDROPOUT_50_pct',\n",
    "        'count': 50,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Path to the .pt file containing tumor locations.\n",
    "# This is a 5D tensor [patient, scan, (x, y, z)]\n",
    "WORK_ROOT = \"D:/NoahSilverberg/ngCBCT\"\n",
    "TUMOR_LOCATIONS_FILE = 'D:/NoahSilverberg/ngCBCT/3D_recon/tumor_location_FF.pt'\n",
    "\n",
    "# --- Advanced Config ---\n",
    "SCANS_AGG_FILE = 'scans_to_agg_FF.txt'\n",
    "SSIM_KWARGS = {\"k1\": 0.03, \"k2\": 0.06, \"kernel_size\": 11}\n",
    "\n",
    "# --- Setup ---\n",
    "# Create Directories and Files objects\n",
    "phase_dataver_dir = os.path.join(WORK_ROOT, f\"phase{PHASE}\", f\"DS{DATA_VERSION}\")\n",
    "DIRECTORIES = Directories(\n",
    "    # mat_projections_dir=os.path.join(\"H:\\Public/Noah\", \"mat\"),\n",
    "    # pt_projections_dir=os.path.join(\"H:\\Public/Noah\", \"prj_pt\"),\n",
    "    # projections_aggregate_dir=os.path.join(PHASE_DATAVER_DIR, \"aggregates\", \"projections\"),\n",
    "    # projections_model_dir=os.path.join('H:\\Public/Noah/phase7/DS14', \"models\", \"projections\"),\n",
    "    # projections_results_dir=os.path.join('H:\\Public/Noah/phase7/DS14', \"results\", \"projections\"),\n",
    "    # projections_gated_dir=os.path.join(\"H:\\Public/Noah\", \"gated\", \"prj_mat\"),\n",
    "    # reconstructions_dir=os.path.join('H:\\Public/Noah/phase7/DS14', \"reconstructions\"),\n",
    "    reconstructions_gated_dir=os.path.join(\"H:\\Public/Noah\", \"gated\", \"fdk_recon\"),\n",
    "    # images_aggregate_dir=os.path.join(phase_dataver_dir, \"aggregates\", \"images\"),\n",
    "    # images_model_dir=os.path.join('H:\\Public/Noah/phase7/DS14', \"models\", \"images\"),\n",
    "    images_results_dir=os.path.join('H:\\Public/Noah/phase7/DS14', \"results\", \"images\"),\n",
    "    error_results_dir= os.path.join('H:\\Public/Noah/phase7/DS14', \"results\", \"error_results\"),\n",
    ")\n",
    "FILES = Files(DIRECTORIES)\n",
    "\n",
    "# Load the list of scans\n",
    "scans_agg, scan_type_agg = read_scans_agg_file(SCANS_AGG_FILE)\n",
    "analysis_scans = scans_agg[SPLIT_TO_ANALYZE]\n",
    "\n",
    "# Load tumor locations\n",
    "tumor_locations = torch.load(TUMOR_LOCATIONS_FILE, weights_only=False)\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE} named '{torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}'\")\n",
    "\n",
    "print(f\"\\nConfiguration loaded.\")\n",
    "print(f\"Analyzing {len(analysis_scans)} scans from the '{SPLIT_TO_ANALYZE}' split.\")\n",
    "print(f\"Found {len(MODELS_TO_ANALYZE)} model(s) to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9357ce85",
   "metadata": {},
   "source": [
    "## Function definitions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114d02c",
   "metadata": {},
   "source": [
    "### Data loading/prep functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b635426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth(files_obj: Files, scan_info, domain, slice_idx=None):\n",
    "    \"\"\"\n",
    "    Loads the ground truth data for a given scan and domain.\n",
    "    \"\"\"\n",
    "    patient, scan, scan_type = scan_info\n",
    "    \n",
    "    if domain == 'PROJ':\n",
    "        gt_path = files_obj.get_projections_results_filepath('fdk', patient, scan, scan_type, gated=True)\n",
    "        data = torch.from_numpy(scipy.io.loadmat(gt_path)['prj']).detach().permute(1, 0, 2)\n",
    "    elif domain == 'FDK':\n",
    "        gt_path = files_obj.get_recon_filepath(\"fdk\", patient, scan, scan_type, gated=True, ensure_exists=False)\n",
    "        data = torch.load(gt_path).detach()\n",
    "        data = data[20:-20, :, :]\n",
    "        if scan_type == \"FF\":\n",
    "            data = data[:, 128:-128, 128:-128]\n",
    "        data = 25. * torch.clip(data, min=0.0, max=0.04)\n",
    "    elif domain == 'IMAG':\n",
    "        # The ground truth for the IMAG domain is the FDK of the gated projection\n",
    "        gt_path = files_obj.get_recon_filepath(\"fdk\", patient, scan, scan_type, gated=True, ensure_exists=False)\n",
    "        data = torch.load(gt_path).detach()\n",
    "        data = data[20:-20, :, :]\n",
    "        if scan_type == \"FF\":\n",
    "            data = data[:, 128:-128, 128:-128]\n",
    "        data = 25. * torch.clip(data, min=0.0, max=0.04)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown domain: {domain}\")\n",
    "\n",
    "    if slice_idx is not None and data.ndim == 3:\n",
    "        return data[slice_idx]\n",
    "    return data\n",
    "\n",
    "def load_predictions(files_obj: Files, model_config, scan_info, slice_idx=None):\n",
    "    \"\"\"\n",
    "    Loads all predictions for a given model, scan, and domain.\n",
    "    \"\"\"\n",
    "    patient, scan, scan_type = scan_info\n",
    "    domain = model_config['domain']\n",
    "    root = model_config['model_version_root']\n",
    "    count = model_config['count']\n",
    "    model_type = model_config['type']\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"Loading {count} predictions for {model_config['name']}...\")\n",
    "    \n",
    "    for i in tqdm(range(count), desc=\"Loading predictions\", leave=False):\n",
    "        passthrough_num = None\n",
    "        model_version = root\n",
    "\n",
    "        if model_type == 'ensemble':\n",
    "            model_version = f\"{root}_{i+1:02d}\"\n",
    "        elif model_type == 'stochastic':\n",
    "            passthrough_num = i\n",
    "\n",
    "        if domain == 'PROJ':\n",
    "            pred_path = files_obj.get_projections_results_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            pred = torch.from_numpy(scipy.io.loadmat(pred_path)['prj']).detach().permute(1, 0, 2)\n",
    "        elif domain == 'FDK':\n",
    "            pred_path = files_obj.get_recon_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            pred = torch.load(pred_path).detach()\n",
    "            pred = pred[20:-20, :, :]\n",
    "            if scan_type == \"FF\":\n",
    "                pred = pred[:, 128:-128, 128:-128]\n",
    "            pred = 25. * torch.clip(pred, min=0.0, max=0.04)\n",
    "        elif domain == 'IMAG':\n",
    "            # This assumes the results are saved with the ID model version name\n",
    "            pred_path = files_obj.get_images_results_filepath(model_version, patient, scan, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            pred = torch.load(pred_path).detach()\n",
    "            pred = torch.squeeze(pred, dim=1)\n",
    "            pred = torch.permute(pred, (0, 2, 1))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown domain: {domain}\")\n",
    "            \n",
    "        predictions.append(pred)\n",
    "\n",
    "    predictions_tensor = torch.stack(predictions)\n",
    "    \n",
    "    if slice_idx is not None and predictions_tensor.ndim == 4:\n",
    "        return predictions_tensor[:, slice_idx, :, :]\n",
    "        \n",
    "    return predictions_tensor\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3cbb74",
   "metadata": {},
   "source": [
    "### Metric calculation functions: Image quality and pre-calibration uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723009ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ause_sparsification(uncertainty, errors):\n",
    "    \"\"\"\n",
    "    Calculates the Area Under the Sparsification Error curve (AUSE) efficiently.\n",
    "    \"\"\"\n",
    "    uncertainty_flat = uncertainty.flatten()\n",
    "    errors_flat = errors.flatten()\n",
    "    \n",
    "    # Normalize by overall MAE so the curve starts at 1\n",
    "    overall_mae = np.mean(errors_flat)\n",
    "    \n",
    "    def get_sparsification_curve_fast(sorted_errs):\n",
    "        n_pixels = len(sorted_errs)\n",
    "        cumulative_errors = np.cumsum(sorted_errs)\n",
    "        total_error_sum = cumulative_errors[-1]\n",
    "        sum_errors_removed = np.insert(cumulative_errors[:-1], 0, 0)\n",
    "        sum_errors_remaining = total_error_sum - sum_errors_removed\n",
    "        n_remaining = np.arange(n_pixels, 0, -1)\n",
    "        curve = sum_errors_remaining / n_remaining\n",
    "        if overall_mae > 0:\n",
    "            curve = curve / overall_mae # Normalize the curve\n",
    "        return curve\n",
    "\n",
    "    # Move arrays to GPU using torch for sorting\n",
    "    uncertainty_tensor = torch.from_numpy(uncertainty_flat).cuda()\n",
    "    errors_tensor = torch.from_numpy(errors_flat).cuda()\n",
    "\n",
    "    # Model curve (sorted by uncertainty)\n",
    "    model_sorted_indices = torch.argsort(uncertainty_tensor, descending=True)\n",
    "    model_sorted_errors = errors_tensor[model_sorted_indices].cpu().numpy()\n",
    "    model_curve = get_sparsification_curve_fast(model_sorted_errors)\n",
    "\n",
    "    # Oracle curve (sorted by error)\n",
    "    oracle_sorted_errors = torch.sort(errors_tensor, descending=True)[0].cpu().numpy()\n",
    "    oracle_curve = get_sparsification_curve_fast(oracle_sorted_errors)\n",
    "    \n",
    "    # The AUSE is the area between the two normalized curves\n",
    "    ause = np.mean(np.abs(model_curve - oracle_curve))\n",
    "    return ause\n",
    "\n",
    "def calculate_spearman_correlation(uncertainty, errors, device):\n",
    "    \"\"\"\n",
    "    Calculates the Spearman's Rank Correlation Coefficient between\n",
    "    the uncertainty and the absolute error.\n",
    "    \"\"\"\n",
    "    uncertainty_flat = torch.from_numpy(uncertainty.flatten()).to(device)\n",
    "    errors_flat = torch.from_numpy(errors.flatten()).to(device)\n",
    "    \n",
    "    # obtain ranks\n",
    "    xr = torch.argsort(torch.argsort(uncertainty_flat)).float()\n",
    "    yr = torch.argsort(torch.argsort(errors_flat)).float()\n",
    "\n",
    "    # demean\n",
    "    xr = xr - xr.mean()\n",
    "    yr = yr - yr.mean()\n",
    "\n",
    "    # compute covariance and norms\n",
    "    cov = (xr * yr).sum() / (xr.numel() - 1)\n",
    "    rho = cov / (xr.std(unbiased=True) * yr.std(unbiased=True))\n",
    "    \n",
    "    return rho.item()\n",
    "\n",
    "def calculate_pearson_correlation(uncertainty, errors, device):\n",
    "    \"\"\"\n",
    "    Calculates the Pearson Correlation Coefficient between\n",
    "    the uncertainty and the absolute error on the GPU.\n",
    "    \"\"\"\n",
    "    uncertainty_flat = torch.from_numpy(uncertainty.flatten()).to(device)\n",
    "    errors_flat = torch.from_numpy(errors.flatten()).to(device)\n",
    "\n",
    "    # Demean\n",
    "    uncertainty_demeaned = uncertainty_flat - uncertainty_flat.mean()\n",
    "    errors_demeaned = errors_flat - errors_flat.mean()\n",
    "\n",
    "    # Compute covariance and standard deviations using N-1 for unbiased estimate\n",
    "    n = uncertainty_flat.numel()\n",
    "    if n < 2:\n",
    "        return 0.0 # Not enough data to compute correlation\n",
    "        \n",
    "    cov = (uncertainty_demeaned * errors_demeaned).sum() / (n - 1)\n",
    "    std_uncertainty = torch.std(uncertainty_flat, unbiased=True)\n",
    "    std_errors = torch.std(errors_flat, unbiased=True)\n",
    "\n",
    "    # Compute Pearson correlation\n",
    "    # Add epsilon to avoid division by zero\n",
    "    rho = cov / (std_uncertainty * std_errors + 1e-6)\n",
    "\n",
    "    return rho.item()\n",
    "\n",
    "\n",
    "import torchmetrics\n",
    "import torchmetrics.image\n",
    "\n",
    "def calculate_volume_metrics_2_pass(files_obj, model_config, scan_info, gt_volume, device):\n",
    "    \"\"\"\n",
    "    Calculates stats and metrics using a two-pass algorithm for variance for improved stability.\n",
    "    Also includes a robust PSNR calculation that handles infinite values.\n",
    "    Pass 1: Calculate the mean of all predictions.\n",
    "    Pass 2: Calculate variance and other metrics using the pre-calculated mean.\n",
    "    \"\"\"\n",
    "    n_samples = model_config['count']\n",
    "    gt_volume = gt_volume.to(device) # Ensure GT is on the correct device\n",
    "\n",
    "    def prediction_generator():\n",
    "        # This generator yields tensors directly on the GPU\n",
    "        patient, scan, scan_type = scan_info\n",
    "        domain = model_config['domain']\n",
    "        root = model_config['model_version_root']\n",
    "        model_type = model_config['type']\n",
    "        for i in range(n_samples):\n",
    "            passthrough_num = None\n",
    "            model_version = root\n",
    "            if model_type == 'ensemble': model_version = f\"{root}_{i+1:02d}\"\n",
    "            elif model_type == 'stochastic': passthrough_num = i\n",
    "\n",
    "            if domain == 'PROJ':\n",
    "                pred_path = files_obj.get_projections_results_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                pred = torch.from_numpy(scipy.io.loadmat(pred_path)['prj']).detach().permute(1, 0, 2)\n",
    "            elif domain == 'FDK':\n",
    "                pred_path = files_obj.get_recon_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                pred = torch.load(pred_path).detach()\n",
    "                pred = pred[20:-20, :, :]\n",
    "                if scan_type == \"FF\":\n",
    "                    pred = pred[:, 128:-128, 128:-128]\n",
    "                pred = 25. * torch.clip(pred, min=0.0, max=0.04)\n",
    "            elif domain == 'IMAG':\n",
    "                pred_path = files_obj.get_images_results_filepath(model_version, patient, scan, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                pred = torch.load(pred_path).detach()\n",
    "                pred = torch.squeeze(pred, dim=1)\n",
    "                pred = torch.permute(pred, (0, 2, 1))\n",
    "            yield pred.to(device)\n",
    "\n",
    "    # --- Pass 1: Calculate Mean ---\n",
    "    print(\"Pass 1: Calculating mean prediction...\")\n",
    "    mean_volume = torch.zeros_like(gt_volume)\n",
    "    # Using a simple sum and divide for the mean\n",
    "    for pred_volume in tqdm(prediction_generator(), total=n_samples, desc=\"Pass 1/2 (Mean)\", leave=False):\n",
    "        mean_volume += pred_volume\n",
    "    mean_volume /= n_samples\n",
    "\n",
    "    # --- Pass 2: Calculate Variance and Metrics ---\n",
    "    print(\"Pass 2: Calculating variance and metrics...\")\n",
    "    sum_sq_diff_volume = torch.zeros_like(gt_volume)\n",
    "    sample_avg_ssims, sample_avg_psnrs, sample_avg_mses, sample_avg_maes = [], [], [], []\n",
    "\n",
    "    # Initialize metrics on the specified device\n",
    "    data_range = 1.0\n",
    "    ssim_metric = torchmetrics.image.StructuralSimilarityIndexMeasure(data_range=data_range, **SSIM_KWARGS).to(device)\n",
    "\n",
    "    # PSNR metric that returns per-slice results to handle 'inf'\n",
    "    psnr_metric = torchmetrics.image.PeakSignalNoiseRatio(data_range=data_range, reduction='none').to(device)\n",
    "\n",
    "    for pred_volume in tqdm(prediction_generator(), total=n_samples, desc=\"Pass 2/2 (Var & Metrics)\", leave=False):\n",
    "        # Variance calculation\n",
    "        diff = pred_volume - mean_volume\n",
    "        sum_sq_diff_volume += diff * diff\n",
    "\n",
    "        # Per-sample metrics\n",
    "        if gt_volume.ndim > 2:\n",
    "            pred_vol_batch = pred_volume.unsqueeze(1)\n",
    "            gt_vol_batch = gt_volume.unsqueeze(1)\n",
    "            \n",
    "            sample_avg_ssims.append(ssim_metric(pred_vol_batch, gt_vol_batch).item())\n",
    "            sample_avg_mses.append(torch.mean((gt_volume - pred_volume)**2).item())\n",
    "            sample_avg_maes.append(torch.mean(torch.abs(gt_volume - pred_volume)).item())\n",
    "\n",
    "            # --- Robust PSNR Calculation ---\n",
    "            psnr_per_slice = psnr_metric(pred_vol_batch, gt_vol_batch)\n",
    "            finite_psnrs = psnr_per_slice[torch.isfinite(psnr_per_slice)] # Filter out inf values\n",
    "            if finite_psnrs.numel() > 0:\n",
    "                sample_avg_psnrs.append(torch.mean(finite_psnrs).item())\n",
    "            else:\n",
    "                # For debugging, print abs difference between GT and prediction\n",
    "                abs_diff = torch.sum(torch.abs(gt_volume - pred_volume))\n",
    "                print(f\"Absolute difference between GT and prediction: {abs_diff.item()}\")\n",
    "                # Raise error\n",
    "                raise ValueError(\"All slices in the prediction are perfect matches, leading to infinite PSNR values.\")\n",
    "\n",
    "    # Finalize variance and uncertainty\n",
    "    if n_samples > 1:\n",
    "        # Using n_samples for population standard deviation, as in the original code.\n",
    "        # For sample standard deviation, use (n_samples - 1).\n",
    "        variance_volume_map = sum_sq_diff_volume / n_samples\n",
    "        uncertainty_volume_map = torch.sqrt(variance_volume_map)\n",
    "    else:\n",
    "        uncertainty_volume_map = torch.zeros_like(mean_volume)\n",
    "\n",
    "    # --- Calculate metrics for the mean prediction ---\n",
    "    metrics = {}\n",
    "    if gt_volume.ndim > 2:\n",
    "        mean_vol_batch = mean_volume.unsqueeze(1)\n",
    "        gt_vol_batch = gt_volume.unsqueeze(1)\n",
    "        \n",
    "        metrics['mean_ssim'] = ssim_metric(mean_vol_batch, gt_vol_batch).item()\n",
    "        metrics['mean_mse'] = torch.mean((gt_volume - mean_volume)**2).item()\n",
    "        metrics['mean_mae'] = torch.mean(torch.abs(gt_volume - mean_volume)).item()\n",
    "\n",
    "        # Robust PSNR for the mean prediction\n",
    "        mean_psnr_per_slice = psnr_metric(mean_vol_batch, gt_vol_batch)\n",
    "        finite_mean_psnrs = mean_psnr_per_slice[torch.isfinite(mean_psnr_per_slice)]\n",
    "        if finite_mean_psnrs.numel() > 0:\n",
    "            metrics['mean_psnr'] = torch.mean(finite_mean_psnrs).item()\n",
    "        else:\n",
    "            # For debugging, print abs difference between GT and mean prediction\n",
    "            abs_diff = torch.sum(torch.abs(gt_volume - mean_volume))\n",
    "            print(f\"Absolute difference between GT and mean prediction: {abs_diff.item()}\")\n",
    "            # Raise error\n",
    "            raise ValueError(\"All slices in the mean prediction are perfect matches, leading to infinite PSNR values.\")\n",
    "\n",
    "    # --- Aggregate per-sample metrics ---\n",
    "    metrics['sample_avg_ssim'] = np.mean(sample_avg_ssims) if sample_avg_ssims else 0\n",
    "    metrics['sample_avg_psnr'] = np.mean(sample_avg_psnrs) if sample_avg_psnrs else 0\n",
    "    metrics['sample_avg_mse'] = np.mean(sample_avg_mses) if sample_avg_mses else 0\n",
    "    metrics['sample_avg_mae'] = np.mean(sample_avg_maes) if sample_avg_maes else 0\n",
    "    metrics['mean_std'] = torch.mean(uncertainty_volume_map).item()\n",
    "    metrics['rmv'] = torch.sqrt(torch.mean(uncertainty_volume_map**2)).item()\n",
    "\n",
    "    return metrics, mean_volume, uncertainty_volume_map\n",
    "\n",
    "def calculate_evidential_volume_metrics(files_obj, model_config, scan_info, gt_volume, device):\n",
    "    \"\"\"\n",
    "    Calculates stats and metrics for a single-pass evidential model.\n",
    "    \"\"\"\n",
    "    patient, scan, scan_type = scan_info\n",
    "    domain = model_config['domain']\n",
    "    root = model_config['model_version_root']\n",
    "    \n",
    "    if domain != 'IMAG':\n",
    "        raise NotImplementedError(\"Evidential regression is only implemented for the IMAG domain.\")\n",
    "\n",
    "    # Path to the dictionary of evidential outputs (passthrough_num is None for the final result)\n",
    "    pred_path = files_obj.get_images_results_filepath(root, patient, scan, passthrough_num=None, ensure_exists=False)\n",
    "    \n",
    "    # Load the dictionary of tensors\n",
    "    evidential_outputs = torch.load(pred_path, map_location=device)\n",
    "    gamma = evidential_outputs['gamma']  # This is the mean prediction\n",
    "    nu = evidential_outputs['nu']\n",
    "    alpha = evidential_outputs['alpha']\n",
    "    beta = evidential_outputs['beta']\n",
    "\n",
    "    # Transpose all tensors\n",
    "    gamma = torch.permute(gamma, (0, 2, 1))\n",
    "    nu = torch.permute(nu, (0, 2, 1))\n",
    "    alpha = torch.permute(alpha, (0, 2, 1))\n",
    "    beta = torch.permute(beta, (0, 2, 1))\n",
    "\n",
    "    # Mean prediction is gamma\n",
    "    mean_volume = gamma.detach()\n",
    "\n",
    "    # Total uncertainty: Var[y] = E[sigma^2] + Var[mu] = (beta / (alpha - 1)) * (1 + 1/nu)\n",
    "    # Add a small epsilon to denominators to avoid division by zero\n",
    "    variance_volume_map = (beta / (alpha - 1.0 + 1e-6)) * (1.0 + (1.0 / (nu + 1e-6)))\n",
    "    uncertainty_volume_map = torch.sqrt(variance_volume_map).detach()\n",
    "\n",
    "    # --- Calculate metrics for the mean prediction ---\n",
    "    metrics = {}\n",
    "    gt_volume = gt_volume.to(device)\n",
    "\n",
    "    # Initialize torchmetrics\n",
    "    data_range = 1.0\n",
    "    ssim_metric = torchmetrics.image.StructuralSimilarityIndexMeasure(data_range=data_range, **SSIM_KWARGS).to(device)\n",
    "    psnr_metric = torchmetrics.image.PeakSignalNoiseRatio(data_range=data_range, reduction='none').to(device)\n",
    "\n",
    "    mean_vol_batch = mean_volume.unsqueeze(1) # Add channel dim\n",
    "    gt_vol_batch = gt_volume.unsqueeze(1)   # Add channel dim\n",
    "\n",
    "    metrics['mean_ssim'] = ssim_metric(mean_vol_batch, gt_vol_batch).item()\n",
    "    metrics['mean_mse'] = torch.mean((gt_volume - mean_volume)**2).item()\n",
    "    metrics['mean_mae'] = torch.mean(torch.abs(gt_volume - mean_volume)).item()\n",
    "\n",
    "    # Robust PSNR calculation\n",
    "    mean_psnr_per_slice = psnr_metric(mean_vol_batch, gt_vol_batch)\n",
    "    finite_mean_psnrs = mean_psnr_per_slice[torch.isfinite(mean_psnr_per_slice)]\n",
    "    if finite_mean_psnrs.numel() > 0:\n",
    "        metrics['mean_psnr'] = torch.mean(finite_mean_psnrs).item()\n",
    "    else:\n",
    "        metrics['mean_psnr'] = float('inf')\n",
    "\n",
    "    # \"sample\" metrics don't apply, so we copy the mean metrics for a consistent DataFrame.\n",
    "    metrics['sample_avg_ssim'] = metrics['mean_ssim']\n",
    "    metrics['sample_avg_psnr'] = metrics['mean_psnr']\n",
    "    metrics['sample_avg_mse'] = metrics['mean_mse']\n",
    "    metrics['sample_avg_mae'] = metrics['mean_mae']\n",
    "    metrics['mean_std'] = torch.mean(uncertainty_volume_map).item()\n",
    "    metrics['rmv'] = torch.sqrt(torch.mean(uncertainty_volume_map**2)).item()\n",
    "\n",
    "    return metrics, mean_volume, uncertainty_volume_map\n",
    "\n",
    "def calculate_error_model_metrics(files_obj, model_config, scan_info, gt_volume, device):\n",
    "    \"\"\"\n",
    "    Calculates stats and metrics for an auxiliary error-prediction model.\n",
    "    The 'mean prediction' is from the primary model, and the 'uncertainty' is the\n",
    "    prediction from the auxiliary error model.\n",
    "    \"\"\"\n",
    "    patient, scan, scan_type = scan_info\n",
    "    # The primary model's version is specified in the 'domain' field for this type\n",
    "    primary_model_version = model_config['domain']\n",
    "    error_model_version = model_config['model_version_root']\n",
    "\n",
    "    # Enforce that the count for this model type must be 1\n",
    "    if model_config['count'] != 1:\n",
    "        raise ValueError(f\"Error models must have a count of 1, but got {model_config['count']}.\")\n",
    "    \n",
    "    # The domain is implicitly IMAG for this model type. The 'domain' field is repurposed.\n",
    "    if model_config['domain'] == 'IMAG':\n",
    "        raise ValueError(\"For 'error' model type, 'domain' should specify the primary model version, not 'IMAG'.\")\n",
    "\n",
    "    # --- Load Mean Prediction (from primary model) ---\n",
    "    # The primary model is deterministic, so passthrough_num is None\n",
    "    mean_pred_path = files_obj.get_images_results_filepath(primary_model_version, patient, scan, passthrough_num=None, ensure_exists=False)\n",
    "    mean_volume = torch.load(mean_pred_path, map_location=device)\n",
    "    mean_volume = torch.squeeze(mean_volume, dim=1)\n",
    "    mean_volume = torch.permute(mean_volume, (0, 2, 1))\n",
    "\n",
    "    # --- Load Uncertainty Prediction (from auxiliary error model) ---\n",
    "    # The error model is also deterministic (count=1), so passthrough_num is None\n",
    "    uncertainty_path = files_obj.get_error_results_filepath(error_model_version, patient, scan, passthrough_num=None, ensure_exists=False)\n",
    "    uncertainty_volume_map = torch.load(uncertainty_path, map_location=device)\n",
    "    # The error model should output a single channel, but we squeeze just in case\n",
    "    uncertainty_volume_map = torch.squeeze(uncertainty_volume_map, dim=1)\n",
    "    uncertainty_volume_map = torch.permute(uncertainty_volume_map, (0, 2, 1))\n",
    "\n",
    "    # --- Calculate metrics for the mean prediction ---\n",
    "    metrics = {}\n",
    "    gt_volume = gt_volume.to(device)\n",
    "\n",
    "    # Initialize torchmetrics\n",
    "    data_range = 1.0\n",
    "    ssim_metric = torchmetrics.image.StructuralSimilarityIndexMeasure(data_range=data_range, **SSIM_KWARGS).to(device)\n",
    "    psnr_metric = torchmetrics.image.PeakSignalNoiseRatio(data_range=data_range, reduction='none').to(device)\n",
    "\n",
    "    mean_vol_batch = mean_volume.unsqueeze(1) # Add channel dim\n",
    "    gt_vol_batch = gt_volume.unsqueeze(1)   # Add channel dim\n",
    "\n",
    "    metrics['mean_ssim'] = ssim_metric(mean_vol_batch, gt_vol_batch).item()\n",
    "    metrics['mean_mse'] = torch.mean((gt_volume - mean_volume)**2).item()\n",
    "    metrics['mean_mae'] = torch.mean(torch.abs(gt_volume - mean_volume)).item()\n",
    "\n",
    "    # Robust PSNR calculation\n",
    "    mean_psnr_per_slice = psnr_metric(mean_vol_batch, gt_vol_batch)\n",
    "    finite_mean_psnrs = mean_psnr_per_slice[torch.isfinite(mean_psnr_per_slice)]\n",
    "    if finite_mean_psnrs.numel() > 0:\n",
    "        metrics['mean_psnr'] = torch.mean(finite_mean_psnrs).item()\n",
    "    else:\n",
    "        metrics['mean_psnr'] = float('inf')\n",
    "\n",
    "    # \"sample\" metrics don't apply, so we copy the mean metrics for a consistent DataFrame.\n",
    "    metrics['sample_avg_ssim'] = metrics['mean_ssim']\n",
    "    metrics['sample_avg_psnr'] = metrics['mean_psnr']\n",
    "    metrics['sample_avg_mse'] = metrics['mean_mse']\n",
    "    metrics['sample_avg_mae'] = metrics['mean_mae']\n",
    "    metrics['mean_std'] = torch.mean(uncertainty_volume_map).item()\n",
    "    metrics['rmv'] = torch.sqrt(torch.mean(uncertainty_volume_map**2)).item()\n",
    "\n",
    "    return metrics, mean_volume, uncertainty_volume_map\n",
    "\n",
    "def plot_error_histogram(errors, model_name, scan_name, combined=False):\n",
    "    \"\"\"Plots histograms of the raw (not absolute) errors with and without percentile range limits.\"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "    scan_title_part = \"All Scans\" if combined else scan_name\n",
    "\n",
    "    # Histogram with percentile range limits\n",
    "    if errors.size > 0:\n",
    "        range_lims = np.percentile(errors, [0.1, 99.9])\n",
    "        axs[0].hist(errors, bins=150, log=False, range=range_lims)\n",
    "        axs[0].set_title(f'Raw Errors (0.1-99.9% Range)\\n{model_name} - {scan_title_part}')\n",
    "    else:\n",
    "        axs[0].hist(errors, bins=150, log=True)\n",
    "        axs[0].set_title(f'Raw Errors (Auto Range)\\n{model_name} - {scan_title_part}')\n",
    "    axs[0].set_xlabel('Error (GT - Prediction)')\n",
    "    axs[0].set_ylabel('Frequency (log scale)')\n",
    "    axs[0].grid(True, linestyle=':')\n",
    "\n",
    "    # Histogram with full range (no limits)\n",
    "    axs[1].hist(errors, bins=150, log=False)\n",
    "    axs[1].set_title(f'Raw Errors (Full Range)\\n{model_name} - {scan_title_part}')\n",
    "    axs[1].set_xlabel('Error (GT - Prediction)')\n",
    "    axs[1].set_ylabel('Frequency (log scale)')\n",
    "    axs[1].grid(True, linestyle=':')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_std_histogram(std_devs, model_name, scan_name, combined=False):\n",
    "    \"\"\"Plots histograms of the predicted standard deviations.\"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "    scan_title_part = \"All Scans\" if combined else scan_name\n",
    "\n",
    "    # Histogram with percentile range limits\n",
    "    if std_devs.size > 0:\n",
    "        range_lims = [0, np.percentile(std_devs, 99.9)]\n",
    "        axs[0].hist(std_devs, bins=150, log=False, range=range_lims)\n",
    "        axs[0].set_title(f'Std Deviations (0-99.9% Range)\\n{model_name} - {scan_title_part}')\n",
    "    else:\n",
    "        axs[0].hist(std_devs, bins=150, log=False)\n",
    "        axs[0].set_title(f'Std Deviations (Auto Range)\\n{model_name} - {scan_title_part}')\n",
    "    axs[0].set_xlabel('Predicted Standard Deviation')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    axs[0].grid(True, linestyle=':')\n",
    "\n",
    "    # Histogram with full range (no limits)\n",
    "    axs[1].hist(std_devs, bins=150, log=False)\n",
    "    axs[1].set_title(f'Std Deviations (Full Range)\\n{model_name} - {scan_title_part}')\n",
    "    axs[1].set_xlabel('Predicted Standard Deviation')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "    axs[1].grid(True, linestyle=':')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Metric calculation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc53009",
   "metadata": {},
   "source": [
    "### Uncertainty calibration and post-calibration metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import isotonic_regression\n",
    "\n",
    "# --- Calibration Methods ---\n",
    "\n",
    "def calculate_platt_scaler(errors, std_devs):\n",
    "    \"\"\"\n",
    "    Calculates the optimal scaling factor 'T' for variance scaling.\n",
    "    This factor is found by minimizing the NLL on a validation set.\n",
    "    The optimal T is sqrt(mean(squared_error / variance)).\n",
    "\n",
    "    Args:\n",
    "        errors (np.ndarray): The absolute errors (ground_truth - prediction).\n",
    "        std_devs (np.ndarray): The predicted standard deviations.\n",
    "\n",
    "    Returns:\n",
    "        float: The scaling factor T.\n",
    "    \"\"\"\n",
    "    errors_flat = errors.flatten()\n",
    "    std_devs_flat = std_devs.flatten()\n",
    "\n",
    "    # To avoid division by zero, add a small epsilon to the variance\n",
    "    variances_flat = std_devs_flat**2 + 1e-6\n",
    "\n",
    "    # Calculate T^2 = mean(error^2 / variance)\n",
    "    t_squared = np.mean(errors_flat**2 / variances_flat)\n",
    "\n",
    "    return np.sqrt(t_squared)\n",
    "\n",
    "\n",
    "def train_isotonic_regression(predicted_std, observed_errors, device):\n",
    "    \"\"\"\n",
    "    Trains an Isotonic Regression model using scipy.optimize.isotonic_regression.\n",
    "    \"\"\"\n",
    "    pred_std_flat = predicted_std.flatten()\n",
    "    obs_err_flat = observed_errors.flatten()\n",
    "\n",
    "    # Sort by predicted standard deviation\n",
    "    print(\"Sorting erorrs...\")\n",
    "    sort_indices = torch.argsort(torch.from_numpy(pred_std_flat).to(device)).cpu().numpy()\n",
    "    sorted_pred_std = pred_std_flat[sort_indices]\n",
    "    sorted_obs_err_sq = obs_err_flat[sort_indices]**2\n",
    "\n",
    "    # Apply scipy's isotonic regression\n",
    "    print(\"Applying isotonic regression...\")\n",
    "    calibrated_variances = isotonic_regression(sorted_obs_err_sq).x\n",
    "    calibrated_std = np.sqrt(calibrated_variances)\n",
    "\n",
    "    # Create an interpolation function to map new predictions\n",
    "    unique_pred_std, unique_indices = np.unique(sorted_pred_std, return_index=True)\n",
    "    unique_calib_std = calibrated_std[unique_indices]\n",
    "    \n",
    "    # check that the unique pred std are sorted\n",
    "    if not np.all(np.diff(unique_pred_std) >= 0):\n",
    "        raise ValueError(\"Predicted standard deviations are not sorted. Ensure the input is sorted before applying isotonic regression.\")\n",
    "\n",
    "    print(\"Creating interpolation model...\")\n",
    "    iso_model = interp1d(unique_pred_std, unique_calib_std, kind='linear', bounds_error=False, \n",
    "                         fill_value=(unique_calib_std[0], unique_calib_std[-1]), assume_sorted=True)\n",
    "\n",
    "    return iso_model\n",
    "\n",
    "def train_cdf_isotonic_regression(ground_truth, mean_pred, uncal_std, device):\n",
    "    \"\"\"\n",
    "    Trains an Isotonic Regression model on the cumulative probabilities,\n",
    "    as described in Kuleshov et al., 2018.\n",
    "\n",
    "    Returns:\n",
    "        scipy.interpolate.interp1d: The trained isotonic regression model.\n",
    "    \"\"\"\n",
    "    gt_flat = ground_truth.flatten()\n",
    "    pred_flat = mean_pred.flatten()\n",
    "    uncert_flat = uncal_std.flatten() + 1e-9 # Avoid zero std dev\n",
    "\n",
    "    print(\"Calculating predicted CDF values for calibration training...\")\n",
    "    # Get the predicted CDF value for each ground truth point\n",
    "    pred_cdfs = scipy.stats.norm.cdf(gt_flat, loc=pred_flat, scale=uncert_flat)\n",
    "    \n",
    "    # Sort the CDF values to prepare for isotonic regression\n",
    "    sorted_indices = torch.argsort(torch.from_numpy(pred_cdfs).to(device)).cpu().numpy()\n",
    "    sorted_pred_cdfs = pred_cdfs[sorted_indices]\n",
    "\n",
    "    # The target values for a perfectly calibrated model would be uniformly spaced\n",
    "    n_points = len(sorted_pred_cdfs)\n",
    "    empirical_cdfs = np.arange(n_points) / n_points\n",
    "\n",
    "    print(\"Training Isotonic Regression model on CDF values...\")\n",
    "    # The isotonic_regression function in scipy returns the calibrated values directly\n",
    "    calibrated_cdfs = isotonic_regression(empirical_cdfs).x\n",
    "    \n",
    "    # Create an interpolation function to map any new predicted cdf to a calibrated one\n",
    "    print(\"Creating interpolation model for CDF calibration...\")\n",
    "    iso_cdf_model = interp1d(\n",
    "        sorted_pred_cdfs, \n",
    "        calibrated_cdfs,\n",
    "        kind='linear',\n",
    "        bounds_error=False,\n",
    "        fill_value=(0.0, 1.0),\n",
    "        assume_sorted=True\n",
    "    )\n",
    "    \n",
    "    return iso_cdf_model\n",
    "\n",
    "\n",
    "# TODO go through this and make sure it makes sense (esp that is actually does what Kuleshov does [obvously other than the ppf part])\n",
    "def apply_cdf_isotonic_regression(iso_cdf_model, uncal_uncertainty_map, target_confidence=0.95):\n",
    "    \"\"\"\n",
    "    Uses a trained CDF isotonic model to find a new scaling factor for the standard deviation.\n",
    "    \"\"\"\n",
    "    print(f\"Finding new std dev scaling factor for {target_confidence*100}% confidence...\")\n",
    "    \n",
    "    # We want to find a new z-score (and thus a new std dev) such that the\n",
    "    # recalibrated confidence interval matches the target confidence.\n",
    "    # We are looking for an original probability p_orig such that:\n",
    "    # iso_cdf_model(p_orig) - iso_cdf_model(1 - p_orig) = target_confidence\n",
    "    \n",
    "    p_lower_orig = (1.0 - target_confidence) / 2.0\n",
    "    p_upper_orig = 1.0 - p_lower_orig\n",
    "\n",
    "    # The calibrated probability of the original interval is:\n",
    "    calibrated_prob = iso_cdf_model(p_upper_orig) - iso_cdf_model(p_lower_orig)\n",
    "    \n",
    "    # We need to find a new z-score that corresponds to this calibrated probability\n",
    "    # The new upper probability is (1 + calibrated_prob) / 2\n",
    "    p_upper_new = (1.0 + calibrated_prob) / 2.0\n",
    "    \n",
    "    # Find the z-score for this new probability\n",
    "    z_new = scipy.stats.norm.ppf(p_upper_new)\n",
    "    \n",
    "    # Find the z-score for the original target confidence\n",
    "    z_orig = scipy.stats.norm.ppf(p_upper_orig)\n",
    "    \n",
    "    # The scaling factor is the ratio of the new z-score to the original one\n",
    "    scaling_factor = z_new / (z_orig + 1e-6)\n",
    "    \n",
    "    print(f\"CDF calibration scaling factor: {scaling_factor:.4f}\")\n",
    "    \n",
    "    return uncal_uncertainty_map * scaling_factor\n",
    "\n",
    "\n",
    "# --- New Evaluation Metrics ---\n",
    "\n",
    "def calculate_nll(ground_truth, mean_pred, uncertainty_map):\n",
    "    \"\"\"\n",
    "    Calculates the Negative Log-Likelihood (NLL) for a Gaussian prediction.\n",
    "    \"\"\"\n",
    "    gt_flat = ground_truth.flatten()\n",
    "    pred_flat = mean_pred.flatten()\n",
    "    uncert_flat = uncertainty_map.flatten()\n",
    "\n",
    "    # Add a small epsilon to variance to prevent log(0) or division by zero\n",
    "    variance = uncert_flat**2 + 1e-9\n",
    "    \n",
    "    # NLL formula for a Gaussian distribution\n",
    "    nll_values = 0.5 * (np.log(2 * np.pi * variance) + (gt_flat - pred_flat)**2 / variance)\n",
    "    \n",
    "    return np.mean(nll_values)\n",
    "\n",
    "\n",
    "def calculate_all_eces(ground_truth, mean_pred, uncertainty_map, n_bins=20):\n",
    "    \"\"\"\n",
    "    Calculates variations of the Expected Calibration Error (ECE) for regression.\n",
    "    \"\"\"\n",
    "    gt_flat = ground_truth.flatten()\n",
    "    pred_flat = mean_pred.flatten()\n",
    "    uncert_flat = uncertainty_map.flatten() + 1e-9 # Avoid zero std dev\n",
    "\n",
    "    # Get the predicted CDF value for each ground truth point\n",
    "    pred_cdfs = scipy.stats.norm.cdf(gt_flat, loc=pred_flat, scale=uncert_flat)\n",
    "    \n",
    "    expected_confidence_levels = np.linspace(0, 1, n_bins)\n",
    "    observed_frequencies = np.array([np.mean(pred_cdfs <= p_j) for p_j in expected_confidence_levels])\n",
    "\n",
    "    # --- Calculate bin weights (proportional to number of points in each bin) ---\n",
    "    bin_weights = np.zeros(n_bins)\n",
    "    for i in range(1, n_bins):\n",
    "        lower_bound = expected_confidence_levels[i-1]\n",
    "        upper_bound = expected_confidence_levels[i]\n",
    "        points_in_bin = (pred_cdfs > lower_bound) & (pred_cdfs <= upper_bound)\n",
    "        bin_weights[i] = np.mean(points_in_bin)\n",
    "    \n",
    "    if np.sum(bin_weights) > 0:\n",
    "        bin_weights /= np.sum(bin_weights)\n",
    "    \n",
    "    # --- Calculate ECE variants ---\n",
    "    abs_diff = np.abs(expected_confidence_levels - observed_frequencies)\n",
    "    sq_diff = (expected_confidence_levels - observed_frequencies)**2\n",
    "\n",
    "    ece_weighted_abs = np.sum(bin_weights * abs_diff)\n",
    "    ece_weighted_sq = np.sum(bin_weights * sq_diff)\n",
    "    ece_unweighted_abs = np.mean(abs_diff)\n",
    "    ece_unweighted_sq = np.mean(sq_diff)\n",
    "    \n",
    "    return {\n",
    "        'ece_weighted_abs': ece_weighted_abs,\n",
    "        'ece_weighted_sq': ece_weighted_sq,\n",
    "        'ece_unweighted_abs': ece_unweighted_abs,\n",
    "        'ece_unweighted_sq': ece_unweighted_sq,\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_ence(ground_truth, mean_pred, uncertainty_map, device, n_bins=20):\n",
    "    \"\"\"\n",
    "    Calculates the Expected Normalized Calibration Error (ENCE) using quantile-based binning\n",
    "    as described in Levi et al., 2020.\n",
    "    \"\"\"\n",
    "    gt_flat = ground_truth.flatten()\n",
    "    pred_flat = mean_pred.flatten()\n",
    "    uncert_flat = uncertainty_map.flatten()\n",
    "    \n",
    "    # Ensure there are enough unique values for binning\n",
    "    if len(np.unique(uncert_flat)) < n_bins:\n",
    "        print(f\"Warning: Number of unique uncertainties is less than n_bins. ENCE may be unreliable.\")\n",
    "\n",
    "    # Get the indices that would sort the uncertainties\n",
    "    sorted_indices = torch.argsort(torch.from_numpy(uncert_flat).to(device)).cpu().numpy()\n",
    "    \n",
    "    # Split the sorted indices into N bins of equal size.\n",
    "    # np.array_split handles cases where the total number of points is not divisible by n_bins.\n",
    "    binned_indices = np.array_split(sorted_indices, n_bins)\n",
    "\n",
    "    ence_sum = 0\n",
    "    \n",
    "    # Loop through each bin of indices\n",
    "    for bin_idx_list in binned_indices:\n",
    "        # Skip empty bins, though this is unlikely with quantile binning\n",
    "        if len(bin_idx_list) > 0:\n",
    "            # Root Mean Variance (RMV) in bin\n",
    "            rmv_j = np.sqrt(np.mean(uncert_flat[bin_idx_list]**2)) + 1e-9\n",
    "            \n",
    "            # Root Mean Squared Error (RMSE) in bin\n",
    "            rmse_j = np.sqrt(np.mean((gt_flat[bin_idx_list] - pred_flat[bin_idx_list])**2))\n",
    "\n",
    "            # Add the normalized error for this bin to the sum\n",
    "            ence_sum += np.abs(rmv_j - rmse_j) / rmv_j\n",
    "\n",
    "    return ence_sum / n_bins\n",
    "\n",
    "\n",
    "def calculate_mpiw(uncertainty_map, confidence_levels=[0.68, 0.95]):\n",
    "    \"\"\"\n",
    "    Calculates the Mean Prediction Interval Width (MPIW) for given confidence levels.\n",
    "    \"\"\"\n",
    "    uncert_flat = uncertainty_map.flatten()\n",
    "    results = {}\n",
    "    for level in confidence_levels:\n",
    "        # Get the z-score for the confidence level (e.g., 1.96 for 95%)\n",
    "        z_score = scipy.stats.norm.ppf(1 - (1 - level) / 2)\n",
    "        \n",
    "        # Width of the prediction interval\n",
    "        widths = 2 * z_score * uncert_flat\n",
    "        \n",
    "        # Store the mean width\n",
    "        results[f'mpiw_{int(level*100)}'] = np.mean(widths)\n",
    "        \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Calibration and advanced uncertainty metric functions are defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85d476",
   "metadata": {},
   "source": [
    "### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b12fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_comparison(mean_pred, ground_truth, uncertainty_map, model_name, scan_name, slice_idx, tumor_coords_xy=None, log_scale=False, clip_pct=None):\n",
    "    \"\"\"Plots the GT, mean prediction, absolute error, and uncertainty map.\"\"\"\n",
    "    error_map = np.abs(ground_truth - mean_pred)\n",
    "\n",
    "    if log_scale:\n",
    "        error_map = np.log1p(error_map)\n",
    "        uncertainty_map = np.log1p(uncertainty_map)\n",
    "\n",
    "    # Clip the the error and uncertainty maps for better visualization\n",
    "    if clip_pct is not None:\n",
    "        error_map = np.clip(error_map, 0, np.percentile(error_map, clip_pct))\n",
    "        uncertainty_map = np.clip(uncertainty_map, 0, np.percentile(uncertainty_map, clip_pct))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    fig.suptitle(f'{model_name} - {scan_name} - Slice {slice_idx} (Mean vs. GT)', fontsize=16)\n",
    "\n",
    "    im1 = axes[0].imshow(ground_truth, cmap='gray')\n",
    "    axes[0].set_title('Ground Truth')\n",
    "    axes[0].axis('off')\n",
    "    fig.colorbar(im1, ax=axes[0])\n",
    "\n",
    "    if tumor_coords_xy:\n",
    "        x, y = tumor_coords_xy\n",
    "        x -= 6\n",
    "        y -= 6\n",
    "        for i in range(4):\n",
    "            axes[i].annotate('', xy=(x, y), xytext=(x - 30, y - 30),\n",
    "                            arrowprops=dict(facecolor='red', edgecolor='red', shrink=0.05, width=1, headwidth=5, headlength=5))\n",
    "\n",
    "    im2 = axes[1].imshow(mean_pred, cmap='gray')\n",
    "    axes[1].set_title('Mean Prediction')\n",
    "    axes[1].axis('off')\n",
    "    fig.colorbar(im2, ax=axes[1])\n",
    "\n",
    "    im3 = axes[2].imshow(error_map, cmap='magma')\n",
    "    if log_scale:\n",
    "        axes[2].set_title('Log1p Absolute Error Map')\n",
    "    else:\n",
    "        axes[2].set_title('Absolute Error Map')\n",
    "    axes[2].axis('off')\n",
    "    fig.colorbar(im3, ax=axes[2])\n",
    "\n",
    "    im4 = axes[3].imshow(uncertainty_map, cmap='viridis')\n",
    "    if log_scale:\n",
    "        axes[3].set_title('Log1p Uncertainty Map (Std Dev)')\n",
    "    else:\n",
    "        axes[3].set_title('Uncertainty (Std Dev)')\n",
    "    axes[3].axis('off')\n",
    "    fig.colorbar(im4, ax=axes[3])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"{model_name}_{scan_name}_slice{slice_idx}_clip_{clip_pct}_mean_comparison.png\", dpi=400)\n",
    "    plt.show()\n",
    "\n",
    "def plot_ssim_map(ssim_map, model_name, scan_name):\n",
    "    \"\"\"Plots the SSIM map.\"\"\"\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(ssim_map, cmap='viridis', vmin=0, vmax=1)\n",
    "    plt.title(f'SSIM Map - {model_name} - {scan_name}')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_calibration_curve(ground_truth, mean_pred, uncertainty_map, model_name, scan_name, n_levels=20):\n",
    "    \"\"\"\n",
    "    Plots the calibration curve with a marginal histogram below the x-axis\n",
    "    showing the distribution of the predicted CDF values.\n",
    "    \"\"\"\n",
    "    gt_flat = ground_truth.flatten()\n",
    "    pred_flat = mean_pred.flatten()\n",
    "    uncert_flat = uncertainty_map.flatten()\n",
    "\n",
    "    # Calculate the predicted CDF value for each point\n",
    "    pred_cdfs = scipy.stats.norm.cdf(gt_flat, loc=pred_flat, scale=uncert_flat)\n",
    "    \n",
    "    # --- Create figure with two subplots, sharing the x-axis ---\n",
    "    fig, (ax_cal, ax_hist) = plt.subplots(\n",
    "        2, 1,\n",
    "        figsize=(8, 8),\n",
    "        sharex=True,\n",
    "        gridspec_kw={'height_ratios': [3, 1]} # Main plot is 3x taller\n",
    "    )\n",
    "    \n",
    "    # --- Main Calibration Plot (top) ---\n",
    "    expected_confidence_levels = np.linspace(0, 1, n_levels)\n",
    "    observed_frequencies = np.array([np.mean(pred_cdfs <= p_j) for p_j in expected_confidence_levels])\n",
    "\n",
    "    ax_cal.plot([0, 1], [0, 1], '--', color='grey', label='Perfectly Calibrated')\n",
    "    ax_cal.plot(expected_confidence_levels, observed_frequencies, '-o', label='Model Calibration')\n",
    "    ax_cal.set_ylabel('Observed Confidence Level')\n",
    "    ax_cal.set_title(f'Calibration Plot - {model_name} - {scan_name}')\n",
    "    ax_cal.legend()\n",
    "    ax_cal.grid(True, linestyle=':')\n",
    "\n",
    "    # --- Marginal Histogram (bottom) ---\n",
    "    ax_hist.hist(pred_cdfs, bins=50, range=(0,1), density=True, color='steelblue', alpha=0.8)\n",
    "    ax_hist.set_xlabel('Expected Confidence Level (Predicted CDF)')\n",
    "    ax_hist.set_ylabel('Density')\n",
    "    ax_hist.set_yscale('log')\n",
    "    # ax_hist.set_yticks([]) # Hide y-ticks for clarity\n",
    "\n",
    "    # Final adjustments\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_sparsification_curve(uncertainty, errors, model_name, scan_name, device):\n",
    "    \"\"\"\n",
    "    Plots the model and oracle sparsification curves used for AUSE calculation.\n",
    "    \"\"\"\n",
    "    uncertainty_flat = uncertainty.flatten()\n",
    "    errors_flat = errors.flatten()\n",
    "    \n",
    "    def get_sparsification_curve_fast(sorted_errs, overall_mae):\n",
    "        n_pixels = len(sorted_errs)\n",
    "        cumulative_errors = np.cumsum(sorted_errs)\n",
    "        total_error_sum = cumulative_errors[-1]\n",
    "        sum_errors_removed = np.insert(cumulative_errors[:-1], 0, 0)\n",
    "        sum_errors_remaining = total_error_sum - sum_errors_removed\n",
    "        n_remaining = np.arange(n_pixels, 0, -1)\n",
    "        curve = sum_errors_remaining / n_remaining\n",
    "        if overall_mae > 0:\n",
    "            curve = curve / overall_mae\n",
    "        return curve\n",
    "    \n",
    "    overall_mae = np.mean(errors_flat)\n",
    "\n",
    "    # Model curve (sorted by uncertainty)\n",
    "    model_sorted_indices = torch.argsort(torch.from_numpy(uncertainty_flat).to(device), descending=True).cpu().numpy()\n",
    "    model_sorted_errors = errors_flat[model_sorted_indices]\n",
    "    model_curve = get_sparsification_curve_fast(model_sorted_errors, overall_mae)\n",
    "\n",
    "    # Oracle curve (sorted by error)\n",
    "    oracle_sorted_errors = torch.sort(torch.from_numpy(errors_flat).to(device), descending=True).values.cpu().numpy()\n",
    "    oracle_curve = get_sparsification_curve_fast(oracle_sorted_errors, overall_mae)\n",
    "    \n",
    "    # X-axis: fraction of pixels removed\n",
    "    fraction_removed = np.linspace(0, 1, len(model_curve))\n",
    "\n",
    "    # Downsample to 1000 points\n",
    "    if len(fraction_removed) > 1000:\n",
    "        step = len(fraction_removed) // 1000\n",
    "        fraction_removed = fraction_removed[::step]\n",
    "        model_curve = model_curve[::step]\n",
    "        oracle_curve = oracle_curve[::step]\n",
    "    \n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(fraction_removed, model_curve, label='Model (Sort by Uncertainty)')\n",
    "    plt.plot(fraction_removed, oracle_curve, '--', label='Oracle (Sort by Error)')\n",
    "    plt.xlabel('Fraction of Pixels Removed')\n",
    "    plt.ylabel('Mean Absolute Error of Remaining Pixels')\n",
    "    plt.title(f'Sparsification Curve - {model_name} - {scan_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':')\n",
    "    plt.show()\n",
    "\n",
    "def plot_samples_comparison(ground_truth, mean_pred, samples, model_name, scan_name, slice_idx, tumor_coords_xy=None):\n",
    "    \"\"\"\n",
    "    Plots the ground truth, mean prediction, and a few individual sample predictions.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth (np.ndarray): The 2D ground truth slice.\n",
    "        mean_pred (np.ndarray): The 2D mean prediction slice.\n",
    "        samples (list of np.ndarray): A list of 2D sample prediction slices.\n",
    "        model_name (str): The name of the model for the title.\n",
    "        scan_name (str): The name of the scan for the title.\n",
    "        slice_idx (int): The index of the slice for the title.\n",
    "        tumor_coords_xy (tuple, optional): (x, y) coordinates for the tumor arrow.\n",
    "    \"\"\"\n",
    "    num_samples = len(samples)\n",
    "    # Total columns = 1 for GT + 1 for Mean + N for samples\n",
    "    num_cols = 2 + num_samples\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_cols, figsize=(4 * num_cols, 4.5), constrained_layout=True)\n",
    "    fig.suptitle(f'{model_name} - {scan_name} - Slice {slice_idx} (GT, Mean, and Samples)', fontsize=16)\n",
    "\n",
    "    # Determine a consistent grayscale range based on the ground truth and mean\n",
    "    vmin = min(ground_truth.min(), mean_pred.min())\n",
    "    vmax = max(ground_truth.max(), mean_pred.max())\n",
    "\n",
    "    # --- Plot Ground Truth ---\n",
    "    axes[0].imshow(ground_truth, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    axes[0].set_title('Ground Truth')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # --- Plot Mean Prediction ---\n",
    "    axes[1].imshow(mean_pred, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    axes[1].set_title('Mean Prediction')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # --- Plot Samples ---\n",
    "    for i in range(num_samples):\n",
    "        ax = axes[i + 2]\n",
    "        im = ax.imshow(samples[i], cmap='gray', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f'Sample {i+1}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # --- Add Tumor Arrow ---\n",
    "    if tumor_coords_xy:\n",
    "        x, y = tumor_coords_xy\n",
    "        x -= 6 # Shift so the arrow doesn't overlap the tumor\n",
    "        y -= 6\n",
    "        for ax in axes:\n",
    "            ax.annotate('', xy=(x, y), xytext=(x - 30, y - 30),\n",
    "                        arrowprops=dict(facecolor='red', edgecolor='red', shrink=0.05, \n",
    "                                        width=1, headwidth=5, headlength=5))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_worst_samples_comparison(ground_truth, mean_pred, worst_samples_data, model_name, scan_name, slice_idx, tumor_coords_xy=None):\n",
    "    \"\"\"\n",
    "    Plots the ground truth, mean prediction, and the worst-performing sample predictions.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth (np.ndarray): The 2D ground truth slice.\n",
    "        mean_pred (np.ndarray): The 2D mean prediction slice.\n",
    "        worst_samples_data (list): A list of tuples, where each tuple is \n",
    "                                   (loss, sample_slice_numpy, sample_index).\n",
    "        model_name (str): The name of the model for the title.\n",
    "        scan_name (str): The name of the scan for the title.\n",
    "        slice_idx (int): The index of the slice for the title.\n",
    "        tumor_coords_xy (tuple, optional): (x, y) coordinates for the tumor arrow.\n",
    "    \"\"\"\n",
    "    num_samples = len(worst_samples_data)\n",
    "    num_cols = 2 + num_samples\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_cols, figsize=(4 * num_cols, 5), constrained_layout=True)\n",
    "    fig.suptitle(f'{model_name} - {scan_name} - Slice {slice_idx} (Top {num_samples} Worst Samples by SmoothL1Loss)', fontsize=16)\n",
    "\n",
    "    # Determine a consistent grayscale range\n",
    "    vmin = min(ground_truth.min(), mean_pred.min())\n",
    "    vmax = max(ground_truth.max(), mean_pred.max())\n",
    "\n",
    "    # --- Plot Ground Truth ---\n",
    "    axes[0].imshow(ground_truth, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    axes[0].set_title('Ground Truth')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # --- Plot Mean Prediction ---\n",
    "    axes[1].imshow(mean_pred, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    axes[1].set_title('Mean Prediction')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # --- Plot Worst Samples ---\n",
    "    for i in range(num_samples):\n",
    "        loss, sample_slice, sample_idx = worst_samples_data[i]\n",
    "        ax = axes[i + 2]\n",
    "        im = ax.imshow(sample_slice, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "        # Add the loss and original sample number to the title\n",
    "        ax.set_title(f'Sample #{sample_idx}\\nLoss: {loss:.4f}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # --- Add Tumor Arrow ---\n",
    "    if tumor_coords_xy:\n",
    "        x, y = tumor_coords_xy\n",
    "        for ax in axes:\n",
    "            ax.annotate('', xy=(x, y), xytext=(x - 30, y - 30),\n",
    "                        arrowprops=dict(facecolor='red', edgecolor='red', shrink=0.05, \n",
    "                                        width=1, headwidth=5, headlength=5))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_combined_calibration_curves(\n",
    "    ground_truth, mean_pred, \n",
    "    platt_uncertainty_map, iso_uncertainty_map, iso_cdf_uncertainty_map, \n",
    "    model_name, scan_name, n_bins=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots ECE and ENCE calibration curves on a single figure,\n",
    "    comparing all three calibration methods directly.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    fig.suptitle(f'Calibration Comparison for {model_name} on {scan_name}', fontsize=16)\n",
    "\n",
    "    # --- ECE Subplot (ax1) ---\n",
    "    ece_calibrations = {\n",
    "        'STD Scaling': platt_uncertainty_map, \n",
    "        'Isotonic (Var)': iso_uncertainty_map,\n",
    "        'Isotonic (CDF)': iso_cdf_uncertainty_map\n",
    "    }\n",
    "    ece_values = {}\n",
    "\n",
    "    for name, uncert_map in ece_calibrations.items():\n",
    "        gt_flat, pred_flat, uncert_flat = ground_truth.flatten(), mean_pred.flatten(), uncert_map.flatten() + 1e-9\n",
    "        pred_cdfs = scipy.stats.norm.cdf(gt_flat, loc=pred_flat, scale=uncert_flat)\n",
    "        \n",
    "        expected_confidence = np.linspace(0, 1, n_bins + 1)\n",
    "        observed_confidence = np.array([np.mean(pred_cdfs <= p_j) for p_j in expected_confidence])\n",
    "        \n",
    "        ax1.plot(expected_confidence, observed_confidence, '-o', label=name, alpha=0.8)\n",
    "        \n",
    "        ece_metric = calculate_all_eces(ground_truth, mean_pred, uncert_map, n_bins=n_bins)\n",
    "        ece_values[name] = ece_metric['ece_unweighted_abs']\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], '--', color='grey', label='Perfect')\n",
    "    title_ece = (\n",
    "        f'ECE Plot\\nSTD ECE: {ece_values[\"STD Scaling\"]:.4f} | '\n",
    "        f'Iso (Var) ECE: {ece_values[\"Isotonic (Var)\"]:.4f} | '\n",
    "        f'Iso (CDF) ECE: {ece_values[\"Isotonic (CDF)\"]:.4f}'\n",
    "    )\n",
    "    ax1.set_title(title_ece)\n",
    "    ax1.set_xlabel('Expected Confidence Level')\n",
    "    ax1.set_ylabel('Observed Confidence Level')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle=':')\n",
    "    ax1.axis('equal')\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "\n",
    "    # --- ENCE Subplot (ax2) ---\n",
    "    ence_calibrations = {\n",
    "        'STD Scaling': platt_uncertainty_map,\n",
    "        'Isotonic (Var)': iso_uncertainty_map,\n",
    "        'Isotonic (CDF)': iso_cdf_uncertainty_map\n",
    "    }\n",
    "    ence_values = {}\n",
    "    all_points = []\n",
    "\n",
    "    for name, uncert_map in ence_calibrations.items():\n",
    "        gt_flat, pred_flat, uncert_flat = ground_truth.flatten(), mean_pred.flatten(), uncert_map.flatten()\n",
    "        \n",
    "        sorted_indices = torch.argsort(torch.from_numpy(uncert_flat).to(DEVICE)).cpu().numpy()\n",
    "        binned_indices = np.array_split(sorted_indices, n_bins)\n",
    "        \n",
    "        bin_rmv, bin_rmse = [], []\n",
    "        for bin_idx_list in binned_indices:\n",
    "            if len(bin_idx_list) > 0:\n",
    "                rmv_j = np.sqrt(np.mean(uncert_flat[bin_idx_list]**2))\n",
    "                rmse_j = np.sqrt(np.mean((gt_flat[bin_idx_list] - pred_flat[bin_idx_list])**2))\n",
    "                bin_rmv.append(rmv_j)\n",
    "                bin_rmse.append(rmse_j)\n",
    "        \n",
    "        ax2.plot(bin_rmv, bin_rmse, '-o', label=name, alpha=0.8, zorder=3)\n",
    "        all_points.extend(bin_rmv)\n",
    "        all_points.extend(bin_rmse)\n",
    "        \n",
    "        ence_values[name] = calculate_ence(ground_truth, mean_pred, uncert_map, DEVICE, n_bins=n_bins)\n",
    "\n",
    "    if all_points:\n",
    "      max_val = np.max(all_points) * 1.1\n",
    "      ax2.plot([0, max_val], [0, max_val], '--', color='grey', label='Perfect')\n",
    "      ax2.set_xlim(left=0, right=max_val)\n",
    "      ax2.set_ylim(bottom=0, top=max_val)\n",
    "\n",
    "    title_ence = (\n",
    "        f'ENCE Plot (RMSE vs. RMV)\\nSTD ENCE: {ence_values[\"STD Scaling\"]:.4f} | '\n",
    "        f'Iso (Var) ENCE: {ence_values[\"Isotonic (Var)\"]:.4f} | '\n",
    "        f'Iso (CDF) ENCE: {ence_values[\"Isotonic (CDF)\"]:.4f}'\n",
    "    )\n",
    "    ax2.set_title(title_ence)\n",
    "    ax2.set_xlabel('Root Mean Variance (RMV) per Bin')\n",
    "    ax2.set_ylabel('Root Mean Squared Error (RMSE) per Bin')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle=':')\n",
    "    ax2.axis('equal')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "def plot_combined_calibration_curves_old(\n",
    "    ground_truth, mean_pred, \n",
    "    platt_uncertainty_map, iso_uncertainty_map, \n",
    "    model_name, scan_name, n_bins=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots ECE and ENCE calibration curves on a single figure,\n",
    "    comparing STD Scaling and Isotonic Regression directly.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    fig.suptitle(f'Calibration Comparison for {model_name} on {scan_name}', fontsize=16)\n",
    "\n",
    "    # --- ECE Subplot (ax1) ---\n",
    "    ece_calibrations = {'STD Scaling': platt_uncertainty_map, 'Isotonic': iso_uncertainty_map}\n",
    "    ece_values = {}\n",
    "\n",
    "    for name, uncert_map in ece_calibrations.items():\n",
    "        gt_flat, pred_flat, uncert_flat = ground_truth.flatten(), mean_pred.flatten(), uncert_map.flatten() + 1e-9\n",
    "        pred_cdfs = scipy.stats.norm.cdf(gt_flat, loc=pred_flat, scale=uncert_flat)\n",
    "        \n",
    "        expected_confidence = np.linspace(0, 1, n_bins + 1)\n",
    "        observed_confidence = np.array([np.mean(pred_cdfs <= p_j) for p_j in expected_confidence])\n",
    "        \n",
    "        ax1.plot(expected_confidence, observed_confidence, '-o', label=name, alpha=0.8)\n",
    "        \n",
    "        ece_metric = calculate_all_eces(ground_truth, mean_pred, uncert_map, n_bins=n_bins)\n",
    "        ece_values[name] = ece_metric['ece_unweighted_abs']\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], '--', color='grey', label='Perfect')\n",
    "    ax1.set_title(f'ECE Plot | STD ECE: {ece_values[\"STD Scaling\"]:.4f} | Isotonic ECE: {ece_values[\"Isotonic\"]:.4f}')\n",
    "    ax1.set_xlabel('Expected Confidence Level')\n",
    "    ax1.set_ylabel('Observed Confidence Level')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle=':')\n",
    "    ax1.axis('equal')\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "\n",
    "    # --- ENCE Subplot (ax2) ---\n",
    "    ence_calibrations = {'STD Scaling': platt_uncertainty_map, 'Isotonic': iso_uncertainty_map}\n",
    "    ence_values = {}\n",
    "    all_points = []\n",
    "\n",
    "    for name, uncert_map in ence_calibrations.items():\n",
    "        gt_flat, pred_flat, uncert_flat = ground_truth.flatten(), mean_pred.flatten(), uncert_map.flatten()\n",
    "        \n",
    "        sorted_indices = torch.argsort(torch.from_numpy(uncert_flat).to(DEVICE)).cpu().numpy()\n",
    "        binned_indices = np.array_split(sorted_indices, n_bins)\n",
    "        \n",
    "        bin_rmv, bin_rmse = [], []\n",
    "        for bin_idx_list in binned_indices:\n",
    "            if len(bin_idx_list) > 0:\n",
    "                rmv_j = np.sqrt(np.mean(uncert_flat[bin_idx_list]**2))\n",
    "                rmse_j = np.sqrt(np.mean((gt_flat[bin_idx_list] - pred_flat[bin_idx_list])**2))\n",
    "                bin_rmv.append(rmv_j)\n",
    "                bin_rmse.append(rmse_j)\n",
    "        \n",
    "        ax2.plot(bin_rmv, bin_rmse, '-o', label=name, alpha=0.8, zorder=3)\n",
    "        all_points.extend(bin_rmv)\n",
    "        all_points.extend(bin_rmse)\n",
    "        \n",
    "        ence_values[name] = calculate_ence(ground_truth, mean_pred, uncert_map, DEVICE, n_bins=n_bins)\n",
    "\n",
    "    if all_points:\n",
    "      max_val = np.max(all_points) * 1.1\n",
    "      ax2.plot([0, max_val], [0, max_val], '--', color='grey', label='Perfect')\n",
    "      ax2.set_xlim(left=0, right=max_val)\n",
    "      ax2.set_ylim(bottom=0, top=max_val)\n",
    "\n",
    "    ax2.set_title(f'ENCE Plot (RMSE vs. RMV) | STD ENCE: {ence_values[\"STD Scaling\"]:.4f} | Isotonic ENCE: {ence_values[\"Isotonic\"]:.4f}')\n",
    "    ax2.set_xlabel('Root Mean Variance (RMV) per Bin')\n",
    "    ax2.set_ylabel('Root Mean Squared Error (RMSE) per Bin')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle=':')\n",
    "    ax2.axis('equal')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df3b1db",
   "metadata": {},
   "source": [
    "## Pre-calibration processing & results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ffbbf",
   "metadata": {},
   "source": [
    "NOTES FOR BBB:\n",
    "pi=0.75, sigma1=1e-1, sigma2=1e-3, beta=1e-1 was good\n",
    "pi=0.75, sigma1=1e-1, sigma2=1e-3, beta=1e-2 was good\n",
    "pi=0.25, sigma1=1e-1, sigma2=1e-3, beta=1e-2 was good\n",
    "\n",
    "pi=0.75, sigma1=1e-1, sigma2=1e-3, beta=1e-3 had hallucinations\n",
    "pi=0.75, sigma1=1e-1, sigma2=1e-3, beta=1e0 had minor hallucinations\n",
    "pi=0.75, beta=1e-2, and sigma1=5e-1, sigma1=5e-2, sigma2=1e-2, sigma2=1e-4 all hallucinated (i.e., whenever I changed sigma1 or sigma2)\n",
    "pi=0.5 has been bad for all beta (hallucinations)\n",
    "pi=0.25, sigma1=1e-1, sigma2=1e-3, beta=1e-1 had large, but not very extreme hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c49df",
   "metadata": {},
   "source": [
    "### Main loop: Pre-calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f091a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list will store dictionaries of results for each scan and model\n",
    "all_results = []\n",
    "# These dictionaries will store all pixel/voxel data for combined histograms\n",
    "all_model_errors = {}\n",
    "all_model_stds = {}\n",
    "\n",
    "\n",
    "for model_config in MODELS_TO_ANALYZE:\n",
    "    model_name = model_config['name']\n",
    "    domain = model_config['domain']\n",
    "    model_type = model_config['type']\n",
    "    \n",
    "    # Initialize lists for the current model\n",
    "    all_model_errors[model_name] = []\n",
    "    all_model_stds[model_name] = []\n",
    "    scan_results = []\n",
    "\n",
    "    for scan_info in tqdm(analysis_scans, desc=f\"Analyzing Model: {model_name}\"):\n",
    "        patient, scan, _ = scan_info\n",
    "        scan_name = f\"p{patient}_{scan}\"\n",
    "        \n",
    "        # --- Determine Domain and Plotting Slice ---\n",
    "        is_visual_domain = domain != \"PROJ\"\n",
    "        plot_slice_idx = None\n",
    "        tumor_xy = None\n",
    "        \n",
    "        if is_visual_domain:\n",
    "            if 'tumor_locations' in locals() and tumor_locations is not None:\n",
    "                try:\n",
    "                    loc = tumor_locations[int(patient), int(scan)]\n",
    "                    tumor_xy = (loc[1].item(), loc[0].item())\n",
    "                    plot_slice_idx = int(loc[2].item()) - 20\n",
    "                except (IndexError, TypeError):\n",
    "                    raise Exception(f\"Could not find tumor location for {scan_name}.\")\n",
    "            else:\n",
    "                raise Exception(f\"Could not find tumor location for {scan_name}.\")\n",
    "        \n",
    "        # --- Data Loading (Ground Truth Only) ---\n",
    "        # For 'error' type, the domain is implicitly 'IMAG' for the ground truth\n",
    "        gt_domain = 'IMAG' if model_type == 'error' else domain\n",
    "        gt_volume = load_ground_truth(FILES, scan_info, gt_domain, slice_idx=None)\n",
    "        gt_volume_np = gt_volume.cpu().numpy()\n",
    "        \n",
    "        # --- Iterative & Evidential Metric Calculation ---\n",
    "        if model_type in ['stochastic', 'ensemble']:\n",
    "            print(\"Calculating metrics iteratively for stochastic/ensemble model...\")\n",
    "            iq_metrics, mean_pred_vol_th, uncertainty_map_vol_th = calculate_volume_metrics_2_pass(\n",
    "                FILES, model_config, scan_info, gt_volume.to(DEVICE), DEVICE\n",
    "            )\n",
    "        elif model_type == 'evidential':\n",
    "            print(\"Calculating metrics for evidential model...\")\n",
    "            iq_metrics, mean_pred_vol_th, uncertainty_map_vol_th = calculate_evidential_volume_metrics(\n",
    "                FILES, model_config, scan_info, gt_volume.to(DEVICE), DEVICE\n",
    "            )\n",
    "        elif model_type == 'error':\n",
    "            print(\"Calculating metrics for error-prediction model...\")\n",
    "            iq_metrics, mean_pred_vol_th, uncertainty_map_vol_th = calculate_error_model_metrics(\n",
    "                FILES, model_config, scan_info, gt_volume.to(DEVICE), DEVICE\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "        mean_pred_vol = mean_pred_vol_th.cpu().numpy()\n",
    "        uncertainty_map_vol = uncertainty_map_vol_th.cpu().numpy()\n",
    "        del mean_pred_vol_th, uncertainty_map_vol_th\n",
    "        \n",
    "        # --- Raw Errors, Data Collection, and Per-Scan Histograms ---\n",
    "        raw_errors_vol = gt_volume_np - mean_pred_vol\n",
    "        \n",
    "        # Store flattened arrays for later analysis\n",
    "        all_model_errors[model_name].append(raw_errors_vol.flatten())\n",
    "        all_model_stds[model_name].append(uncertainty_map_vol.flatten())\n",
    "        \n",
    "        # Plot per-scan histograms\n",
    "        plot_error_histogram(raw_errors_vol.flatten(), model_name, scan_name)\n",
    "        plot_std_histogram(uncertainty_map_vol.flatten(), model_name, scan_name)\n",
    "\n",
    "        # --- Uncertainty Metric Calculation ---\n",
    "        errors_vol = np.abs(raw_errors_vol)\n",
    "\n",
    "        print(\"Calculating AUSE...\")\n",
    "        ause_val = calculate_ause_sparsification(uncertainty_map_vol, errors_vol)\n",
    "\n",
    "        print(\"Calculating Spearman's correlation (uncertainty vs. error)...\")\n",
    "        spearman_val = calculate_spearman_correlation(uncertainty_map_vol, errors_vol, DEVICE)\n",
    "        \n",
    "        print(\"Calculating Pearson's correlation (uncertainty vs. error)...\")\n",
    "        pearson_val = calculate_pearson_correlation(uncertainty_map_vol, errors_vol, DEVICE)\n",
    "\n",
    "        print(\"Calculating MPIW...\")\n",
    "        mpiw_metrics = calculate_mpiw(uncertainty_map_vol, confidence_levels=[0.95])\n",
    "\n",
    "        # --- Store Results ---\n",
    "        scan_result = {\n",
    "            'model_name': model_name,\n",
    "            'scan_name': scan_name,\n",
    "            **iq_metrics,\n",
    "            'ause': ause_val,\n",
    "            'spearman_corr_uncert_err': spearman_val,\n",
    "            'pearson_corr_uncert_err': pearson_val,\n",
    "            **mpiw_metrics,\n",
    "        }\n",
    "        scan_results.append(scan_result)\n",
    "        \n",
    "        # --- Visualization ---\n",
    "        print(f\"\\n--- Results for {model_name} on {scan_name} ---\")\n",
    "        \n",
    "        # plot_calibration_curve(gt_volume_np, mean_pred_vol, uncertainty_map_vol, model_name, scan_name)\n",
    "        # plot_sparsification_curve(uncertainty_map_vol, errors_vol, model_name, scan_name, DEVICE)\n",
    "        \n",
    "        if is_visual_domain:\n",
    "            gt_slice_np = gt_volume_np[plot_slice_idx]\n",
    "            mean_pred_slice = mean_pred_vol[plot_slice_idx]\n",
    "            uncertainty_map_slice = uncertainty_map_vol[plot_slice_idx]\n",
    "            \n",
    "            plot_mean_comparison(mean_pred_slice, gt_slice_np, uncertainty_map_slice, \n",
    "                                 model_name, scan_name, plot_slice_idx, tumor_coords_xy=tumor_xy)\n",
    "\n",
    "            plot_mean_comparison(mean_pred_slice, gt_slice_np, uncertainty_map_slice, \n",
    "                                 model_name, scan_name, plot_slice_idx, tumor_coords_xy=tumor_xy, log_scale=False, clip_pct=99)\n",
    "\n",
    "            if model_type not in ['evidential', 'error']:\n",
    "                # --- Load and plot a few samples for visual comparison ---\n",
    "                print(\"Loading and plotting samples...\")\n",
    "                SAMPLES_TO_PLOT = 3\n",
    "                sample_slices_for_plotting = []\n",
    "                \n",
    "                # Determine the number of samples to load (can't be more than what's available)\n",
    "                num_to_load = min(SAMPLES_TO_PLOT, model_config['count'])\n",
    "\n",
    "                for i in range(num_to_load):\n",
    "                    passthrough_num = None\n",
    "                    model_version = model_config['model_version_root']\n",
    "\n",
    "                    if model_type == 'ensemble':\n",
    "                        model_version = f\"{model_config['model_version_root']}_{i+1:02d}\"\n",
    "                    elif model_type == 'stochastic':\n",
    "                        passthrough_num = i\n",
    "                    \n",
    "                    # This loading logic is copied from your metrics function\n",
    "                    pred = None\n",
    "                    if domain == 'FDK':\n",
    "                        pred_path = FILES.get_recon_filepath(model_version, patient, scan, scan_type_agg, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                        pred = torch.load(pred_path).detach()\n",
    "                        pred = pred[20:-20, :, :]\n",
    "                        if scan_type_agg == \"FF\":\n",
    "                            pred = pred[:, 128:-128, 128:-128]\n",
    "                        pred = 25. * torch.clip(pred, min=0.0, max=0.04)\n",
    "                    elif domain == 'IMAG':\n",
    "                        pred_path = FILES.get_images_results_filepath(model_version, patient, scan, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                        pred = torch.load(pred_path).detach()\n",
    "                        pred = torch.squeeze(pred, dim=1)\n",
    "                        pred = torch.permute(pred, (0, 2, 1))\n",
    "                    \n",
    "                    # Get the specific slice and convert to a numpy array for plotting\n",
    "                    if pred is not None:\n",
    "                        sample_slice = pred[plot_slice_idx].cpu().numpy()\n",
    "                        sample_slices_for_plotting.append(sample_slice)\n",
    "\n",
    "                # Call the new plotting function\n",
    "                if sample_slices_for_plotting:\n",
    "                    plot_samples_comparison(\n",
    "                        ground_truth=gt_slice_np,\n",
    "                        mean_pred=mean_pred_slice,\n",
    "                        samples=sample_slices_for_plotting,\n",
    "                        model_name=model_name,\n",
    "                        scan_name=scan_name,\n",
    "                        slice_idx=plot_slice_idx,\n",
    "                        tumor_coords_xy=tumor_xy\n",
    "                    )\n",
    "\n",
    "                # print(\"Finding and plotting worst samples by Smooth L1 Loss...\")\n",
    "                # WORST_SAMPLES_TO_PLOT = 5\n",
    "\n",
    "                # # List to store tuples of (loss, sample_slice_numpy, sample_index)\n",
    "                # worst_samples_data = []\n",
    "                # # Get the ground truth slice as a tensor on the correct device\n",
    "                # gt_slice_tensor = gt_volume[plot_slice_idx].to(DEVICE)\n",
    "\n",
    "                # # Loop through all available samples to find the worst ones\n",
    "                # for i in tqdm(range(model_config['count']), desc=\"Finding Worst Samples\", leave=False):\n",
    "                #     passthrough_num = None\n",
    "                #     model_version = model_config['model_version_root']\n",
    "                #     model_type = model_config['type']\n",
    "\n",
    "                #     if model_type == 'ensemble':\n",
    "                #         model_version = f\"{model_config['model_version_root']}_{i+1:02d}\"\n",
    "                #     elif model_type == 'stochastic':\n",
    "                #         passthrough_num = i\n",
    "                    \n",
    "                #     # Load the prediction volume as a tensor on the GPU\n",
    "                #     pred_vol_tensor = None\n",
    "                #     if domain == 'FDK':\n",
    "                #         pred_path = FILES.get_recon_filepath(model_version, patient, scan, scan_type_agg, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                #         pred_vol_tensor = torch.load(pred_path, map_location=DEVICE).detach()\n",
    "                #         pred_vol_tensor = pred_vol_tensor[20:-20, :, :]\n",
    "                #         pred_vol_tensor = 25. * torch.clip(pred_vol_tensor, min=0.0, max=0.04)\n",
    "                #     elif domain == 'IMAG':\n",
    "                #         pred_path = FILES.get_images_results_filepath(model_version, patient, scan, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                #         pred_vol_tensor = torch.load(pred_path, map_location=DEVICE).detach()\n",
    "                #         pred_vol_tensor = torch.squeeze(pred_vol_tensor, dim=1)\n",
    "                #         pred_vol_tensor = torch.permute(pred_vol_tensor, (0, 2, 1))\n",
    "\n",
    "                #     if pred_vol_tensor is not None:\n",
    "                #         pred_slice_tensor = pred_vol_tensor[plot_slice_idx]\n",
    "                        \n",
    "                #         # Calculate Smooth L1 Loss for the current slice\n",
    "                #         import torch.nn.functional as F\n",
    "                #         loss = F.smooth_l1_loss(pred_slice_tensor, gt_slice_tensor, reduction='mean').item()\n",
    "\n",
    "                #         # Keep track of the top 5 worst samples (highest loss)\n",
    "                #         if len(worst_samples_data) < WORST_SAMPLES_TO_PLOT:\n",
    "                #             worst_samples_data.append((loss, pred_slice_tensor.cpu().numpy(), i))\n",
    "                #         else:\n",
    "                #             # Find the sample with the minimum loss currently in our list\n",
    "                #             min_loss_in_list = min(worst_samples_data, key=lambda x: x[0])\n",
    "                #             if loss > min_loss_in_list[0]:\n",
    "                #                 # If current sample is worse, replace the \"best of the worst\"\n",
    "                #                 worst_samples_data.remove(min_loss_in_list)\n",
    "                #                 worst_samples_data.append((loss, pred_slice_tensor.cpu().numpy(), i))\n",
    "\n",
    "                # # Sort the final list from worst to best for plotting\n",
    "                # worst_samples_data.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "                # # Call the new plotting function with the results\n",
    "                # if worst_samples_data:\n",
    "                #     plot_worst_samples_comparison(\n",
    "                #         ground_truth=gt_slice_np,\n",
    "                #         mean_pred=mean_pred_slice,\n",
    "                #         worst_samples_data=worst_samples_data,\n",
    "                #         model_name=model_name,\n",
    "                #         scan_name=scan_name,\n",
    "                #         slice_idx=plot_slice_idx,\n",
    "                #         tumor_coords_xy=tumor_xy\n",
    "                #     )\n",
    "\n",
    "        # --- Clean up memory ---\n",
    "        del gt_volume, gt_volume_np, mean_pred_vol, uncertainty_map_vol, errors_vol, raw_errors_vol\n",
    "        gc.collect()\n",
    "        \n",
    "    all_results.extend(scan_results)\n",
    "\n",
    "# Convert results to a pandas DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\n\\nâœ… Analysis complete for all models and scans.\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb60e045",
   "metadata": {},
   "source": [
    "### Summary/Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad791ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n--- Combined Analysis Across All Scans ---\")\n",
    "\n",
    "for model_name in [m['name'] for m in MODELS_TO_ANALYZE]:\n",
    "    print(f\"\\n{'='*25}\\nMODEL: {model_name}\\n{'='*25}\")\n",
    "\n",
    "    # --- 1. Combined Histograms ---\n",
    "    print(\"\\n--- Combined Histograms ---\")\n",
    "    # Concatenate all errors and stds for the current model\n",
    "    if all_model_errors[model_name]:\n",
    "        combined_errors = np.concatenate(all_model_errors[model_name])\n",
    "        plot_error_histogram(combined_errors, model_name, \"\", combined=True)\n",
    "    else:\n",
    "        print(\"No error data to plot for combined histogram.\")\n",
    "\n",
    "    if all_model_stds[model_name]:\n",
    "        combined_stds = np.concatenate(all_model_stds[model_name])\n",
    "        plot_std_histogram(combined_stds, model_name, \"\", combined=True)\n",
    "    else:\n",
    "        print(\"No std dev data to plot for combined histogram.\")\n",
    "\n",
    "\n",
    "    # --- 2. Cross-Scan Correlations ---\n",
    "    print(\"\\n--- Cross-Scan Correlation Results ---\")\n",
    "    model_df = results_df[results_df['model_name'] == model_name].copy()\n",
    "\n",
    "    if len(model_df) < 2:\n",
    "        print(\"Cannot calculate cross-scan correlations with fewer than 2 scans.\")\n",
    "        continue\n",
    "\n",
    "    # Define the metrics to correlate\n",
    "    uncert_metrics_to_correlate = ['mean_std', 'rmv']\n",
    "    iq_metrics_to_correlate = ['mean_ssim', 'mean_psnr', 'sample_avg_ssim', 'sample_avg_psnr']\n",
    "    \n",
    "    # For evidential models, sample_avg is the same as mean, so we remove duplicates\n",
    "    if MODELS_TO_ANALYZE[0]['type'] == 'evidential':\n",
    "        iq_metrics_to_correlate = ['mean_ssim', 'mean_psnr']\n",
    "\n",
    "    correlation_results = []\n",
    "\n",
    "    for uncert_metric in uncert_metrics_to_correlate:\n",
    "        for iq_metric in iq_metrics_to_correlate:\n",
    "            # Ensure the columns exist before trying to access them\n",
    "            if uncert_metric in model_df.columns and iq_metric in model_df.columns:\n",
    "                x = model_df[uncert_metric]\n",
    "                y = model_df[iq_metric]\n",
    "\n",
    "                pearson_corr, pearson_p = scipy.stats.pearsonr(x, y)\n",
    "                spearman_corr, spearman_p = scipy.stats.spearmanr(x, y)\n",
    "\n",
    "                correlation_results.append({\n",
    "                    'Uncertainty Metric': uncert_metric,\n",
    "                    'IQ Metric': iq_metric,\n",
    "                    'Pearson Correlation': pearson_corr,\n",
    "                    'Pearson p-value': pearson_p,\n",
    "                    'Spearman Correlation': spearman_corr,\n",
    "                    'Spearman p-value': spearman_p,\n",
    "                })\n",
    "\n",
    "    if correlation_results:\n",
    "        corr_df = pd.DataFrame(correlation_results)\n",
    "        display(corr_df.round(4))\n",
    "    else:\n",
    "        print(\"Could not compute any correlations.\")\n",
    "\n",
    "    # --- 3. Cross-Scan Correlation Scatter Plots ---\n",
    "    print(\"\\n--- Cross-Scan Correlation Scatter Plots ---\")\n",
    "    for uncert_metric in uncert_metrics_to_correlate:\n",
    "        num_iq_metrics = len(iq_metrics_to_correlate)\n",
    "        fig, axes = plt.subplots(1, num_iq_metrics, figsize=(5 * num_iq_metrics, 4.5))\n",
    "        fig.suptitle(f'{model_name}: {uncert_metric} vs. Image Quality Metrics', fontsize=16)\n",
    "        \n",
    "        # Handle case where there's only one subplot\n",
    "        if num_iq_metrics == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for i, iq_metric in enumerate(iq_metrics_to_correlate):\n",
    "            if uncert_metric in model_df.columns and iq_metric in model_df.columns:\n",
    "                axes[i].scatter(model_df[uncert_metric], model_df[iq_metric], alpha=0.7)\n",
    "                axes[i].set_xlabel(uncert_metric)\n",
    "                axes[i].set_ylabel(iq_metric)\n",
    "                axes[i].set_title(f'{uncert_metric} vs. {iq_metric}')\n",
    "                axes[i].grid(True, linestyle=':')\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "# --- 4. Cross-Model Histogram Comparison ---\n",
    "print(f\"\\n{'='*25}\\nCROSS-MODEL HISTOGRAMS\\n{'='*25}\")\n",
    "\n",
    "# Plotting combined error histograms for all models (with range limits)\n",
    "plt.figure(figsize=(12, 6))\n",
    "for model_name, errors_list in all_model_errors.items():\n",
    "    if errors_list:\n",
    "        combined_errors = np.concatenate(errors_list)\n",
    "        range_lims = np.percentile(combined_errors, [0.1, 99.9])\n",
    "        plt.hist(combined_errors, bins=200, log=True, range=range_lims, alpha=0.6, label=model_name)\n",
    "plt.title('Combined Raw Error Distribution (All Models, 0.1-99.9% Range)')\n",
    "plt.xlabel('Error (GT - Prediction)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.show()\n",
    "\n",
    "# Plotting combined error histograms for all models (no range limits)\n",
    "plt.figure(figsize=(12, 6))\n",
    "for model_name, errors_list in all_model_errors.items():\n",
    "    if errors_list:\n",
    "        combined_errors = np.concatenate(errors_list)\n",
    "        plt.hist(combined_errors, bins=200, log=True, alpha=0.6, label=model_name)\n",
    "plt.title('Combined Raw Error Distribution (All Models, Full Range)')\n",
    "plt.xlabel('Error (GT - Prediction)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.show()\n",
    "\n",
    "# Plotting combined std dev histograms for all models (with range limits)\n",
    "plt.figure(figsize=(12, 6))\n",
    "for model_name, stds_list in all_model_stds.items():\n",
    "    if stds_list:\n",
    "        combined_stds = np.concatenate(stds_list)\n",
    "        range_lims = [0, np.percentile(combined_stds, 99.9)]\n",
    "        plt.hist(combined_stds, bins=200, log=True, range=range_lims, alpha=0.6, label=model_name)\n",
    "plt.title('Combined Std Deviation Distribution (All Models, (0-99.9% Range))')\n",
    "plt.xlabel('Predicted Standard Deviation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.show()\n",
    "\n",
    "# Plotting combined std dev histograms for all models (no range limits)\n",
    "plt.figure(figsize=(12, 6))\n",
    "for model_name, stds_list in all_model_stds.items():\n",
    "    if stds_list:\n",
    "        combined_stds = np.concatenate(stds_list)\n",
    "        plt.hist(combined_stds, bins=200, log=True, alpha=0.6, label=model_name)\n",
    "plt.title('Combined Std Deviation Distribution (All Models, Full Range)')\n",
    "plt.xlabel('Predicted Standard Deviation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad791ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_csv('MCdropout_FDK_results_val.csv')\n",
    "\n",
    "# Only aggregate numeric columns\n",
    "numeric_cols = results_df.select_dtypes(include=[np.number]).columns\n",
    "summary = results_df.groupby('model_name')[numeric_cols].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# save results_df and summary to CSV files\n",
    "# results_df.to_csv('MCdropout_FDK_results_val.csv', index=False)\n",
    "# summary.to_csv('MCdropout_FDK_summary_val.csv', index=False)\n",
    "\n",
    "# summaries = [\n",
    "#     ('BBB pi=0.75 mu=0.0 sigma1=1e-1 sigma2=1e-3 beta=1e-2 (50)', 'BBB_summary_test.csv'),\n",
    "#     ('MC Dropoout 30% (50)', 'MCdropout_30_summary_test.csv'),\n",
    "#     ('Ensemble (10)', 'ensemble_summary_test.csv'),\n",
    "# ]\n",
    "\n",
    "# # Read and concatenate all summary files\n",
    "# import pandas as pd\n",
    "# summary_list = []\n",
    "# for name, path in summaries:\n",
    "#     summary_df = pd.read_csv(path, header=[0, 1])  # Read with multi-index columns\n",
    "\n",
    "#     # Note there might be multiple rows\n",
    "#     # We only take the row that matches the model name\n",
    "#     # Find the row(s) where the model_name matches the given name (allow partial match for ensemble size)\n",
    "#     row_mask = summary_df[('model_name', 'Unnamed: 0_level_1')] == name\n",
    "#     summary_df = summary_df[row_mask].reset_index(drop=True)\n",
    "\n",
    "#     summary_df['model_name'] = name  # Add model name column\n",
    "#     summary_list.append(summary_df)\n",
    "# summary = pd.concat(summary_list, ignore_index=True)\n",
    "# numeric_cols = [col for col, id in summary.select_dtypes(include=[np.number]).columns if id == 'mean']\n",
    "\n",
    "# Prepare summary_display with formatted mean Â± std for each metric\n",
    "summary_display = pd.DataFrame()\n",
    "summary_display['model_name'] = summary['model_name']\n",
    "for col in numeric_cols:\n",
    "    mean_col = (col, 'mean')\n",
    "    std_col = (col, 'std')\n",
    "    summary_display[col] = summary[mean_col].map('{:.4f}'.format) + ' Â± ' + summary[std_col].map('{:.4f}'.format)\n",
    "\n",
    "print(\"\\n\\n=======================================================\")\n",
    "print(\"               Model Comparison Summary\")\n",
    "print(\"=======================================================\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(summary_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede1b72",
   "metadata": {},
   "source": [
    "BBB NOTES:\n",
    "BBB pi=0.75, sigma1=0.1, sigma2=0.001, beta=0.01: decent (small hallucination) <-- KEEP\n",
    "BBB pi=0.25, sigma1=0.1, sigma2=0.001, beta=0.01: decent (bad metrics)\n",
    "BBB pi=0.5, sigma1=0.1, sigma2=0.001, beta=0.01: bad (big hallucination)\n",
    "BBB pi=0.5, sigma1=0.3, sigma2=0.001, beta=0.01: bad (hallucination)\n",
    "BBB pi=0.5, sigma1=0.03, sigma2=0.001, beta=0.01: decent (ok-ish image quality) <-- KEEP\n",
    "BBB pi=0.5, sigma1=0.01, sigma2=0.003, beta=0.01: decent (ok-ish image quality) <-- KEEP\n",
    "BBB pi=0.5, sigma1=0.01, sigma2=0.03, beta=0.01: bad texture in air\n",
    "BBB pi=0.5, sigma1=0.01, sigma2=0.001, beta=0.001: bad texture in air, and hallucination\n",
    "BBB pi=0.5, sigma1=0.01, sigma2=0.001, beta=0.1: hallucination\n",
    "BBB pi=0.75, sigma1=0.3, sigma2=0.001, beta=0.01: hallucination\n",
    "BBB pi=0.75, sigma1=0.03, sigma2=0.001, beta=0.01: minor hallucination outside <-- KEEP\n",
    "BBB pi=0.75, sigma1=0.1, sigma2=0.003, beta=0.01: minor hallucinations outside <-- KEEP\n",
    "BBB pi=0.75, sigma1=0.1, sigma2=0.0003, beta=0.01: hallucination\n",
    "BBB pi=0.75, sigma1=0.1, sigma2=0.001, beta=0.001: hallucination\n",
    "BBB pi=0.75, sigma1=0.01, sigma2=0.001, beta=0.1: minor hallucination outside <-- KEEP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad6420",
   "metadata": {},
   "source": [
    "### Compare results across models/ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa95f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the trends of image quality and uncrtainty metrics over ensemble size\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.read_csv('MCdropout_30_summary_val.csv')\n",
    "\n",
    "# Now plot the trends of image quality and uncertainty metrics over ensemble size\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Note that columns come in pairs: mean and std like this \"mean_ssim\", \"mean_ssim.1\" and the second one is the std\n",
    "# and we also need to skip the first row after the headers\n",
    "columns = results_df.columns[1:]  # Skip the first column which is 'model_name'\n",
    "# delete the first row after the headers\n",
    "results_df = results_df.iloc[1:]  # Skip the first row after the headers\n",
    "ensemble_sizes = results_df['model_name'].str.extract(r'\\((\\d+)\\)').astype(int).values.flatten()\n",
    "# Plot each metric on a separate plot\n",
    "# Group columns by metric type\n",
    "ssim_cols = [col for col in columns if 'ssim' in col and not col.endswith('.1')]\n",
    "psnr_cols = [col for col in columns if 'psnr' in col and not col.endswith('.1')]\n",
    "mae_cols = [col for col in columns if 'mae' in col and not col.endswith('.1')]\n",
    "mse_cols = [col for col in columns if 'mse' in col and not col.endswith('.1')]\n",
    "ause_cols = [col for col in columns if 'ause' in col and not col.endswith('.1')]\n",
    "corr_cols = [col for col in columns if 'corr' in col and not col.endswith('.1')]\n",
    "\n",
    "def plot_metrics(metric_cols, title):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in metric_cols:\n",
    "        metric_mean = results_df[col].astype(float)\n",
    "        metric_std = results_df[f\"{col}.1\"].astype(float)\n",
    "        plt.errorbar(ensemble_sizes, metric_mean, yerr=metric_std, label=col, fmt='-o', capsize=5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Ensemble Size')\n",
    "    plt.xticks(ensemble_sizes)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot grouped metrics\n",
    "if ssim_cols:\n",
    "    plot_metrics(ssim_cols, 'SSIM Metrics Over Ensemble Size')\n",
    "if psnr_cols:\n",
    "    plot_metrics(psnr_cols, 'PSNR Metrics Over Ensemble Size')\n",
    "if mae_cols:\n",
    "    plot_metrics(mae_cols, 'MAE Metrics Over Ensemble Size')\n",
    "if mse_cols:\n",
    "    plot_metrics(mse_cols, 'MSE Metrics Over Ensemble Size')\n",
    "if ause_cols:\n",
    "    plot_metrics(ause_cols, 'AUSE Metrics Over Ensemble Size')\n",
    "if corr_cols:\n",
    "    plot_metrics(corr_cols, 'Correlation Metrics Over Ensemble Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32642ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of CSVs to compare\n",
    "csv_files = [\n",
    "    # 'MCdropout_15_summary_val.csv',\n",
    "    # 'MCdropout_30_summary_val.csv',\n",
    "    # 'MCdropout_50_summary_val.csv',\n",
    "    # 'ensemble_summary_val.csv',\n",
    "    'MCdropout_30_summary_test.csv',\n",
    "    # 'ensemble_summary_test.csv',\n",
    "    'BBB_summary_test.csv'\n",
    "    # Add more CSV file paths here\n",
    "]\n",
    "\n",
    "results_dfs = [pd.read_csv(csv) for csv in csv_files]\n",
    "labels = [csv.split('.')[0] for csv in csv_files]\n",
    "colors = plt.cm.tab10.colors  # Up to 10 distinct colors\n",
    "\n",
    "def get_metric_cols(columns, metric):\n",
    "    return [col for col in columns if metric in col and not col.endswith('.1')]\n",
    "\n",
    "metrics = ['ssim', 'psnr', 'mae', 'mse', 'ause', 'corr']\n",
    "metric_titles = {\n",
    "    'ssim': 'SSIM Metrics Over Ensemble Size',\n",
    "    'psnr': 'PSNR Metrics Over Ensemble Size',\n",
    "    'mae': 'MAE Metrics Over Ensemble Size',\n",
    "    'mse': 'MSE Metrics Over Ensemble Size',\n",
    "    'ause': 'AUSE Metrics Over Ensemble Size',\n",
    "    'corr': 'Correlation Metrics Over Ensemble Size'\n",
    "}\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # Find the minimum max ensemble size across all CSVs\n",
    "    max_x = min([\n",
    "        results_df.iloc[1:]['model_name'].str.extract(r'\\((\\d+)\\)').astype(int).values.flatten().max()\n",
    "        for results_df in results_dfs\n",
    "    ])\n",
    "    for i, results_df in enumerate(results_dfs):\n",
    "        columns = results_df.columns[1:]\n",
    "        results_df = results_df.iloc[1:]\n",
    "        ensemble_sizes = results_df['model_name'].str.extract(r'\\((\\d+)\\)').astype(int).values.flatten()\n",
    "        metric_cols = get_metric_cols(columns, metric)\n",
    "        for j, col in enumerate(metric_cols):\n",
    "            metric_mean = results_df[col].astype(float).values\n",
    "            # Sort by ensemble size\n",
    "            sorted_idx = np.argsort(ensemble_sizes)\n",
    "            sorted_ensemble_sizes = ensemble_sizes[sorted_idx]\n",
    "            sorted_metric_mean = metric_mean[sorted_idx]\n",
    "            linestyle = '-' if j == 0 else '--'\n",
    "            label = f\"{labels[i]}: {col}\"\n",
    "            # Only plot up to max_x\n",
    "            mask = sorted_ensemble_sizes <= max_x\n",
    "            plt.plot(\n",
    "                sorted_ensemble_sizes[mask], sorted_metric_mean[mask],\n",
    "                label=label,\n",
    "                color=colors[i % len(colors)], linestyle=linestyle, marker='o'\n",
    "            )\n",
    "    plt.title(metric_titles[metric])\n",
    "    plt.xlabel('Ensemble Size')\n",
    "    plt.xlim(left=None, right=max_x)\n",
    "    plt.xticks(np.arange(sorted_ensemble_sizes.min(), max_x + 1))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287da0be",
   "metadata": {},
   "source": [
    "## Post-calibration processing & results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calibration Analysis (Train on Validation, Evaluate on Test)\n",
    "# This cell performs a separate analysis focused on uncertainty calibration.\n",
    "# 1. It trains calibration models (Platt, Isotonic) for each model on the VALIDATION data.\n",
    "# 2. It then evaluates the uncalibrated and calibrated uncertainties on the TEST data.\n",
    "\n",
    "# This list will store dictionaries of post-calibration results\n",
    "calibration_results = []\n",
    "\n",
    "# Load the scan lists for validation and testing, assuming they don't change\n",
    "all_scans, _ = read_scans_agg_file(SCANS_AGG_FILE)\n",
    "validation_scans = all_scans['VALIDATION']\n",
    "test_scans = all_scans['TEST']\n",
    "\n",
    "for model_config in MODELS_TO_ANALYZE:\n",
    "    model_name = model_config['name']\n",
    "    domain = model_config['domain']\n",
    "    \n",
    "    # ======================================================================\n",
    "    # 1. CALIBRATION TRAINING on the VALIDATION set\n",
    "    # ======================================================================\n",
    "    print(f\"\\n--- Training calibration for model: {model_name} ---\")\n",
    "    \n",
    "    all_val_errors = []\n",
    "    all_val_uncertainties = []\n",
    "    all_val_means = []\n",
    "    all_val_gt = []\n",
    "\n",
    "    for scan_info in tqdm(validation_scans, desc=f\"Gathering validation data for {model_name}\", leave=False):\n",
    "        # Load GT and calculate uncalibrated predictions for this validation scan\n",
    "        gt_volume = load_ground_truth(FILES, scan_info, domain)\n",
    "        # The 'calculate_volume_metrics_2_pass' is efficient for getting mean and std dev\n",
    "        if model_config['type'] in ['stochastic', 'ensemble']:\n",
    "            _, mean_pred_vol, uncertainty_map_vol = calculate_volume_metrics_2_pass(\n",
    "                FILES, model_config, scan_info, gt_volume.to(DEVICE), DEVICE\n",
    "            )\n",
    "        elif model_config['type'] == 'evidential':\n",
    "            _, mean_pred_vol, uncertainty_map_vol = calculate_evidential_volume_metrics(\n",
    "                FILES, model_config, scan_info, gt_volume.to(DEVICE), DEVICE\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_config['type']}\")\n",
    "        \n",
    "        # Move results to CPU and convert to numpy for calibration training\n",
    "        gt_volume_np = gt_volume.cpu().numpy()\n",
    "        del gt_volume\n",
    "        mean_pred_vol_np = mean_pred_vol.cpu().numpy()\n",
    "        del mean_pred_vol\n",
    "        uncertainty_map_vol_np = uncertainty_map_vol.cpu().numpy()\n",
    "        del uncertainty_map_vol\n",
    "\n",
    "        errors_vol = np.abs(gt_volume_np - mean_pred_vol_np)\n",
    "        \n",
    "        all_val_errors.append(errors_vol.flatten())\n",
    "        all_val_uncertainties.append(uncertainty_map_vol_np.flatten())\n",
    "        # Store means and GT for CDF calibration\n",
    "        all_val_means.append(mean_pred_vol_np.flatten())\n",
    "        all_val_gt.append(gt_volume_np.flatten())\n",
    "\n",
    "\n",
    "    # Consolidate all validation data into single arrays\n",
    "    val_errors_full = np.concatenate(all_val_errors)\n",
    "    val_uncertainties_full = np.concatenate(all_val_uncertainties)\n",
    "    # Consolidate means and GT\n",
    "    val_means_full = np.concatenate(all_val_means)\n",
    "    val_gt_full = np.concatenate(all_val_gt)\n",
    "    \n",
    "    # Train the calibration models using the full validation dataset\n",
    "    print(\"Calculating Platt Scaler...\")\n",
    "    platt_scaler = calculate_platt_scaler(val_errors_full, val_uncertainties_full)\n",
    "    print(f\"Platt Scaler T={platt_scaler:.4f}\")\n",
    "    \n",
    "    print(\"Training Isotonic Regression model on variance...\")\n",
    "    isotonic_model = train_isotonic_regression(val_uncertainties_full, val_errors_full, DEVICE)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # # Train the CDF-based Isotonic Regression model\n",
    "    # print(\"Training Isotonic Regression model on CDF...\")\n",
    "    # iso_cdf_model = train_cdf_isotonic_regression(val_gt_full, val_means_full, val_uncertainties_full, DEVICE)\n",
    "    # print(\"Done.\")\n",
    "        \n",
    "    # Clean up memory from the training phase\n",
    "    del all_val_errors, all_val_uncertainties, val_errors_full, val_uncertainties_full\n",
    "    del all_val_means, all_val_gt, val_means_full, val_gt_full\n",
    "    gc.collect()\n",
    "\n",
    "    # ======================================================================\n",
    "    # 2. FINAL EVALUATION on the TEST set\n",
    "    # ======================================================================\n",
    "    print(f\"\\n--- Evaluating model: {model_name} on the TEST set ---\")\n",
    "    \n",
    "    for scan_info in tqdm(test_scans, desc=f\"Analyzing Test Scans for {model_name}\"):\n",
    "        patient, scan, _ = scan_info\n",
    "        scan_name = f\"p{patient}_{scan}\"\n",
    "        \n",
    "        # --- Data Loading & Base Uncalibrated Calculation ---\n",
    "        gt_volume = load_ground_truth(FILES, scan_info, domain)\n",
    "        gt_volume_np = gt_volume.cpu().numpy()\n",
    "        \n",
    "        if model_config['type'] in ['stochastic', 'ensemble']:\n",
    "            _, mean_pred_vol, uncal_uncertainty_map = calculate_volume_metrics_2_pass(\n",
    "                FILES, model_config, scan_info, gt_volume.to(DEVICE), DEVICE\n",
    "            )\n",
    "        elif model_config['type'] == 'evidential':\n",
    "            _, mean_pred_vol, uncal_uncertainty_map = calculate_evidential_volume_metrics(\n",
    "                FILES, model_config, scan_info, gt_volume.to(DEVICE), DEVICE\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_config['type']}\")\n",
    "        del gt_volume\n",
    "        \n",
    "        mean_pred_vol_np = mean_pred_vol.cpu().numpy()\n",
    "        del mean_pred_vol\n",
    "        uncal_uncertainty_map_np = uncal_uncertainty_map.cpu().numpy()\n",
    "        del uncal_uncertainty_map\n",
    "        \n",
    "        # --- Apply Calibrations ---\n",
    "        print(\"Applying calibrations...\")\n",
    "        platt_uncertainty_map_np = uncal_uncertainty_map_np * platt_scaler\n",
    "        iso_uncertainty_map_np = isotonic_model(uncal_uncertainty_map_np)\n",
    "        # # --- Apply the CDF-based calibration ---\n",
    "        # iso_cdf_uncertainty_map_np = apply_cdf_isotonic_regression(iso_cdf_model, uncal_uncertainty_map_np)\n",
    "\n",
    "        # --- Store results for this scan ---\n",
    "        scan_result = {'model_name': model_name, 'scan_name': scan_name}\n",
    "        \n",
    "        # --- Calculate and Store All Metrics (Uncalibrated, Platt, Isotonic) ---\n",
    "        calibrations = {\n",
    "            'platt': platt_uncertainty_map_np,\n",
    "            'iso_var': iso_uncertainty_map_np,\n",
    "            # 'iso_cdf': iso_cdf_uncertainty_map_np,\n",
    "        }\n",
    "\n",
    "        # We need the error map for AUSE calculation\n",
    "        errors_vol = np.abs(gt_volume_np - mean_pred_vol_np)\n",
    "        \n",
    "        print(\"Evaluating metrics for each calibration...\")\n",
    "        for cal_name, uncertainty_map in calibrations.items():\n",
    "            print(\"Calculating metrics for calibration:\", cal_name)\n",
    "\n",
    "            print(\"Calculating AUSE...\")\n",
    "            ause_val = calculate_ause_sparsification(uncertainty_map, errors_vol)\n",
    "            scan_result[f'ause_{cal_name}'] = ause_val\n",
    "\n",
    "            print(\"Calculating ECE...\")\n",
    "            for n_bins in [10, 20, 50]:\n",
    "                eces = calculate_all_eces(gt_volume_np, mean_pred_vol_np, uncertainty_map, n_bins)\n",
    "                for key, val in eces.items():\n",
    "                    scan_result[f'{key}_{n_bins}bins_{cal_name}'] = val\n",
    "\n",
    "            print(\"Calculating ENCE...\")         \n",
    "            for n_bins in [10, 20, 50]:\n",
    "                scan_result[f'ence_{n_bins}bins_{cal_name}'] = calculate_ence(gt_volume_np, mean_pred_vol_np, uncertainty_map, DEVICE, n_bins)\n",
    "            \n",
    "            print(\"Calculating NLL...\")\n",
    "            scan_result[f'nll_{cal_name}'] = calculate_nll(gt_volume_np, mean_pred_vol_np, uncertainty_map)\n",
    "            \n",
    "            print(\"Calculating MPIW...\")\n",
    "            mpiws = calculate_mpiw(uncertainty_map, confidence_levels=[0.68, 0.95])\n",
    "            for key, val in mpiws.items():\n",
    "                scan_result[f'{key}_{cal_name}'] = val\n",
    "            print(\"Done with metrics for\", cal_name)\n",
    "\n",
    "        calibration_results.append(scan_result)\n",
    "\n",
    "        # Platt Scaling Plots\n",
    "        print(f\"--- Generating combined calibration plots for {model_name} on {scan_name} ---\")\n",
    "        # plot_combined_calibration_curves(\n",
    "        #     ground_truth=gt_volume_np,\n",
    "        #     mean_pred=mean_pred_vol_np,\n",
    "        #     platt_uncertainty_map=calibrations['platt'],\n",
    "        #     iso_uncertainty_map=calibrations['iso_var'],\n",
    "        #     iso_cdf_uncertainty_map=calibrations['iso_cdf'],\n",
    "        #     model_name=model_name,\n",
    "        #     scan_name=scan_name,\n",
    "        #     n_bins=20\n",
    "        # )\n",
    "        plot_combined_calibration_curves_old(\n",
    "            ground_truth=gt_volume_np,\n",
    "            mean_pred=mean_pred_vol_np,\n",
    "            platt_uncertainty_map=calibrations['platt'],\n",
    "            iso_uncertainty_map=calibrations['iso_var'],\n",
    "            model_name=model_name,\n",
    "            scan_name=scan_name,\n",
    "            n_bins=20\n",
    "        )\n",
    "        \n",
    "        # --- Clean up memory ---\n",
    "        del gt_volume_np, mean_pred_vol_np, uncal_uncertainty_map_np, errors_vol\n",
    "        del platt_uncertainty_map_np, iso_uncertainty_map_np #, iso_cdf_uncertainty_map_np\n",
    "        gc.collect()\n",
    "\n",
    "# Convert results to a pandas DataFrame for easier analysis\n",
    "calibration_results_df = pd.DataFrame(calibration_results)\n",
    "\n",
    "print(\"\\n\\nâœ… Calibration analysis complete.\")\n",
    "calibration_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276442bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aggregate Results Summary\n",
    "\n",
    "# Select only the numeric columns for aggregation\n",
    "numeric_cols = calibration_results_df.select_dtypes(include=[np.number]).columns\n",
    "summary = calibration_results_df.groupby('model_name')[numeric_cols].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Save results_df and summary to CSV files\n",
    "calibration_results_df.to_csv('BBB_results_calibration.csv', index=False)\n",
    "summary.to_csv('BBB_summary_calibration.csv', index=False)\n",
    "\n",
    "# Prepare a display DataFrame with formatted 'mean Â± std' strings\n",
    "summary_display = pd.DataFrame()\n",
    "summary_display['model_name'] = summary['model_name']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    mean_col = (col, 'mean')\n",
    "    std_col = (col, 'std')\n",
    "    # Format the string, handling potential NaN values in std dev\n",
    "    summary_display[col] = summary[mean_col].map('{:.4f}'.format) + ' Â± ' + summary[std_col].map('{:.4f}'.format)\n",
    "\n",
    "print(\"\\\\n\\\\n=======================================================\")\n",
    "print(\"               Model Comparison Summary\")\n",
    "print(\"=======================================================\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(summary_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf98081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Combine multiple summary DataFrames from different CSVs\n",
    "import pandas as pd\n",
    "\n",
    "# List of summary CSV files to compare\n",
    "summary_csv_files = [\n",
    "    'MCdropout_30_summary_calibration.csv',\n",
    "    'ensemble_summary_calibration.csv',\n",
    "    'BBB_summary_calibration.csv',\n",
    "    # Add more summary CSV file paths here\n",
    "]\n",
    "columns_to_include = [\n",
    "    # 'ece_weighted_abs_20bins_platt',\n",
    "    'ece_unweighted_abs_20bins_platt',\n",
    "    'ence_20bins_platt',\n",
    "    'nll_platt',\n",
    "    # 'mpiw_68_platt',\n",
    "    'mpiw_95_platt',\n",
    "    # 'ece_weighted_abs_20bins_iso',\n",
    "    'ece_unweighted_abs_20bins_iso',\n",
    "    'ence_20bins_iso',\n",
    "    'nll_iso',\n",
    "    # 'mpiw_68_iso',\n",
    "    'mpiw_95_iso',\n",
    "]\n",
    "\n",
    "# Load each summary CSV as a DataFrame and append to a list\n",
    "summaries = [pd.read_csv(csv) for csv in summary_csv_files]\n",
    "\n",
    "for i in range(len(summaries)):\n",
    "    # Discard the first non-header row\n",
    "    summaries[i] = summaries[i].iloc[1:]\n",
    "\n",
    "    # Convert all columns to numeric (except 'model_name')\n",
    "    for col in summaries[i].columns:\n",
    "        if col != 'model_name':\n",
    "            summaries[i][col] = pd.to_numeric(summaries[i][col], errors='raise')\n",
    "\n",
    "# Concatenate all summary DataFrames row-wise\n",
    "if summaries:\n",
    "    combined_summary = pd.concat(summaries, ignore_index=True)\n",
    "    print(\"\\n\\n=======================================================\")\n",
    "    print(\"         Combined Model Calibration Summaries\")\n",
    "    print(\"=======================================================\")\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    # Display the dataframe with values formatted as 'mean Â± std'\n",
    "    for col in columns_to_include:\n",
    "        mean_col = col\n",
    "        std_col = col + '.1'\n",
    "        combined_summary[col] = combined_summary[mean_col].map('{:.4f}'.format) + ' Â± ' + combined_summary[std_col].map('{:.4f}'.format)\n",
    "    \n",
    "    # Keep only the relevant columns\n",
    "    combined_summary = combined_summary[['model_name'] + columns_to_include]\n",
    "    display(combined_summary)\n",
    "else:\n",
    "    print(\"No summary CSV files provided.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
