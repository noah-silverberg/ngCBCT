{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b5c8d3",
   "metadata": {},
   "source": [
    "### Setup/Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0320969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from pipeline.paths import Directories, Files\n",
    "from pipeline.utils import read_scans_agg_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec7764",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eaa8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASE = \"7\"\n",
    "DATA_VERSION = \"13\"\n",
    "WORK_ROOT = \"D:/NoahSilverberg/ngCBCT\"\n",
    "SPLIT_TO_ANALYZE = 'VALIDATION'  # Options: 'TRAIN', 'VALIDATION', 'TEST'\n",
    "\n",
    "MODELS_TO_ANALYZE = [\n",
    "    {\n",
    "        'name': 'MC Dropoout 30%',\n",
    "        'type': 'stochastic',\n",
    "        'domain': 'FDK',\n",
    "        'model_version_root': 'MK7_MCDROPOUT_30_pct_NEW',\n",
    "        'count': 20,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Ensemble',\n",
    "        'type': 'ensemble',\n",
    "        'domain': 'FDK',\n",
    "        'model_version_root': 'MK7',\n",
    "        'count': 5,\n",
    "    },\n",
    "    # Add other models here\n",
    "]\n",
    "\n",
    "# Path to the .pt file containing tumor locations.\n",
    "# This is a 5D tensor [patient, scan, (x, y, z)]\n",
    "TUMOR_LOCATIONS_FILE = 'D:/NoahSilverberg/ngCBCT/3D_recon/tumor_location.pt'\n",
    "\n",
    "# --- Advanced Config ---\n",
    "SCANS_AGG_FILE = 'scans_to_agg.txt'\n",
    "SSIM_KWARGS = {\"K1\": 0.03, \"K2\": 0.06, \"win_size\": 15}\n",
    "\n",
    "# --- Setup ---\n",
    "# Create Directories and Files objects\n",
    "phase_dataver_dir = os.path.join(WORK_ROOT, f\"phase{PHASE}\", f\"DS{DATA_VERSION}\")\n",
    "DIRECTORIES = Directories(\n",
    "    projections_results_dir=os.path.join(phase_dataver_dir, \"results\", \"projections\"),\n",
    "    projections_gated_dir=os.path.join(WORK_ROOT, \"gated\", \"prj_mat\"),\n",
    "    reconstructions_dir=os.path.join(phase_dataver_dir, \"reconstructions\"),\n",
    "    reconstructions_gated_dir=os.path.join(WORK_ROOT, \"gated\", \"fdk_recon\"),\n",
    "    images_results_dir=os.path.join(phase_dataver_dir, \"results\", \"images\"),\n",
    ")\n",
    "FILES = Files(DIRECTORIES)\n",
    "\n",
    "# Load the list of scans\n",
    "scans_agg, scan_type_agg = read_scans_agg_file(SCANS_AGG_FILE)\n",
    "analysis_scans = scans_agg[SPLIT_TO_ANALYZE][-1:] # TODO\n",
    "\n",
    "# Load tumor locations\n",
    "tumor_locations = torch.load(TUMOR_LOCATIONS_FILE, weights_only=False)\n",
    "\n",
    "print(f\"\\nConfiguration loaded.\")\n",
    "print(f\"Analyzing {len(analysis_scans)} scans from the '{SPLIT_TO_ANALYZE}' split.\")\n",
    "print(f\"Found {len(MODELS_TO_ANALYZE)} model(s) to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114d02c",
   "metadata": {},
   "source": [
    "### Data loading/prep functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b635426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth(files_obj: Files, scan_info, domain, slice_idx=None):\n",
    "    \"\"\"\n",
    "    Loads the ground truth data for a given scan and domain.\n",
    "    \"\"\"\n",
    "    patient, scan, scan_type = scan_info\n",
    "    \n",
    "    if domain == 'PROJ':\n",
    "        gt_path = files_obj.get_projections_results_filepath('fdk', patient, scan, scan_type, gated=True)\n",
    "        data = torch.from_numpy(scipy.io.loadmat(gt_path)['prj']).detach().permute(1, 0, 2)\n",
    "    elif domain == 'FDK':\n",
    "        gt_path = files_obj.get_recon_filepath(\"fdk\", patient, scan, scan_type, gated=True, ensure_exists=False)\n",
    "        data = torch.load(gt_path).detach()\n",
    "        data = data[20:-20, :, :]\n",
    "        data = 25. * torch.clip(data, min=0.0, max=0.04)\n",
    "    elif domain == 'IMAG':\n",
    "        # The ground truth for the IMAG domain is the FDK of the gated projection\n",
    "        gt_path = files_obj.get_recon_filepath(\"fdk\", patient, scan, scan_type, gated=True, ensure_exists=False)\n",
    "        data = torch.load(gt_path).detach()\n",
    "        data = data[20:-20, :, :]\n",
    "        data = 25. * torch.clip(data, min=0.0, max=0.04)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown domain: {domain}\")\n",
    "\n",
    "    if slice_idx is not None and data.ndim == 3:\n",
    "        return data[slice_idx]\n",
    "    return data\n",
    "\n",
    "def load_predictions(files_obj: Files, model_config, scan_info, slice_idx=None):\n",
    "    \"\"\"\n",
    "    Loads all predictions for a given model, scan, and domain.\n",
    "    \"\"\"\n",
    "    patient, scan, scan_type = scan_info\n",
    "    domain = model_config['domain']\n",
    "    root = model_config['model_version_root']\n",
    "    count = model_config['count']\n",
    "    model_type = model_config['type']\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"Loading {count} predictions for {model_config['name']}...\")\n",
    "    \n",
    "    for i in tqdm(range(count), desc=\"Loading predictions\", leave=False):\n",
    "        passthrough_num = None\n",
    "        model_version = root\n",
    "\n",
    "        if model_type == 'ensemble':\n",
    "            model_version = f\"{root}_{i+1:02d}\"\n",
    "        elif model_type == 'stochastic':\n",
    "            passthrough_num = i\n",
    "\n",
    "        if domain == 'PROJ':\n",
    "            pred_path = files_obj.get_projections_results_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            pred = torch.from_numpy(scipy.io.loadmat(pred_path)['prj']).detach().permute(1, 0, 2)\n",
    "        elif domain == 'FDK':\n",
    "            pred_path = files_obj.get_recon_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            pred = torch.load(pred_path).detach()\n",
    "            pred = pred[20:-20, :, :]\n",
    "            pred = 25. * torch.clip(pred, min=0.0, max=0.04)\n",
    "        elif domain == 'IMAG':\n",
    "            # This assumes the results are saved with the ID model version name\n",
    "            pred_path = files_obj.get_images_results_filepath(model_version, patient, scan, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            pred = torch.load(pred_path).detach()\n",
    "            pred = torch.squeeze(pred, dim=1)\n",
    "            pred = torch.permute(pred, (0, 2, 1))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown domain: {domain}\")\n",
    "            \n",
    "        predictions.append(pred)\n",
    "\n",
    "    predictions_tensor = torch.stack(predictions)\n",
    "    \n",
    "    if slice_idx is not None and predictions_tensor.ndim == 4:\n",
    "        return predictions_tensor[:, slice_idx, :, :]\n",
    "        \n",
    "    return predictions_tensor\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3cbb74",
   "metadata": {},
   "source": [
    "### Metric calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723009ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ause_sparsification(uncertainty, errors):\n",
    "    \"\"\"\n",
    "    Calculates the Area Under the Sparsification Error curve (AUSE) efficiently.\n",
    "    This version avoids the O(N^2) complexity of the naive implementation by using\n",
    "    a vectorized approach with cumulative sums.\n",
    "\n",
    "    Args:\n",
    "        uncertainty (np.ndarray): The uncertainty map (e.g., standard deviation).\n",
    "        errors (np.ndarray): The absolute error map (AE) between prediction and ground truth.\n",
    "\n",
    "    Returns:\n",
    "        float: The AUSE value.\n",
    "    \"\"\"\n",
    "    uncertainty_flat = uncertainty.flatten()\n",
    "    errors_flat = errors.flatten()\n",
    "\n",
    "    # Sort errors based on descending uncertainty for the model curve\n",
    "    model_sorted_indices = np.argsort(uncertainty_flat)[::-1]\n",
    "    model_sorted_errors = errors_flat[model_sorted_indices]\n",
    "\n",
    "    # Sort errors based on descending error for the oracle curve\n",
    "    oracle_sorted_errors = np.sort(errors_flat)[::-1]\n",
    "\n",
    "    # --- Efficiently calculate the sparsification curves ---\n",
    "    def get_sparsification_curve_fast(sorted_errs):\n",
    "        n_pixels = len(sorted_errs)\n",
    "        # Calculate cumulative sum of errors once (O(N))\n",
    "        cumulative_errors = np.cumsum(sorted_errs)\n",
    "        total_error_sum = cumulative_errors[-1]\n",
    "\n",
    "        # Calculate the sum of errors removed at each step k using the pre-computed sum\n",
    "        sum_errors_removed = np.insert(cumulative_errors[:-1], 0, 0)\n",
    "\n",
    "        # Vectorized calculation of remaining error sums\n",
    "        sum_errors_remaining = total_error_sum - sum_errors_removed\n",
    "\n",
    "        # Vectorized calculation of number of remaining pixels\n",
    "        n_remaining = np.arange(n_pixels, 0, -1)\n",
    "\n",
    "        # The sparsification curve (MAE of remaining pixels)\n",
    "        curve = sum_errors_remaining / n_remaining\n",
    "        return curve\n",
    "\n",
    "    model_curve = get_sparsification_curve_fast(model_sorted_errors)\n",
    "    oracle_curve = get_sparsification_curve_fast(oracle_sorted_errors)\n",
    "\n",
    "    # Calculate the area between the two curves\n",
    "    ause = np.mean(np.abs(model_curve - oracle_curve))\n",
    "\n",
    "    return ause\n",
    "\n",
    "\n",
    "def calculate_ece(ground_truth, mean_pred, uncertainty_map, n_levels=20):\n",
    "    \"\"\"\n",
    "    Calculates the calibration error for regression tasks.\n",
    "\n",
    "    This implementation is based on Equations 8 and 9 from Kuleshov et al., 2018,\n",
    "    \"Accurate Uncertainties for Deep Learning Using Calibrated Regression\"[cite: 526].\n",
    "    It is consistent with the logic in the paper and the provided GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        ground_truth (np.ndarray): The ground truth volume/image.\n",
    "        mean_pred (np.ndarray): The model's mean prediction volume/image.\n",
    "        uncertainty_map (np.ndarray): The model's uncertainty (std dev) volume/image.\n",
    "        n_levels (int): The number of confidence levels to use for the calculation.\n",
    "\n",
    "    Returns:\n",
    "        float: The calibration error score.\n",
    "    \"\"\"\n",
    "    gt_flat = ground_truth.flatten()\n",
    "    pred_flat = mean_pred.flatten()\n",
    "    uncert_flat = uncertainty_map.flatten()\n",
    "\n",
    "    # Step 1: Calculate the predicted CDF value F_t(y_t) for each ground truth point.\n",
    "    # This is the first step in building a calibration plot (Section 3.5 of the paper) and\n",
    "    # is consistent with the `pcdf` method in the provided `calibrated_regression.py`.\n",
    "    # We assume the predictive distribution is Gaussian, as is standard[cite: 815].\n",
    "    pred_cdfs = scipy.stats.norm.cdf(gt_flat, loc=pred_flat, scale=uncert_flat)\n",
    "\n",
    "    # Step 2: Define the expected confidence levels, p_j, as per Equation 8[cite: 836].\n",
    "    # These are the points on the x-axis of the calibration plot in Figure 3 of the paper[cite: 663, 698].\n",
    "    expected_confidence_levels = np.linspace(0, 1, n_levels)\n",
    "\n",
    "    # Step 3: Calculate the observed frequency, hat{p}_j, at each confidence level, as defined in Equation 8[cite: 836].\n",
    "    # This is the empirical CDF of the `pred_cdfs` values, evaluated at each p_j.\n",
    "    # It corresponds to the y-axis of the calibration plot.\n",
    "    observed_frequencies = np.array([\n",
    "        np.mean(pred_cdfs <= p_j) for p_j in expected_confidence_levels\n",
    "    ])\n",
    "\n",
    "    # Step 4: Calculate the final calibration error using Equation 9.\n",
    "    # The paper states, \"We used w_j = 1 in our experiments\"[cite: 842].\n",
    "    # This is the sum of squared errors between the calibration curve and the ideal diagonal line.\n",
    "    calibration_error = np.sum((expected_confidence_levels - observed_frequencies)**2)\n",
    "\n",
    "    return calibration_error\n",
    "\n",
    "def calculate_volume_metrics_iteratively(files_obj: Files, model_config, scan_info, gt_volume_np: np.ndarray):\n",
    "    \"\"\"\n",
    "    Loads one prediction volume at a time to calculate stats and metrics iteratively,\n",
    "    avoiding high memory usage. Metrics like SSIM/PSNR are averaged over all slices.\n",
    "    Computes mean and uncertainty using Welford's algorithm for online variance.\n",
    "    \"\"\"\n",
    "    # --- Online Statistics Initialization for the entire volume ---\n",
    "    n_samples = model_config['count']\n",
    "    mean_volume_np = np.zeros_like(gt_volume_np, dtype=np.float32)\n",
    "    m2_volume_np = np.zeros_like(gt_volume_np, dtype=np.float32)\n",
    "\n",
    "    # --- Iterative Metric Initialization ---\n",
    "    sample_avg_ssims, sample_avg_psnrs, sample_avg_mses, sample_avg_maes = [], [], [], []\n",
    "    data_range = np.max(gt_volume_np) - np.min(gt_volume_np)\n",
    "\n",
    "    # --- Prediction Generator ---\n",
    "    def prediction_generator():\n",
    "        patient, scan, scan_type = scan_info\n",
    "        domain = model_config['domain']\n",
    "        root = model_config['model_version_root']\n",
    "        model_type = model_config['type']\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            passthrough_num = None\n",
    "            model_version = root\n",
    "            if model_type == 'ensemble':\n",
    "                model_version = f\"{root}_{i+1:02d}\"\n",
    "            elif model_type == 'stochastic':\n",
    "                passthrough_num = i\n",
    "\n",
    "            # Load the full prediction volume (slice_idx=None)\n",
    "            if domain == 'PROJ':\n",
    "                pred_path = files_obj.get_projections_results_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                pred = torch.from_numpy(scipy.io.loadmat(pred_path)['prj']).detach().permute(1, 0, 2)\n",
    "            elif domain == 'FDK':\n",
    "                pred_path = files_obj.get_recon_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                pred = torch.load(pred_path).detach()\n",
    "                pred = pred[20:-20, :, :]\n",
    "                pred = 25. * torch.clip(pred, min=0.0, max=0.04)\n",
    "            elif domain == 'IMAG':\n",
    "                pred_path = files_obj.get_images_results_filepath(model_version, patient, scan, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                pred = torch.load(pred_path).detach()\n",
    "                pred = torch.squeeze(pred, dim=1)\n",
    "                pred = torch.permute(pred, (0, 2, 1))\n",
    "\n",
    "            yield pred.cpu().numpy()\n",
    "\n",
    "    # --- Main Iteration Loop ---\n",
    "    for i, pred_volume_np in enumerate(tqdm(prediction_generator(), total=n_samples, desc=\"Iterative Metrics\", leave=False)):\n",
    "        # --- Calculate metrics for this sample by averaging over all slices ---\n",
    "        ssims, psnrs, mses, maes = [], [], [], []\n",
    "        # Check if the volume is sliceable (for FDK/IMAG)\n",
    "        is_sliceable = gt_volume_np.ndim > 2 and pred_volume_np.ndim > 2\n",
    "        \n",
    "        if is_sliceable:\n",
    "            for s in range(gt_volume_np.shape[0]):\n",
    "                gt_slice = gt_volume_np[s]\n",
    "                pred_slice = pred_volume_np[s]\n",
    "                slice_data_range = np.max(gt_slice) - np.min(gt_slice)\n",
    "                if slice_data_range == 0: continue # Skip empty slices\n",
    "                \n",
    "                ssims.append(ssim(gt_slice, pred_slice, data_range=slice_data_range, **SSIM_KWARGS))\n",
    "                psnrs.append(psnr(gt_slice, pred_slice, data_range=slice_data_range))\n",
    "                mses.append(np.mean((gt_slice - pred_slice)**2))\n",
    "                maes.append(np.mean(np.abs(gt_slice - pred_slice)))\n",
    "            \n",
    "            sample_avg_ssims.append(np.mean(ssims) if ssims else 0)\n",
    "            sample_avg_psnrs.append(np.mean(psnrs) if psnrs else 0)\n",
    "            sample_avg_mses.append(np.mean(mses) if mses else 0)\n",
    "            sample_avg_maes.append(np.mean(maes) if maes else 0)\n",
    "\n",
    "        # --- Update online statistics for the whole volume (Welford's Algorithm) ---\n",
    "        delta = pred_volume_np - mean_volume_np\n",
    "        mean_volume_np += delta / (i + 1)\n",
    "        delta2 = pred_volume_np - mean_volume_np\n",
    "        m2_volume_np += delta * delta2\n",
    "\n",
    "    # --- Finalize stats and metrics ---\n",
    "    uncertainty_volume_map = np.sqrt(m2_volume_np / n_samples) if n_samples > 1 else np.zeros_like(mean_volume_np)\n",
    "    metrics = {}\n",
    "\n",
    "    # Calculate metrics on the final mean prediction (averaged over slices)\n",
    "    mean_ssims, mean_psnrs, mean_mses, mean_maes = [], [], [], []\n",
    "    if is_sliceable:\n",
    "        for s in range(gt_volume_np.shape[0]):\n",
    "            gt_slice = gt_volume_np[s]\n",
    "            mean_slice = mean_volume_np[s]\n",
    "            slice_data_range = np.max(gt_slice) - np.min(gt_slice)\n",
    "            if slice_data_range == 0: continue\n",
    "            \n",
    "            mean_ssims.append(ssim(gt_slice, mean_slice, data_range=slice_data_range, **SSIM_KWARGS))\n",
    "            mean_psnrs.append(psnr(gt_slice, mean_slice, data_range=slice_data_range))\n",
    "            mean_mses.append(np.mean((gt_slice - mean_slice)**2))\n",
    "            mean_maes.append(np.mean(np.abs(gt_slice - mean_slice)))\n",
    "\n",
    "    metrics['mean_ssim'] = np.mean(mean_ssims) if mean_ssims else 0\n",
    "    metrics['mean_psnr'] = np.mean(mean_psnrs) if mean_psnrs else 0\n",
    "    metrics['mean_mse'] = np.mean(mean_mses) if mean_mses else 0\n",
    "    metrics['mean_mae'] = np.mean(mean_maes) if mean_maes else 0\n",
    "\n",
    "    # Add the averaged sample metrics\n",
    "    metrics['sample_avg_ssim'] = np.mean(sample_avg_ssims) if sample_avg_ssims else 0\n",
    "    metrics['sample_avg_psnr'] = np.mean(sample_avg_psnrs) if sample_avg_psnrs else 0\n",
    "    metrics['sample_avg_mse'] = np.mean(sample_avg_mses) if sample_avg_mses else 0\n",
    "    metrics['sample_avg_mae'] = np.mean(sample_avg_maes) if sample_avg_maes else 0\n",
    "\n",
    "    return metrics, mean_volume_np, uncertainty_volume_map\n",
    "\n",
    "print(\"Metric calculation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85d476",
   "metadata": {},
   "source": [
    "### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b12fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_comparison(mean_pred, ground_truth, uncertainty_map, model_name, scan_name, slice_idx, tumor_coords_xy=None):\n",
    "    \"\"\"Plots the GT, mean prediction, absolute error, and uncertainty map.\"\"\"\n",
    "    error_map = np.abs(ground_truth - mean_pred)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    fig.suptitle(f'{model_name} - {scan_name} - Slice {slice_idx} (Mean vs. GT)', fontsize=16)\n",
    "\n",
    "    im1 = axes[0].imshow(ground_truth, cmap='gray')\n",
    "    axes[0].set_title('Ground Truth')\n",
    "    axes[0].axis('off')\n",
    "    fig.colorbar(im1, ax=axes[0])\n",
    "\n",
    "    im2 = axes[1].imshow(mean_pred, cmap='gray')\n",
    "    axes[1].set_title('Mean Prediction')\n",
    "    axes[1].axis('off')\n",
    "    fig.colorbar(im2, ax=axes[1])\n",
    "\n",
    "    im3 = axes[2].imshow(error_map, cmap='magma')\n",
    "    axes[2].set_title('Absolute Error Map')\n",
    "    axes[2].axis('off')\n",
    "    fig.colorbar(im3, ax=axes[2])\n",
    "\n",
    "    im4 = axes[3].imshow(uncertainty_map, cmap='viridis')\n",
    "    axes[3].set_title('Uncertainty (Std Dev)')\n",
    "    axes[3].axis('off')\n",
    "    fig.colorbar(im4, ax=axes[3])\n",
    "\n",
    "    if tumor_coords_xy:\n",
    "        x, y = tumor_coords_xy\n",
    "        axes[0].annotate('', xy=(x, y), xytext=(x - 30, y - 30),\n",
    "                         arrowprops=dict(facecolor='red', edgecolor='red', shrink=0.05, width=1, headwidth=5, headlength=5))\n",
    "        axes[1].annotate('', xy=(x, y), xytext=(x - 30, y - 30),\n",
    "                         arrowprops=dict(facecolor='red', edgecolor='red', shrink=0.05, width=1, headwidth=5, headlength=5))\n",
    "        axes[2].annotate('', xy=(x, y), xytext=(x - 30, y - 30),\n",
    "                         arrowprops=dict(facecolor='red', edgecolor='red', shrink=0.05, width=1, headwidth=5, headlength=5))\n",
    "        axes[3].annotate('', xy=(x, y), xytext=(x - 30, y - 30),\n",
    "                         arrowprops=dict(facecolor='red', edgecolor='red', shrink=0.05, width=1, headwidth=5, headlength=5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ssim_map(ssim_map, model_name, scan_name):\n",
    "    \"\"\"Plots the SSIM map.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(ssim_map, cmap='viridis', vmin=0, vmax=1)\n",
    "    plt.title(f'SSIM Map - {model_name} - {scan_name}')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_calibration_curve(ground_truth, mean_pred, uncertainty_map, model_name, scan_name, n_levels=20):\n",
    "    \"\"\"\n",
    "    Plots the calibration curve as described in Kuleshov et al., 2018.\n",
    "    \"\"\"\n",
    "    gt_flat = ground_truth.flatten()\n",
    "    pred_flat = mean_pred.flatten()\n",
    "    uncert_flat = uncertainty_map.flatten()\n",
    "\n",
    "    pred_cdfs = scipy.stats.norm.cdf(gt_flat, loc=pred_flat, scale=uncert_flat)\n",
    "    expected_confidence_levels = np.linspace(0, 1, n_levels)\n",
    "    observed_frequencies = np.array([np.mean(pred_cdfs <= p_j) for p_j in expected_confidence_levels])\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot([0, 1], [0, 1], '--', color='grey', label='Perfectly Calibrated')\n",
    "    plt.plot(expected_confidence_levels, observed_frequencies, '-o', label='Model Calibration')\n",
    "    plt.xlabel('Expected Confidence Level')\n",
    "    plt.ylabel('Observed Confidence Level')\n",
    "    plt.title(f'Calibration Plot - {model_name} - {scan_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Visualization functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c49df",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list will store dictionaries of results for each scan and model\n",
    "all_results = []\n",
    "\n",
    "for model_config in MODELS_TO_ANALYZE:\n",
    "    model_name = model_config['name']\n",
    "    domain = model_config['domain']\n",
    "    \n",
    "    scan_results = []\n",
    "\n",
    "    for scan_info in tqdm(analysis_scans, desc=f\"Analyzing Model: {model_name}\"):\n",
    "        patient, scan, _ = scan_info\n",
    "        scan_name = f\"p{patient}_{scan}\"\n",
    "        \n",
    "        # --- Determine Domain and Plotting Slice ---\n",
    "        is_visual_domain = domain in ['FDK', 'IMAG']\n",
    "        plot_slice_idx = None\n",
    "        tumor_xy = None\n",
    "        \n",
    "        if is_visual_domain:\n",
    "            if 'tumor_locations' in locals() and tumor_locations is not None:\n",
    "                try:\n",
    "                    loc = tumor_locations[int(patient), int(scan)]\n",
    "                    tumor_xy = (loc[1].item(), loc[0].item())\n",
    "                    plot_slice_idx = int(loc[2].item()) - 20\n",
    "                except (IndexError, TypeError):\n",
    "                    print(f\"Warning: Could not find tumor location for {scan_name}. Plotting will not have an arrow.\")\n",
    "                    plot_slice_idx = 100\n",
    "            else:\n",
    "                plot_slice_idx = 100\n",
    "        \n",
    "        # --- Data Loading (Ground Truth Only) ---\n",
    "        gt_volume = load_ground_truth(FILES, scan_info, domain, slice_idx=None)\n",
    "        gt_volume_np = gt_volume.cpu().numpy()\n",
    "        \n",
    "        # --- Iterative Metric Calculation ---\n",
    "        print(\"Calculating metrics for the entire volume...\")\n",
    "        iq_metrics, mean_pred_vol, uncertainty_map_vol = calculate_volume_metrics_iteratively(\n",
    "            FILES, model_config, scan_info, gt_volume_np\n",
    "        )\n",
    "        \n",
    "        # --- Uncertainty Metric Calculation ---\n",
    "        errors_vol = np.abs(gt_volume_np - mean_pred_vol)\n",
    "        print(\"Calculating AUSE...\")\n",
    "        ause_val = calculate_ause_sparsification(uncertainty_map_vol, errors_vol)\n",
    "        print(\"Calculating ECE...\")\n",
    "        ece_val = calculate_ece(gt_volume_np, mean_pred_vol, uncertainty_map_vol)\n",
    "        print(\"DONE\")\n",
    "        \n",
    "        # --- Store Results ---\n",
    "        scan_result = {\n",
    "            'model_name': model_name,\n",
    "            'scan_name': scan_name,\n",
    "            **iq_metrics,\n",
    "            'ause': ause_val,\n",
    "            'ece': ece_val,\n",
    "        }\n",
    "        scan_results.append(scan_result)\n",
    "        \n",
    "        # --- Visualization (Conditional) ---\n",
    "        if is_visual_domain:\n",
    "            print(f\"\\n--- Results for {model_name} on {scan_name} (Plotting Slice: {plot_slice_idx}) ---\")\n",
    "            gt_slice_np = gt_volume_np[plot_slice_idx]\n",
    "            mean_pred_slice = mean_pred_vol[plot_slice_idx]\n",
    "            uncertainty_map_slice = uncertainty_map_vol[plot_slice_idx]\n",
    "            _, ssim_map_val = ssim(gt_slice_np, mean_pred_slice, \n",
    "                                  data_range=(np.max(gt_slice_np) - np.min(gt_slice_np)), \n",
    "                                  full=True, **SSIM_KWARGS)\n",
    "            \n",
    "            plot_mean_comparison(mean_pred_slice, gt_slice_np, uncertainty_map_slice, \n",
    "                                 model_name, scan_name, plot_slice_idx, tumor_coords_xy=tumor_xy)\n",
    "            plot_ssim_map(ssim_map_val, model_name, scan_name)\n",
    "            \n",
    "            # New call to plot the calibration curve\n",
    "            plot_calibration_curve(gt_volume_np, mean_pred_vol, uncertainty_map_vol, model_name, scan_name)\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\n--- Metrics calculated for {model_name} on {scan_name} (PROJ domain, no plots) ---\")\n",
    "            # Still plot the calibration curve for non-visual domains\n",
    "            plot_calibration_curve(gt_volume_np, mean_pred_vol, uncertainty_map_vol, model_name, scan_name)\n",
    "\n",
    "        # --- Clean up memory ---\n",
    "        del gt_volume, gt_volume_np, mean_pred_vol, uncertainty_map_vol, errors_vol\n",
    "        gc.collect()\n",
    "        \n",
    "    all_results.extend(scan_results)\n",
    "\n",
    "# Convert results to a pandas DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\n\\n✅ Analysis complete for all models and scans.\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f9dce6",
   "metadata": {},
   "source": [
    "### Summary/Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff11ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(MODELS_TO_ANALYZE) > 1:\n",
    "    summary_data = []\n",
    "    \n",
    "    # Only aggregate numeric columns\n",
    "    numeric_cols = results_df.select_dtypes(include=[np.number]).columns\n",
    "    summary = results_df.groupby('model_name')[numeric_cols].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    # Prepare summary_display with formatted mean ± std for each metric\n",
    "    summary_display = pd.DataFrame()\n",
    "    summary_display['model_name'] = summary['model_name']\n",
    "    for col in numeric_cols:\n",
    "        mean_col = (col, 'mean')\n",
    "        std_col = (col, 'std')\n",
    "        summary_display[col] = summary[mean_col].map('{:.4f}'.format) + ' ± ' + summary[std_col].map('{:.4f}'.format)\n",
    "    \n",
    "    print(\"\\n\\n=======================================================\")\n",
    "    print(\"               Model Comparison Summary\")\n",
    "    print(\"=======================================================\")\n",
    "    \n",
    "    display(summary_display)\n",
    "\n",
    "else:\n",
    "    print(\"\\nOnly one model was analyzed. No comparison table to generate.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
