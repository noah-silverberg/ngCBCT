{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4229da",
   "metadata": {},
   "source": [
    "# Setup Logging & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f3a1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a15cef",
   "metadata": {},
   "source": [
    "### Setting up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8c4bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pipeline:DEBUG mode is enabled. Detailed logs will be shown.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"pipeline\")\n",
    "\n",
    "# Show info messages if DEBUG mode is enabled\n",
    "if DEBUG:\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.debug(\"DEBUG mode is enabled. Detailed logs will be shown.\")\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.info(\"DEBUG mode is disabled. Only essential logs will be shown.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0476a6",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd091fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# We set up CUDA first to ensure it is configured correctly\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "CUDA_DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.proj import load_projection_mat, reformat_sinogram, interpolate_projections, pad_and_reshape, divide_sinogram\n",
    "from pipeline.aggregate_prj import aggregate_saved_projections\n",
    "from pipeline.aggregate_ct import aggregate_saved_recons\n",
    "from pipeline.apply_model import apply_model_to_projections, load_model\n",
    "# from .infer3d import inference_3d\n",
    "from pipeline.utils import ensure_dir, read_scans_agg_file\n",
    "import torch\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yaml\n",
    "import importlib\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import tigre.utilities.gpu as gpu\n",
    "from pipeline.FDK_half.FDK_half import FDKHalf\n",
    "from pipeline.utils import get_geometry\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    logger.error(\"CUDA is not available. Please check your PyTorch installation. Using CPU instead...this will be slow.\")\n",
    "    CUDA_DEVICE = \"cpu\"\n",
    "\n",
    "# TODO run FDK via: FFrecon_reconFDK(input_mat, output_mat); in file \"FFrecon_fullFDK.m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d9f9e",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18df6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scans to convert to PyTorch tensors\n",
    "# Put None if you don't have any scans to convert\n",
    "# See the README for how to write this file correctly\n",
    "# NOTE: This will throw an error if the scan has already been converted\n",
    "#       If you would like to re-convert a scan,\n",
    "#       you can delete the file manually\n",
    "# SCANS_CONVERT = 'scans_convert_to_pt.txt'\n",
    "SCANS_CONVERT = None\n",
    "\n",
    "# Phase of the project (all data, models, etc. will be saved under this phase)\n",
    "PHASE = \"7\"\n",
    "\n",
    "# If this data version already exists in this phase, it will be loaded\n",
    "# Otherwise it will be created using whatever the most updated data creation script is\n",
    "DATA_VERSION = '13'\n",
    "\n",
    "# Scans to use for training, val, and testing\n",
    "# Set this to None if you don't want to do any aggregation\n",
    "# See the README for how to write this file correctly\n",
    "# NOTE: This will throw an error if there are already aggregated scans\n",
    "#       (even if they are not the same as the ones in this file)\n",
    "#       If you would like to re-aggregate,\n",
    "#       you can delete the file manually or change the data version\n",
    "# SCANS_AGG = 'scans_to_agg.txt'\n",
    "SCANS_AGG = None\n",
    "\n",
    "# Whether to augment the data for the image domain\n",
    "# This will only be used if you are doing image domain aggregation\n",
    "AUGMENT_ID = True\n",
    "\n",
    "# List of yaml files that contain configurations for the pipeline\n",
    "# Each file should contain the paramters for a specific model/ensemble\n",
    "CONFIG_FILES = [\n",
    "    # \"config_01epoch.yaml\",\n",
    "    \"config_20epoch.yaml\",\n",
    "]\n",
    "\n",
    "# Base directory\n",
    "WORK_ROOT = \"E:/NoahSilverberg/ngCBCT\"\n",
    "\n",
    "# NSG_CBCT Path where the raw matlab data is stored\n",
    "NSG_CBCT_PATH = \"D:/MitchellYu/NSG_CBCT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa6e84",
   "metadata": {},
   "source": [
    "### Some immediate variable definitions and setting changes based on the configuration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38798306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pipeline:All directories are set up successfully.\n"
     ]
    }
   ],
   "source": [
    "# Directories derived from bases\n",
    "PHASE_DATAVER_DIR = os.path.join(\n",
    "    WORK_ROOT, f\"phase{PHASE}\", f\"DS{DATA_VERSION}\"\n",
    ")  # everything should go inside this directory\n",
    "MODEL_DIR = os.path.join(PHASE_DATAVER_DIR, \"model\")  # for trained models\n",
    "RESULT_DIR = os.path.join(PHASE_DATAVER_DIR, \"result\")  # for outputs of CNN\n",
    "AGG_DIR = os.path.join(\n",
    "    PHASE_DATAVER_DIR, \"agg\"\n",
    ")  # for aggregated data (for PD and ID training)\n",
    "RECON_DIR = os.path.join(\n",
    "    PHASE_DATAVER_DIR, \"recon\"\n",
    ")  # for reconstructions (FDK)\n",
    "\n",
    "# Make the folders if they don't already exist\n",
    "ensure_dir(PHASE_DATAVER_DIR)\n",
    "ensure_dir(MODEL_DIR)\n",
    "ensure_dir(RESULT_DIR)\n",
    "ensure_dir(AGG_DIR)\n",
    "ensure_dir(RECON_DIR)\n",
    "\n",
    "logger.debug(\"All directories are set up successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b49a2c",
   "metadata": {},
   "source": [
    "# Data Preparation: projection interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ccc147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:No scans to convert. Skipping projection data processing.\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output directories exist\n",
    "g_dir = os.path.join(WORK_ROOT, 'data_pt', 'prj', 'gated')\n",
    "ng_dir = os.path.join(WORK_ROOT, 'data_pt', 'prj', 'ng')\n",
    "ensure_dir(g_dir)\n",
    "ensure_dir(ng_dir)\n",
    "\n",
    "if SCANS_CONVERT is not None:\n",
    "    # Read the scans to convert file\n",
    "    with open(SCANS_CONVERT, \"r\") as f:\n",
    "        SCANS_CONVERT = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            patient, scan, scan_type = line.split()\n",
    "            SCANS_CONVERT.append((patient, scan, scan_type))\n",
    "\n",
    "    logger.debug(f\"Loaded scan list for conversion: {SCANS_CONVERT}\")\n",
    "\n",
    "    logger.info(\"Starting to process projection data...\")\n",
    "\n",
    "    for patient, scan, scan_type in SCANS_CONVERT:\n",
    "        # Load the projection data from the matlab files\n",
    "        odd_index, angles, prj = load_projection_mat(patient, scan, scan_type, NSG_CBCT_PATH)\n",
    "\n",
    "        # Log shapes of loaded data\n",
    "        logger.debug(f'Processing patient {patient}, scan {scan}, type {scan_type}')\n",
    "        logger.debug(f'Loaded odd_index shape: {odd_index.shape}')\n",
    "        logger.debug(f'Loaded angles shape: {angles.shape}')\n",
    "        logger.debug(f'Loaded projection shape: {prj.shape}')\n",
    "\n",
    "        # Flip and permute to get it in the right format\n",
    "        prj_gcbct, angles1 = reformat_sinogram(prj, angles)\n",
    "\n",
    "        # Log shapes after reformatting\n",
    "        logger.debug(f'Reformatted projection shape: {prj_gcbct.shape}')\n",
    "\n",
    "        # Simulate ngCBCT projections\n",
    "        prj_ngcbct_li = interpolate_projections(prj_gcbct, odd_index)\n",
    "\n",
    "        # Log shapes after interpolation\n",
    "        logger.debug(f'Interpolated ngCBCT projection shape: {prj_ngcbct_li.shape}')\n",
    "\n",
    "        # Split the projections into two halves so they are good dimensions for the CNN\n",
    "        combined_gcbct = divide_sinogram(pad_and_reshape(prj_gcbct), v_dim=512 if scan_type == \"HF\" else 256)\n",
    "        combined_ngcbct = divide_sinogram(pad_and_reshape(prj_ngcbct_li), v_dim=512 if scan_type == \"HF\" else 256)\n",
    "\n",
    "        # Log shapes after dividing sinograms\n",
    "        logger.debug(f'Combined gCBCT shape: {combined_gcbct.shape}')\n",
    "        logger.debug(f'Combined ngCBCT shape: {combined_ngcbct.shape}')\n",
    "\n",
    "        logger.debug(f'Saving projections...')\n",
    "        \n",
    "        # NOTE: These need to have the same name since later we will aggregate them, and we just sort by the name\n",
    "        g_path = os.path.join(g_dir, f'{scan_type}_p{patient}_{scan}.pt')\n",
    "        ng_path = os.path.join(ng_dir, f'{scan_type}_p{patient}_{scan}.pt')\n",
    "\n",
    "        # Make sure the files do not already exist\n",
    "        if os.path.exists(g_path) or os.path.exists(ng_path):\n",
    "            raise FileExistsError(f\"Projection files already exist for patient {patient}, scan {scan}, type {scan_type}. Please delete them before re-running.\")\n",
    "        \n",
    "        # Save the projections\n",
    "        torch.save(combined_gcbct, g_path) # e.g., HF_p01_01.pt\n",
    "        torch.save(combined_ngcbct, ng_path)\n",
    "\n",
    "        logger.debug(f'Done with patient {patient}, scan {scan}, type {scan_type}\\n')\n",
    "\n",
    "    logger.info(\"All projections saved successfully.\")\n",
    "    logger.info(\"Gated projections saved in: %s\", g_dir)\n",
    "    logger.info(\"Nonstop-gated projections saved in: %s\", ng_dir)\n",
    "\n",
    "    # Free up memory\n",
    "    del odd_index, angles, prj, prj_gcbct, angles1, prj_ngcbct_li, combined_gcbct, combined_ngcbct\n",
    "else:\n",
    "    logger.info(\"No scans to convert. Skipping projection data processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23247d",
   "metadata": {},
   "source": [
    "### DEBUG: Sample projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b21297",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG and SCANS_CONVERT is not None:\n",
    "    # Pick the first HF scan and first FF scan\n",
    "    hf_scan = None\n",
    "    ff_scan = None\n",
    "    for patient, scan, scan_type in SCANS_CONVERT:\n",
    "        if scan_type == \"HF\":\n",
    "            hf_scan = (patient, scan, scan_type)\n",
    "            break\n",
    "    for patient, scan, scan_type in SCANS_CONVERT:\n",
    "        if scan_type == \"FF\":\n",
    "            ff_scan = (patient, scan, scan_type)\n",
    "            break\n",
    "\n",
    "    # Display the first HF scan\n",
    "    # Show the gated and nonstop-gated on subplots\n",
    "    if hf_scan:\n",
    "        hf_patient, hf_scan_num, hf_scan_type = hf_scan\n",
    "        hf_gated_prj = torch.load(os.path.join(g_dir, f'{hf_scan_type}_p{hf_patient}_{hf_scan_num}.pt'))\n",
    "        hf_ng_prj = torch.load(os.path.join(ng_dir, f'{hf_scan_type}_p{hf_patient}_{hf_scan_num}.pt'))\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(hf_gated_prj[0, 0, :, :].cpu().numpy(), cmap='gray')\n",
    "        plt.title(f'Gated Projection - {hf_scan_type} p{hf_patient}_{hf_scan_num}')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(hf_ng_prj[0, 0, :, :].cpu().numpy(), cmap='gray')\n",
    "        plt.title(f'Nonstop-Gated Projection - {hf_scan_type} p{hf_patient}_{hf_scan_num}')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Free up memory\n",
    "        del hf_gated_prj, hf_ng_prj\n",
    "\n",
    "    # Repeat for FF scan\n",
    "    if ff_scan:\n",
    "        ff_patient, ff_scan_num, ff_scan_type = ff_scan\n",
    "        ff_gated_prj = torch.load(os.path.join(g_dir, f'{ff_scan_type}_p{ff_patient}_{ff_scan_num}.pt'))\n",
    "        ff_ng_prj = torch.load(os.path.join(ng_dir, f'{ff_scan_type}_p{ff_patient}_{ff_scan_num}.pt'))\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(ff_gated_prj[0, 0, :, :].cpu().numpy(), cmap='gray')\n",
    "        plt.title(f'Gated Projection - {ff_scan_type} p{ff_patient}_{ff_scan_num}')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(ff_ng_prj[0, 0, :, :].cpu().numpy(), cmap='gray')\n",
    "        plt.title(f'Nonstop-Gated Projection - {ff_scan_type} p{ff_patient}_{ff_scan_num}')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Free up memory\n",
    "        del ff_gated_prj, ff_ng_prj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e695f",
   "metadata": {},
   "source": [
    "# Aggregate projections for train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "188d3403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline:No scans to aggregate. Skipping projection data aggregation.\n"
     ]
    }
   ],
   "source": [
    "if SCANS_AGG is not None:\n",
    "    SCANS_AGG, scan_type = read_scans_agg_file(SCANS_AGG)\n",
    "    logger.debug(f\"Loaded scan list for aggregation: {SCANS_AGG}\")\n",
    "\n",
    "    # Only aggregate projections if they don't already exist\n",
    "    if len([f for f in os.listdir(AGG_DIR) if f.startswith(\"PROJ\")]) > 0:\n",
    "        raise FileExistsError(f\"Aggregated projection data for phase {PHASE} data version {DATA_VERSION} already exists in {AGG_DIR}. Please delete the existing files or change the data version to re-aggregate.\")\n",
    "    else:\n",
    "        logger.info(\"Starting to aggregate projection data...\")\n",
    "        pt_prj_dir = os.path.join('G:', 'data_pt', 'prj')\n",
    "        # Aggregate and save projection data sets\n",
    "        for sample in ['TRAIN', 'VALIDATION', 'TEST']:\n",
    "            if len(SCANS_AGG[sample]):\n",
    "                prj_ngcbct = aggregate_saved_projections(scan_type, sample, pt_prj_dir, SCANS_AGG, truth=False)\n",
    "                np.save(os.path.join(AGG_DIR, f\"PROJ_ng_{scan_type}_{sample}.npy\"), prj_ngcbct.numpy()) # e.g., PROJ_ng_HF_TRAIN.npy\n",
    "                del prj_ngcbct\n",
    "                logger.debug(\"Done with nonstop-gated...\")\n",
    "                prj_gcbct = aggregate_saved_projections(scan_type, sample, pt_prj_dir, SCANS_AGG, truth=True)\n",
    "                np.save(os.path.join(AGG_DIR, f\"PROJ_gated_{scan_type}_{sample}.npy\"), prj_gcbct.numpy())\n",
    "                del prj_gcbct\n",
    "                logger.debug(\"Done with gated...\")\n",
    "\n",
    "                logger.debug(f\"Aggregated projections saved for {scan_type} {sample}.\\n\")\n",
    "            else:\n",
    "                logger.debug(f\"No scans to aggregate for {scan_type} {sample}. Skipping aggregation.\")\n",
    "\n",
    "        # Free up memory\n",
    "        del prj_gcbct, prj_ngcbct\n",
    "\n",
    "    logger.info(\"Projection data aggregation completed successfully.\")\n",
    "    logger.info(\"Aggregated projection data saved in: %s\", AGG_DIR)\n",
    "else:\n",
    "    logger.info(\"No scans to aggregate. Skipping projection data aggregation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae3e79",
   "metadata": {},
   "source": [
    "# Training PD CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daead9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pipeline:Loaded configuration from config_20epoch.yaml\n",
      "DEBUG:pipeline:Using CUDA; 1 devices.\n",
      "DEBUG:pipeline:Loaded class TrainingApp from module train_app_MK6_numpy\n",
      "INFO:pipeline:Going to try training the 1-th model with configuration from config_20epoch.yaml...\n",
      "DEBUG:pipeline:Starting TrainingApp, {'training': True, 'training_app': 'train_app_MK6_numpy.TrainingApp', 'epochs': 20, 'learning_rate': 0.001, 'network_name': 'IResNet', 'model_version': 'MK7_01', 'batch_size': 8, 'optimizer': 'NAdam', 'num_workers': 0, 'shuffle': True, 'grad_clip': True, 'grad_max': 0.01, 'betas_NAdam': (0.9, 0.999), 'momentum_decay_NAdam': 0.0004, 'momentum_SGD': 0.99, 'weight_decay_SGD': 1e-08, 'checkpoint_save_step': 5, 'tensor_board': False, 'tensor_board_comment': '', 'train_during_inference': False, 'ensemble_size': 3, 'scan_type': 'HF', 'input_type': None, 'data_version': '13', 'domain': 'PROJ', 'MODEL_DIR': 'E:/NoahSilverberg/ngCBCT\\\\phase7\\\\DS13\\\\model', 'AGG_DIR': 'E:/NoahSilverberg/ngCBCT\\\\phase7\\\\DS13\\\\agg'}\n",
      "DEBUG:pipeline:TRAIN images path: E:/NoahSilverberg/ngCBCT\\phase7\\DS13\\agg\\PROJ_ng_HF_TRAIN.npy\n",
      "DEBUG:pipeline:TRAIN ground truth images path: E:/NoahSilverberg/ngCBCT\\phase7\\DS13\\agg\\PROJ_gated_HF_TRAIN.npy\n",
      "DEBUG:pipeline:TRAIN dataset loaded with 32088 samples, each with shape torch.Size([1, 512, 512]).\n",
      "DEBUG:pipeline:TRAIN dataloader initialized with 4011 batches of size 8, with 0 workers, shuffle=True, and pin_memory=True.\n",
      "DEBUG:pipeline:VALIDATION images path: E:/NoahSilverberg/ngCBCT\\phase7\\DS13\\agg\\PROJ_ng_HF_VALIDATION.npy\n",
      "DEBUG:pipeline:VALIDATION ground truth images path: E:/NoahSilverberg/ngCBCT\\phase7\\DS13\\agg\\PROJ_gated_HF_VALIDATION.npy\n",
      "DEBUG:pipeline:VALIDATION dataset loaded with 4584 samples, each with shape torch.Size([1, 512, 512]).\n",
      "DEBUG:pipeline:VALIDATION dataloader initialized with 573 batches of size 8, with 0 workers, shuffle=True, and pin_memory=True.\n",
      "DEBUG:pipeline:Optimizer: NAdam with learning rate 0.001, betas (0.9, 0.999), momentum_decay 0.0004\n",
      "INFO:pipeline:TRAINING SETTINGS:\n",
      "INFO:pipeline:{'training': True, 'training_app': 'train_app_MK6_numpy.TrainingApp', 'epochs': 20, 'learning_rate': 0.001, 'network_name': 'IResNet', 'model_version': 'MK7_01', 'batch_size': 8, 'optimizer': 'NAdam', 'num_workers': 0, 'shuffle': True, 'grad_clip': True, 'grad_max': 0.01, 'betas_NAdam': (0.9, 0.999), 'momentum_decay_NAdam': 0.0004, 'momentum_SGD': 0.99, 'weight_decay_SGD': 1e-08, 'checkpoint_save_step': 5, 'tensor_board': False, 'tensor_board_comment': '', 'train_during_inference': False, 'ensemble_size': 3, 'scan_type': 'HF', 'input_type': None, 'data_version': '13', 'domain': 'PROJ', 'MODEL_DIR': 'E:/NoahSilverberg/ngCBCT\\\\phase7\\\\DS13\\\\model', 'AGG_DIR': 'E:/NoahSilverberg/ngCBCT\\\\phase7\\\\DS13\\\\agg'}\n",
      "INFO:pipeline:STARTING TRAINING...\n",
      "DEBUG:pipeline:Learning rate is 0.001 at epoch 1.\n",
      "DEBUG:pipeline:Model set to training mode for training.\n",
      "Epoch 1 Training:  14%|█▍        | 581/4011 [07:26<43:19,  1.32it/s]"
     ]
    }
   ],
   "source": [
    "for config_file in CONFIG_FILES:\n",
    "    # Load the yaml configuration file\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    logger.debug(f\"Loaded configuration from {config_file}\")\n",
    "\n",
    "    # Skip this config if the user has set PD_training to False\n",
    "    if not config['PD_settings']['training']:\n",
    "        logger.info(f\"Skipping PD training for {config_file} as PD training is set to False.\")\n",
    "        continue\n",
    "\n",
    "    # Get the training application\n",
    "    module_name, class_name = config['PD_settings']['training_app'].rsplit('.', 1)\n",
    "    module = importlib.import_module(\"pipeline.\" + module_name)\n",
    "    cls = getattr(module, class_name)\n",
    "\n",
    "    logger.debug(f\"Loaded class {class_name} from module {module_name}\")\n",
    "\n",
    "    # Get the model version (for naming purposes)\n",
    "    model_version = config['PD_settings']['model_version']\n",
    "\n",
    "    # Get the ensemble size, and loop through it\n",
    "    ensemble_size = config['PD_settings']['ensemble_size']\n",
    "    for i in range(ensemble_size):\n",
    "        # If we are training an ensemble, we add an identifier to the model version\n",
    "        if ensemble_size > 1:\n",
    "            # Deepcopy config so we don't affect the original\n",
    "            cfg = copy.deepcopy(config)\n",
    "            cfg['PD_settings']['model_version'] = f\"{model_version}_{i+1:02}\" # e.g., \"v1_01\"\n",
    "        else:\n",
    "            cfg = config\n",
    "\n",
    "        # Add the data version to the configuration\n",
    "        cfg['PD_settings']['data_version'] = DATA_VERSION\n",
    "\n",
    "        # Instantiate with the loaded configuration\n",
    "        instance = cls(cfg, \"PROJ\", DEBUG, MODEL_DIR, AGG_DIR)\n",
    "\n",
    "        logger.info(f\"Going to try training the {i + 1}-th model with configuration from {config_file}...\")\n",
    "\n",
    "        # Run the training\n",
    "        instance.main()\n",
    "\n",
    "        logger.info(f\"Finished training the {i + 1}-th model.\\n\")\n",
    "\n",
    "        del instance, cfg\n",
    "        gc.collect()\n",
    "\n",
    "    # Free up memory\n",
    "    del module, cls, config, module_name, class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef2c2c",
   "metadata": {},
   "source": [
    "# Apply PD model to all nonstop-gated sinograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28452019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ng_dir = os.path.join(WORK_ROOT, 'data_pt', 'prj', 'ng')\n",
    "# g_dir = os.path.join(WORK_ROOT, 'data_pt', 'prj', 'gated')\n",
    "ng_dir = os.path.join('G:', 'data_pt', 'prj', 'ng') # TODO change back\n",
    "g_dir = os.path.join('G:', 'data_pt', 'prj', 'gated')\n",
    "\n",
    "# Loop through the configurations again\n",
    "for config_file in CONFIG_FILES:\n",
    "    # Load the yaml configuration file\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    logger.debug(f\"Loaded configuration from {config_file}\")\n",
    "\n",
    "    # Get the ensemble size, and loop through it\n",
    "    ensemble_size = config['PD_settings']['ensemble_size']\n",
    "    for i in range(ensemble_size):\n",
    "        model_version = config['PD_settings']['model_version']\n",
    "\n",
    "        # If we are training an ensemble, we add an identifier to the model version\n",
    "        if ensemble_size > 1:\n",
    "            model_version = f\"{model_version}_{i+1:02}\"\n",
    "\n",
    "        scan_type_model = config['PD_settings']['scan_type']\n",
    "\n",
    "        # Load the trained PD model onto the GPU\n",
    "        PD_model = load_model(config['PD_settings']['network_name'], model_version, torch.device(CUDA_DEVICE), MODEL_DIR, DATA_VERSION, 'PROJ', scan_type_model)\n",
    "\n",
    "        for file in tqdm(os.listdir(ng_dir), desc=f\"Applying model {model_version} to projections\"):\n",
    "            scan_type, patient, scan = file.split('_')\n",
    "\n",
    "            # Skip the scans that come from different scan types (if any)\n",
    "            if scan_type != scan_type_model:\n",
    "                logger.debug(f\"Skipping file {file} as it does not match the scan type {scan_type_model}.\")\n",
    "                continue\n",
    "\n",
    "            patient = patient[1:]  # Remove the 'p' from the patient number)\n",
    "            scan = scan.split('.')[0]  # Remove the file extension\n",
    "\n",
    "            # Get the matlab dicts for the ground truth and CNN projections\n",
    "            g_mat, cnn_mat = apply_model_to_projections(patient, scan, scan_type, PD_model, g_dir, ng_dir, CUDA_DEVICE, NSG_CBCT_PATH)\n",
    "\n",
    "            # save_dir = os.path.join(RESULT_DIR, model_version)\n",
    "            save_dir = os.path.join('G:', 'result', model_version) # TODO change back\n",
    "            ensure_dir(save_dir)  # Ensure the directory exists\n",
    "\n",
    "            # Save the ground truth and CNN projections\n",
    "            scipy.io.savemat(os.path.join(save_dir, f'PROJ_gated_{scan_type}_p{patient}_{scan}.mat'), g_mat) # e.g., PROJ_gated_HF_p01_01.mat\n",
    "            scipy.io.savemat(os.path.join(save_dir, f'PROJ_ng_{scan_type}_p{patient}_{scan}.mat'), cnn_mat)\n",
    "\n",
    "            logger.debug(f\"Saved projections for {scan_type} p{patient}_{scan}.\")\n",
    "\n",
    "        # Free up memory\n",
    "        del PD_model, g_mat, cnn_mat\n",
    "\n",
    "logger.info(\"All models applied to projections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feaa971",
   "metadata": {},
   "source": [
    "# Apply FDK to all projections that were processed by the PD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do we do this differently for FF?\n",
    "gpuids = gpu.getGpuIds()\n",
    "gpuids.devices = [0]\n",
    "geo = get_geometry()\n",
    "\n",
    "# Loop through the configurations again\n",
    "for config_file in CONFIG_FILES:\n",
    "    # Load the yaml configuration file\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    logger.debug(f\"Loaded configuration from {config_file}\")\n",
    "\n",
    "    # Get the ensemble size, and loop through it\n",
    "    ensemble_size = config['PD_settings']['ensemble_size']\n",
    "    for i in range(ensemble_size):\n",
    "        model_version = config['PD_settings']['model_version']\n",
    "\n",
    "        # If we are training an ensemble, we add an identifier to the model version\n",
    "        if ensemble_size > 1:\n",
    "            model_version = f\"{model_version}_{i+1:02}\"\n",
    "\n",
    "        scan_type_model = config['PD_settings']['scan_type']\n",
    "\n",
    "        # Load the trained PD model onto the GPU\n",
    "        PD_model = load_model(config['PD_settings']['network_name'], model_version, torch.device(CUDA_DEVICE), MODEL_DIR, DATA_VERSION, 'PROJ', scan_type_model)\n",
    "            \n",
    "        mat_result_dir = os.path.join(RESULT_DIR, model_version)\n",
    "\n",
    "        for file in tqdm(os.listdir(mat_result_dir), desc=f\"Applying FDK to gated and nonstop-gated projections for model {model_version}\"):\n",
    "            domain, truth, scan_type, patient, scan = file.split('_')\n",
    "\n",
    "            # Skip the scans that come from different scan types (if any)\n",
    "            if scan_type != scan_type_model:\n",
    "                logger.debug(f\"Skipping file {file} as it does not match the scan type {scan_type_model}.\")\n",
    "                continue\n",
    "\n",
    "            # Skip the file if it is not projection data\n",
    "            if domain != 'PROJ':\n",
    "                logger.debug(f\"Skipping file {file} as it is not projection data.\")\n",
    "                continue\n",
    "\n",
    "            # Load the scan and perform FDK reconstruction\n",
    "            mat = scipy.io.loadmat(os.path.join(mat_result_dir, file))\n",
    "            fdk_half = FDKHalf()(mat['prj'], get_geometry(), mat['angles'].flatten(), filter='hann', parker=True)\n",
    "            fdk_half = torch.from_numpy(fdk_half).detach()\n",
    "\n",
    "            patient = patient[1:]  # Remove the 'p' from the patient number)\n",
    "            scan = scan.split('.')[0]  # Remove the file extension\n",
    "\n",
    "            save_dir = os.path.join(RECON_DIR, model_version)\n",
    "            ensure_dir(save_dir)  # Ensure the directory exists\n",
    "\n",
    "            # Save the recon results as .pt\n",
    "            torch.save(fdk_half, os.path.join(save_dir, f'FDK_{truth}_{scan_type}_p{patient}_{scan}.pt')) # e.g., FDK_gated_HF_p01_01.pt\n",
    "\n",
    "            logger.debug(f\"Saved {truth} projection for {scan_type} p{patient}_{scan}.\")\n",
    "\n",
    "        # Free up memory\n",
    "        del PD_model, g_mat, cnn_mat\n",
    "\n",
    "logger.info(\"All models applied to projections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09b672",
   "metadata": {},
   "source": [
    "# Aggregate CT volumes for train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCANS_AGG is not None:\n",
    "    SCANS_AGG, scan_type = read_scans_agg_file(SCANS_AGG)\n",
    "    logger.debug(f\"Loaded scan list for aggregation: {SCANS_AGG}\")\n",
    "\n",
    "    # Only aggregate reconstructions if they don't already exist\n",
    "    if len([f for f in os.listdir(AGG_DIR) if f.startswith(\"FDK\")]) > 0:\n",
    "        raise FileExistsError(f\"Aggregated reconstruction data for phase {PHASE} data version {DATA_VERSION} already exists in {AGG_DIR}. Please delete the existing files or change the data version to re-aggregate.\")\n",
    "    else:\n",
    "        logger.info(\"Starting to aggregate reconstruction data...\")\n",
    "\n",
    "        pt_recon_dir = os.path.join(WORK_ROOT, 'data_pt', 'recon')\n",
    "\n",
    "        # Aggregate and save reconstruction data sets\n",
    "        for sample in ['TRAIN', 'VALIDATION', 'TEST']:\n",
    "            if len(SCANS_AGG[sample]):\n",
    "                recon_ngcbct = aggregate_saved_recons(scan_type, sample, pt_recon_dir, SCANS_AGG, truth=False, augment=AUGMENT_ID)\n",
    "                np.save(os.path.join(AGG_DIR, f\"FDK_IMAG_ng_{scan_type}_{sample}.npy\"), recon_ngcbct.numpy()) # e.g., IMAG_ng_HF_TRAIN.npy\n",
    "                del recon_ngcbct\n",
    "                logger.debug(\"Done with nonstop-gated...\")\n",
    "                recon_gcbct = aggregate_saved_recons(scan_type, sample, pt_recon_dir, SCANS_AGG, truth=True, augment=AUGMENT_ID)\n",
    "                np.save(os.path.join(AGG_DIR, f\"FDK_IMAG_gated_{scan_type}_{sample}.npy\"), recon_gcbct.numpy())\n",
    "                del recon_gcbct\n",
    "                logger.debug(\"Done with gated...\")\n",
    "\n",
    "                logger.debug(f\"Aggregated reconstructions saved for {scan_type} {sample}.\\n\")\n",
    "            else:\n",
    "                logger.debug(f\"No scans to aggregate for {scan_type} {sample}. Skipping aggregation.\")\n",
    "\n",
    "        # Free up memory\n",
    "        del recon_gcbct, recon_ngcbct\n",
    "\n",
    "    logger.info(\"Reconstruction data aggregation completed successfully.\")\n",
    "    logger.info(\"Aggregated reconstruction data saved in: %s\", AGG_DIR)\n",
    "else:\n",
    "    logger.info(\"No scans to aggregate. Skipping reconstruction data aggregation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e933115",
   "metadata": {},
   "source": [
    "# Train ID CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b35e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for config_file in CONFIG_FILES:\n",
    "    # Load the yaml configuration file\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    logger.debug(f\"Loaded configuration from {config_file}\")\n",
    "\n",
    "    # Skip this config if the user has set ID_training to False\n",
    "    if not config['ID_settings']['training']:\n",
    "        logger.info(f\"Skipping ID training for {config_file} as ID training is set to False.\")\n",
    "        continue\n",
    "\n",
    "    # Get the training application\n",
    "    module_name, class_name = config['ID_settings']['training_app'].rsplit('.', 1)\n",
    "    module = importlib.import_module(\"pipeline.\" + module_name)\n",
    "    cls = getattr(module, class_name)\n",
    "\n",
    "    logger.debug(f\"Loaded class {class_name} from module {module_name}\")\n",
    "\n",
    "    # Get the model version (for naming purposes)\n",
    "    model_version = config['ID_settings']['model_version']\n",
    "\n",
    "    # Get the ensemble size, and loop through it\n",
    "    ensemble_size = config['ID_settings']['ensemble_size']\n",
    "    for i in range(ensemble_size):\n",
    "        # If we are training an ensemble, we add an identifier to the model version\n",
    "        if ensemble_size > 1:\n",
    "            # Deepcopy config so we don't affect the original\n",
    "            cfg = copy.deepcopy(config)\n",
    "            cfg['ID_settings']['model_version'] = f\"{model_version}_{i+1:02}\" # e.g., \"v1_01\"\n",
    "        else:\n",
    "            cfg = config\n",
    "\n",
    "        # Add the data version to the configuration\n",
    "        cfg['ID_settings']['data_version'] = DATA_VERSION\n",
    "\n",
    "        # Instantiate with the loaded configuration\n",
    "        instance = cls(cfg, \"IMAG\", DEBUG, MODEL_DIR, AGG_DIR)\n",
    "\n",
    "        logger.info(f\"Going to try training the {i + 1}-th model with configuration from {config_file}...\")\n",
    "\n",
    "        # Run the training\n",
    "        instance.main()\n",
    "\n",
    "        logger.info(f\"Finished training the {i + 1}-th model.\\n\")\n",
    "\n",
    "        del instance, cfg\n",
    "        gc.collect()\n",
    "\n",
    "    # Free up memory\n",
    "    del module, cls, config, module_name, class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f74da1",
   "metadata": {},
   "source": [
    "# Pass all samples through the ID model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4461679",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
