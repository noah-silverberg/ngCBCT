{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b5c8d3",
   "metadata": {},
   "source": [
    "### Setup/Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0320969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from pipeline.paths import Directories, Files\n",
    "from pipeline.utils import read_scans_agg_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec7764",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eaa8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASE = \"7\"\n",
    "DATA_VERSION = \"13\"\n",
    "\n",
    "# Parameters for each model dictionary:\n",
    "#   'name': A custom name for plotting and tables (e.g., \"Ensemble\", \"MC Dropout\").\n",
    "#   'type': The type of model. Options:\n",
    "#           'ensemble': For a collection of deterministically trained models.\n",
    "#           'stochastic': For a single model that requires multiple forward passes (e.g., MC Dropout, BBB).\n",
    "#   'domain': The domain to analyze. Options: 'PROJ', 'FDK', 'IMAG'.\n",
    "#           'PROJ': For projection domain analysis (after PD model).\n",
    "#           'FDK': For FDK reconstruction analysis (between PD and ID models).\n",
    "#           'IMAG': For image domain analysis (after ID model).\n",
    "#   'model_version_root': The base name of the model version.\n",
    "#                         For ensembles, this is the name without the '_XX' suffix.\n",
    "#                         For stochastic models, this is the full model version name.\n",
    "#   'count': The number of models in the ensemble or the number of stochastic passes.\n",
    "\n",
    "MODELS_TO_ANALYZE = [\n",
    "    {\n",
    "        'name': 'Ensemble',\n",
    "        'type': 'ensemble',\n",
    "        'domain': 'IMAG',\n",
    "        'model_version_root': 'MK7',\n",
    "        'count': 2,\n",
    "    },\n",
    "    # {\n",
    "    #     'name': 'MC Dropout 30%',\n",
    "    #     'type': 'stochastic',\n",
    "    #     'domain': 'PROJ',\n",
    "    #     'model_version_root': 'MK7_MCDROPOUT_30_pct_NEW',\n",
    "    #     'count': 10,\n",
    "    # },\n",
    "    # {\n",
    "    #     'name': 'MC Dropout 50%',\n",
    "    #     'type': 'stochastic',\n",
    "    #     'domain': 'PROJ',\n",
    "    #     'model_version_root': 'MK7_MCDROPOUT_50_pct_NEW',\n",
    "    #     'count': 10,\n",
    "    # },\n",
    "]\n",
    "\n",
    "WORK_ROOT = \"D:/NoahSilverberg/ngCBCT\"\n",
    "SCANS_AGG_FILE = 'scans_to_agg.txt'\n",
    "SPLIT_TO_ANALYZE = 'VALIDATION' # Options: 'TRAIN', 'VALIDATION', 'TEST'\n",
    "\n",
    "SLICE_IDX = 100 # TODO: add tumor slice option for FDK and IMAG domains\n",
    "\n",
    "SSIM_KWARGS = {\"K1\": 0.03, \"K2\": 0.06, \"win_size\": 15}\n",
    "\n",
    "# Create Directories and Files objects\n",
    "phase_dataver_dir = os.path.join(WORK_ROOT, f\"phase{PHASE}\", f\"DS{DATA_VERSION}\")\n",
    "DIRECTORIES = Directories(\n",
    "    projections_results_dir=os.path.join(phase_dataver_dir, \"results\", \"projections\"),\n",
    "    projections_gated_dir=os.path.join(WORK_ROOT, \"gated\", \"prj_mat\"),\n",
    "    reconstructions_dir=os.path.join(phase_dataver_dir, \"reconstructions\"),\n",
    "    reconstructions_gated_dir=os.path.join(WORK_ROOT, \"gated\", \"fdk_recon\"),\n",
    "    images_results_dir=os.path.join(phase_dataver_dir, \"results\", \"images\"),\n",
    ")\n",
    "FILES = Files(DIRECTORIES)\n",
    "\n",
    "# Load the list of scans\n",
    "scans_agg, scan_type_agg = read_scans_agg_file(SCANS_AGG_FILE)\n",
    "analysis_scans = scans_agg[SPLIT_TO_ANALYZE]\n",
    "\n",
    "print(f\"Configuration loaded.\")\n",
    "print(f\"Analyzing {len(analysis_scans)} scans from the '{SPLIT_TO_ANALYZE}' split.\")\n",
    "print(f\"Found {len(MODELS_TO_ANALYZE)} model(s) to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114d02c",
   "metadata": {},
   "source": [
    "### Data loading/prep functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b635426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth(files_obj: Files, scan_info, domain, slice_idx=None):\n",
    "    \"\"\"\n",
    "    Loads the ground truth data for a given scan and domain.\n",
    "    \"\"\"\n",
    "    patient, scan, scan_type = scan_info\n",
    "    \n",
    "    if domain == 'PROJ':\n",
    "        gt_path = files_obj.get_projections_results_filepath('fdk', patient, scan, scan_type, gated=True)\n",
    "        data = torch.from_numpy(scipy.io.loadmat(gt_path)['prj']).detach().permute(1, 0, 2)\n",
    "    elif domain == 'FDK':\n",
    "        gt_path = files_obj.get_recon_filepath(\"fdk\", patient, scan, scan_type, gated=True, ensure_exists=False)\n",
    "        data = torch.load(gt_path).detach()\n",
    "        data = 25. * torch.clip(data, min=0.0, max=0.04)\n",
    "    elif domain == 'IMAG':\n",
    "        # The ground truth for the IMAG domain is the FDK of the gated projection\n",
    "        gt_path = files_obj.get_recon_filepath(\"fdk\", patient, scan, scan_type, gated=True, ensure_exists=False)\n",
    "        data = torch.load(gt_path).detach()\n",
    "        data = data[20:-20, :, :]\n",
    "        data = 25. * torch.clip(data, min=0.0, max=0.04)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown domain: {domain}\")\n",
    "\n",
    "    if slice_idx is not None and data.ndim == 3:\n",
    "        return data[slice_idx]\n",
    "    return data\n",
    "\n",
    "def load_predictions(files_obj: Files, model_config, scan_info, slice_idx=None):\n",
    "    \"\"\"\n",
    "    Loads all predictions for a given model, scan, and domain.\n",
    "    \"\"\"\n",
    "    patient, scan, scan_type = scan_info\n",
    "    domain = model_config['domain']\n",
    "    root = model_config['model_version_root']\n",
    "    count = model_config['count']\n",
    "    model_type = model_config['type']\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"Loading {count} predictions for {model_config['name']}...\")\n",
    "    \n",
    "    for i in tqdm(range(count), desc=\"Loading predictions\", leave=False):\n",
    "        passthrough_num = None\n",
    "        model_version = root\n",
    "\n",
    "        if model_type == 'ensemble':\n",
    "            model_version = f\"{root}_{i+1:02d}\"\n",
    "        elif model_type == 'stochastic':\n",
    "            passthrough_num = i\n",
    "\n",
    "        if domain == 'PROJ':\n",
    "            pred_path = files_obj.get_projections_results_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            pred = torch.from_numpy(scipy.io.loadmat(pred_path)['prj']).detach().permute(1, 0, 2)\n",
    "        elif domain == 'FDK':\n",
    "            pred_path = files_obj.get_recon_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            pred = torch.load(pred_path).detach()\n",
    "            pred = 25. * torch.clip(pred, min=0.0, max=0.04)\n",
    "        elif domain == 'IMAG':\n",
    "            # This assumes the results are saved with the ID model version name\n",
    "            pred_path = files_obj.get_images_results_filepath(model_version, patient, scan, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "            pred = torch.load(pred_path).detach()\n",
    "            pred = torch.squeeze(pred, dim=1)\n",
    "            pred = torch.permute(pred, (0, 2, 1))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown domain: {domain}\")\n",
    "            \n",
    "        predictions.append(pred)\n",
    "\n",
    "    predictions_tensor = torch.stack(predictions)\n",
    "    \n",
    "    if slice_idx is not None and predictions_tensor.ndim == 4:\n",
    "        return predictions_tensor[:, slice_idx, :, :]\n",
    "        \n",
    "    return predictions_tensor\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3cbb74",
   "metadata": {},
   "source": [
    "### Metric calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723009ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_quality_metrics(predictions: torch.Tensor, ground_truth: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Calculates image quality metrics on both the mean prediction and on individual samples.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Ensure data is on CPU and numpy format\n",
    "    gt_np = ground_truth.cpu().numpy()\n",
    "    preds_np = predictions.cpu().numpy()\n",
    "    \n",
    "    # --- Metrics on the Mean Prediction ---\n",
    "    mean_pred_np = np.mean(preds_np, axis=0)\n",
    "    data_range = np.max(gt_np) - np.min(gt_np)\n",
    "    \n",
    "    metrics['mean_ssim'] = ssim(gt_np, mean_pred_np, data_range=data_range, **SSIM_KWARGS)\n",
    "    metrics['mean_psnr'] = psnr(gt_np, mean_pred_np, data_range=data_range)\n",
    "    metrics['mean_mse'] = np.mean((gt_np - mean_pred_np)**2)\n",
    "    metrics['mean_mae'] = np.mean(np.abs(gt_np - mean_pred_np))\n",
    "    \n",
    "    # Get the SSIM map\n",
    "    _, ssim_map = ssim(gt_np, mean_pred_np, data_range=data_range, full=True, **SSIM_KWARGS)\n",
    "    \n",
    "    # --- Metrics on Individual Samples (then averaged) ---\n",
    "    sample_ssims, sample_psnrs, sample_mses, sample_maes = [], [], [], []\n",
    "    for i in range(preds_np.shape[0]):\n",
    "        sample_pred_np = preds_np[i]\n",
    "        sample_ssims.append(ssim(gt_np, sample_pred_np, data_range=data_range, **SSIM_KWARGS))\n",
    "        sample_psnrs.append(psnr(gt_np, sample_pred_np, data_range=data_range))\n",
    "        sample_mses.append(np.mean((gt_np - sample_pred_np)**2))\n",
    "        sample_maes.append(np.mean(np.abs(gt_np - sample_pred_np)))\n",
    "        \n",
    "    metrics['sample_avg_ssim'] = np.mean(sample_ssims)\n",
    "    metrics['sample_avg_psnr'] = np.mean(sample_psnrs)\n",
    "    metrics['sample_avg_mse'] = np.mean(sample_mses)\n",
    "    metrics['sample_avg_mae'] = np.mean(sample_maes)\n",
    "    \n",
    "    return metrics, ssim_map\n",
    "\n",
    "def calculate_ece(mean_pred, uncertainty, errors, n_bins=20):\n",
    "    \"\"\"\n",
    "    Calculates the Expected Calibration Error (ECE).\n",
    "    \"\"\"\n",
    "    bin_boundaries = np.linspace(np.min(uncertainty), np.max(uncertainty), n_bins + 1)\n",
    "    ece = 0\n",
    "    bin_maes = []\n",
    "    bin_confs = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        in_bin = (uncertainty >= bin_boundaries[i]) & (uncertainty < bin_boundaries[i+1])\n",
    "        if np.sum(in_bin) > 0:\n",
    "            mae_in_bin = np.mean(errors[in_bin])\n",
    "            avg_conf_in_bin = np.mean(uncertainty[in_bin])\n",
    "            ece += np.abs(mae_in_bin - avg_conf_in_bin) * (np.sum(in_bin) / len(uncertainty.flatten()))\n",
    "            bin_maes.append(mae_in_bin)\n",
    "            bin_confs.append(avg_conf_in_bin)\n",
    "\n",
    "    return ece, bin_maes, bin_confs\n",
    "\n",
    "\n",
    "def calculate_ause(uncertainty, errors):\n",
    "    \"\"\"\n",
    "    Calculates the Area Under the Sparsification Error curve (AUSE).\n",
    "    Uses MAE as the error metric.\n",
    "    \"\"\"\n",
    "    # Flatten arrays\n",
    "    uncertainty_flat = uncertainty.flatten()\n",
    "    errors_flat = errors.flatten()\n",
    "    \n",
    "    # Sort by uncertainty\n",
    "    sorted_indices = np.argsort(uncertainty_flat)\n",
    "    sorted_errors = errors_flat[sorted_indices]\n",
    "    \n",
    "    # Calculate cumulative error\n",
    "    cumulative_error = np.cumsum(sorted_errors) / np.arange(1, len(sorted_errors) + 1)\n",
    "    \n",
    "    # Calculate AUSE\n",
    "    ause = np.mean(cumulative_error)\n",
    "    return ause, cumulative_error\n",
    "\n",
    "def calculate_metrics_iteratively(files_obj: Files, model_config, scan_info, gt_np: np.ndarray):\n",
    "    \"\"\"\n",
    "    Loads one prediction at a time to calculate stats and metrics iteratively,\n",
    "    avoiding high memory usage from stacking all predictions.\n",
    "    \n",
    "    This function computes the mean and uncertainty map using Welford's algorithm for online variance.\n",
    "    \"\"\"\n",
    "    # --- Online Statistics Initialization ---\n",
    "    n_samples = model_config['count']\n",
    "    mean_np = np.zeros_like(gt_np, dtype=np.float32)\n",
    "    m2_np = np.zeros_like(gt_np, dtype=np.float32) # Sum of squares of differences from the current mean\n",
    "    \n",
    "    # --- Iterative Metric Initialization ---\n",
    "    sample_ssims, sample_psnrs, sample_mses, sample_maes = [], [], [], []\n",
    "    data_range = np.max(gt_np) - np.min(gt_np)\n",
    "\n",
    "    # --- Loop through predictions one by one ---\n",
    "    # This generator function will yield one prediction at a time without storing them all.\n",
    "    # Note: We are re-using your existing load_predictions function in a clever way.\n",
    "    def prediction_generator():\n",
    "        for i in range(n_samples):\n",
    "            # Temporarily set count to 1 and change root to get the specific prediction\n",
    "            single_pred_config = model_config.copy()\n",
    "            if single_pred_config['type'] == 'ensemble':\n",
    "                single_pred_config['model_version_root'] = f\"{model_config['model_version_root']}_{i+1:02d}\"\n",
    "            else: # stochastic\n",
    "                # This assumes load_predictions can handle a single passthrough_num, which it can't directly.\n",
    "                # A small modification to load_predictions would be needed, or do it here.\n",
    "                # Let's do it here for simplicity:\n",
    "                patient, scan, scan_type = scan_info\n",
    "                passthrough_num = i if model_config['type'] == 'stochastic' else None\n",
    "                model_version = single_pred_config['model_version_root']\n",
    "\n",
    "                if single_pred_config['domain'] == 'PROJ':\n",
    "                    pred_path = files_obj.get_projections_results_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                    pred = torch.from_numpy(scipy.io.loadmat(pred_path)['prj']).detach().permute(1, 0, 2)\n",
    "                elif single_pred_config['domain'] == 'FDK':\n",
    "                    pred_path = files_obj.get_recon_filepath(model_version, patient, scan, scan_type, gated=False, passthrough_num=passthrough_num, ensure_exists=False)\n",
    "                    pred = torch.load(pred_path).detach()\n",
    "                    pred = 25. * torch.clip(pred, min=0.0, max=0.04)\n",
    "                # Add IMAG domain if needed\n",
    "                \n",
    "                yield pred.cpu().numpy()\n",
    "\n",
    "\n",
    "    for i, pred_np in enumerate(tqdm(prediction_generator(), total=n_samples, desc=\"Iterative Metrics\", leave=False)):\n",
    "        # --- Calculate metrics for this single sample ---\n",
    "        sample_ssims.append(ssim(gt_np, pred_np, data_range=data_range, **SSIM_KWARGS))\n",
    "        sample_psnrs.append(psnr(gt_np, pred_np, data_range=data_range))\n",
    "        sample_mses.append(np.mean((gt_np - pred_np)**2))\n",
    "        sample_maes.append(np.mean(np.abs(gt_np - pred_np)))\n",
    "\n",
    "        # --- Update online statistics (Welford's Algorithm) ---\n",
    "        delta = pred_np - mean_np\n",
    "        mean_np += delta / (i + 1)\n",
    "        delta2 = pred_np - mean_np\n",
    "        m2_np += delta * delta2\n",
    "    \n",
    "    # Finalize stats\n",
    "    uncertainty_map = np.sqrt(m2_np / n_samples) if n_samples > 1 else np.zeros_like(mean_np)\n",
    "    \n",
    "    # --- Calculate metrics on the final mean prediction ---\n",
    "    metrics = {}\n",
    "    metrics['mean_ssim'] = ssim(gt_np, mean_np, data_range=data_range, **SSIM_KWARGS)\n",
    "    metrics['mean_psnr'] = psnr(gt_np, mean_np, data_range=data_range)\n",
    "    metrics['mean_mse'] = np.mean((gt_np - mean_np)**2)\n",
    "    metrics['mean_mae'] = np.mean(np.abs(gt_np - mean_np))\n",
    "\n",
    "    # Add the averaged sample metrics\n",
    "    metrics['sample_avg_ssim'] = np.mean(sample_ssims)\n",
    "    metrics['sample_avg_psnr'] = np.mean(sample_psnrs)\n",
    "    metrics['sample_avg_mse'] = np.mean(sample_mses)\n",
    "    metrics['sample_avg_mae'] = np.mean(sample_maes)\n",
    "\n",
    "    _, ssim_map = ssim(gt_np, mean_np, data_range=data_range, full=True)\n",
    "\n",
    "    return metrics, ssim_map, mean_np, uncertainty_map\n",
    "\n",
    "\n",
    "print(\"Metric calculation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85d476",
   "metadata": {},
   "source": [
    "### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b12fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_comparison(mean_pred, ground_truth, uncertainty_map, model_name, scan_name, slice_idx):\n",
    "    \"\"\"Plots the GT, mean prediction, absolute error, and uncertainty map.\"\"\"\n",
    "    error_map = np.abs(ground_truth - mean_pred)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    fig.suptitle(f'{model_name} - {scan_name} - Slice {slice_idx} (Mean vs. GT)', fontsize=16)\n",
    "    \n",
    "    im1 = axes[0].imshow(ground_truth, cmap='gray')\n",
    "    axes[0].set_title('Ground Truth')\n",
    "    axes[0].axis('off')\n",
    "    fig.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    im2 = axes[1].imshow(mean_pred, cmap='gray')\n",
    "    axes[1].set_title('Mean Prediction')\n",
    "    axes[1].axis('off')\n",
    "    fig.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    im3 = axes[2].imshow(error_map, cmap='magma')\n",
    "    axes[2].set_title('Absolute Error Map')\n",
    "    axes[2].axis('off')\n",
    "    fig.colorbar(im3, ax=axes[2])\n",
    "    \n",
    "    im4 = axes[3].imshow(uncertainty_map, cmap='viridis')\n",
    "    axes[3].set_title('Uncertainty (Std Dev)')\n",
    "    axes[3].axis('off')\n",
    "    fig.colorbar(im4, ax=axes[3])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_sample_comparison(samples, ground_truth, uncertainty_map, model_name, scan_name, slice_idx):\n",
    "    \"\"\"Plots the GT and 3 random sample predictions.\"\"\"\n",
    "    if len(samples) < 3:\n",
    "        print(\"Not enough samples to plot 3, plotting all available.\")\n",
    "        sample_indices = range(len(samples))\n",
    "    else:\n",
    "        sample_indices = np.random.choice(len(samples), 3, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    fig.suptitle(f'{model_name} - {scan_name} - Slice {slice_idx} (Random Samples vs. GT)', fontsize=16)\n",
    "\n",
    "    # Plot Ground Truth\n",
    "    im = axes[0].imshow(ground_truth, cmap='gray')\n",
    "    axes[0].set_title('Ground Truth')\n",
    "    axes[0].axis('off')\n",
    "    fig.colorbar(im, ax=axes[0])\n",
    "\n",
    "    # Plot Samples\n",
    "    for i, sample_idx in enumerate(sample_indices):\n",
    "        im = axes[i+1].imshow(samples[sample_idx], cmap='gray')\n",
    "        axes[i+1].set_title(f'Sample Prediction #{sample_idx+1}')\n",
    "        axes[i+1].axis('off')\n",
    "        fig.colorbar(im, ax=axes[i+1])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ssim_map(ssim_map, model_name, scan_name):\n",
    "    \"\"\"Plots the SSIM map.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(ssim_map, cmap='viridis', vmin=0, vmax=1)\n",
    "    plt.title(f'SSIM Map - {model_name} - {scan_name}')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c49df",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbced87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list will store dictionaries of results for each scan and model\n",
    "all_results = []\n",
    "\n",
    "for model_config in MODELS_TO_ANALYZE:\n",
    "    model_name = model_config['name']\n",
    "    domain = model_config['domain']\n",
    "    \n",
    "    scan_results = []\n",
    "\n",
    "    for scan_info in tqdm(analysis_scans, desc=f\"Analyzing Model: {model_name}\"):\n",
    "        patient, scan, _ = scan_info\n",
    "        scan_name = f\"p{patient}_{scan}\"\n",
    "        \n",
    "        # --- Data Loading ---\n",
    "        use_slice = SLICE_IDX if domain != 'PROJ' else None # Handle slicing\n",
    "        gt = load_ground_truth(FILES, scan_info, domain, slice_idx=use_slice)\n",
    "        preds = load_predictions(FILES, model_config, scan_info, slice_idx=use_slice)\n",
    "        \n",
    "        # --- Core Calculations ---\n",
    "        gt_np = gt.cpu().numpy()\n",
    "        preds_np = preds.cpu().numpy()\n",
    "\n",
    "        mean_pred = np.mean(preds_np, axis=0)\n",
    "        uncertainty_map = np.std(preds_np, axis=0)\n",
    "        errors = np.abs(gt_np - mean_pred)\n",
    "\n",
    "        # --- Metric Calculation ---\n",
    "        iq_metrics, ssim_map_val = calculate_image_quality_metrics(preds, gt)\n",
    "        ause_val, _ = calculate_ause(uncertainty_map, errors)\n",
    "        ece_val, _, _ = calculate_ece(mean_pred, uncertainty_map, errors, n_bins=20)\n",
    "        \n",
    "        scan_result = {\n",
    "            'model_name': model_name,\n",
    "            'scan_name': scan_name,\n",
    "            **iq_metrics,\n",
    "            'ause': ause_val,\n",
    "            'ece': ece_val\n",
    "        }\n",
    "        scan_results.append(scan_result)\n",
    "        \n",
    "        # --- Visualization ---\n",
    "        print(f\"\\n--- Results for {model_name} on {scan_name} (Slice: {SLICE_IDX}) ---\")\n",
    "        plot_mean_comparison(mean_pred, gt_np, uncertainty_map, model_name, scan_name, SLICE_IDX)\n",
    "        plot_sample_comparison(preds_np, gt_np, uncertainty_map, model_name, scan_name, SLICE_IDX)\n",
    "        plot_ssim_map(ssim_map_val, model_name, scan_name)\n",
    "        \n",
    "        # Clean up memory\n",
    "        del gt, preds, gt_np, preds_np, mean_pred, uncertainty_map, errors\n",
    "        gc.collect()\n",
    "        \n",
    "    all_results.extend(scan_results)\n",
    "\n",
    "# Convert results to a pandas DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\n\\n✅ Analysis complete for all models and scans.\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This list will store dictionaries of results for each scan and model\n",
    "# all_results = []\n",
    "\n",
    "# for model_config in MODELS_TO_ANALYZE:\n",
    "#     model_name = model_config['name']\n",
    "#     domain = model_config['domain']\n",
    "    \n",
    "#     scan_results = []\n",
    "\n",
    "#     for scan_info in tqdm(analysis_scans, desc=f\"Analyzing Model: {model_name}\"):\n",
    "#         patient, scan, _ = scan_info\n",
    "#         scan_name = f\"p{patient}_{scan}\"\n",
    "        \n",
    "#         # --- Data Loading (Ground Truth Only) ---\n",
    "#         # Note: We now only load the ground truth here.\n",
    "#         use_slice = SLICE_IDX if domain != 'PROJ' else None\n",
    "#         gt = load_ground_truth(FILES, scan_info, domain, slice_idx=use_slice)\n",
    "#         gt_np = gt.cpu().numpy()\n",
    "        \n",
    "#         # --- Iterative Metric Calculation ---\n",
    "#         # This new function replaces load_predictions and parts of the core calculations.\n",
    "#         iq_metrics, ssim_map_val, mean_pred, uncertainty_map = calculate_metrics_iteratively(\n",
    "#             FILES, model_config, scan_info, gt_np\n",
    "#         )\n",
    "        \n",
    "#         # --- Uncertainty Metric Calculation ---\n",
    "#         errors = np.abs(gt_np - mean_pred)\n",
    "#         ause_val, _ = calculate_ause(uncertainty_map, errors)\n",
    "#         ece_val, _, _ = calculate_ece(mean_pred, uncertainty_map, errors, n_bins=20)\n",
    "        \n",
    "#         scan_result = {\n",
    "#             'model_name': model_name,\n",
    "#             'scan_name': scan_name,\n",
    "#             **iq_metrics,\n",
    "#             'ause': ause_val,\n",
    "#             'ece': ece_val\n",
    "#         }\n",
    "#         scan_results.append(scan_result)\n",
    "        \n",
    "#         # --- Visualization ---\n",
    "#         print(f\"\\n--- Results for {model_name} on {scan_name} (Slice: {SLICE_IDX}) ---\")\n",
    "#         # Note: plot_sample_comparison cannot be used with this memory-saving method\n",
    "#         # as it requires multiple samples at once. It can be re-enabled if needed by\n",
    "#         # modifying the generator to yield a few samples for plotting.\n",
    "#         plot_mean_comparison(mean_pred, gt_np, uncertainty_map, model_name, scan_name, SLICE_IDX)\n",
    "#         # plot_sample_comparison(preds_np, gt_np, uncertainty_map, model_name, scan_name, SLICE_IDX)\n",
    "#         plot_ssim_map(ssim_map_val, model_name, scan_name)\n",
    "        \n",
    "#         # Clean up memory\n",
    "#         del gt, gt_np, mean_pred, uncertainty_map, errors, ssim_map_val\n",
    "#         gc.collect()\n",
    "        \n",
    "#     all_results.extend(scan_results)\n",
    "\n",
    "# # Convert results to a pandas DataFrame for easier analysis\n",
    "# results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# print(\"\\n\\n✅ Analysis complete for all models and scans.\")\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f9dce6",
   "metadata": {},
   "source": [
    "### Summary/Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff11ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(MODELS_TO_ANALYZE) > 1:\n",
    "    summary_data = []\n",
    "    \n",
    "    # Only aggregate numeric columns\n",
    "    numeric_cols = results_df.select_dtypes(include=[np.number]).columns\n",
    "    summary = results_df.groupby('model_name')[numeric_cols].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    # Prepare summary_display with formatted mean ± std for each metric\n",
    "    summary_display = pd.DataFrame()\n",
    "    summary_display['model_name'] = summary['model_name']\n",
    "    for col in numeric_cols:\n",
    "        mean_col = (col, 'mean')\n",
    "        std_col = (col, 'std')\n",
    "        summary_display[col] = summary[mean_col].map('{:.4f}'.format) + ' ± ' + summary[std_col].map('{:.4f}'.format)\n",
    "    \n",
    "    print(\"\\n\\n=======================================================\")\n",
    "    print(\"               Model Comparison Summary\")\n",
    "    print(\"=======================================================\")\n",
    "    \n",
    "    display(summary_display)\n",
    "\n",
    "else:\n",
    "    print(\"\\nOnly one model was analyzed. No comparison table to generate.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
