
# PD Training settings
PD_settings:
  training: false # Whether to train the PD model
  training_app: "train_app_MK6_numpy.TrainingApp"
  epochs: 20 # Number of epochs for PD training
  learning_rate: 1e-3  # Either float or list (if the list is shorter than the number of epochs, the last value is used for the rest of the epochs)
  network_name: "IResNetDropout"  # Network name for PD CNN (see network_instance.py)
  network_kwargs: {'p' : 0.5}  # Additional keyword arguments for the network (e.g., {'p': 0.5} for dropout, or 'prior_mu' and/or 'prior_sigma' for BBB)
  model_version: "MK7_MCDROPOUT_50_pct_NEW" # Unique model name (note: you can have the same name for an image domain model)
  batch_size: 8 # Batch size for PD training
  optimizer: "NAdam" # Optimizer for PD training (SGD, Adam, or NAdam)
  num_workers: 0 # Number of workers for data loading (0 means only the main process will load data)
  shuffle: true # Whether to shuffle the data at the beginning of each epoch
  grad_clip: true # Whether to clip gradients during PD training
  grad_max: 0.01  # Only used if grad_clip is True
  betas_NAdam: (0.9, 0.999)  # Only for NAdam, otherwise ignored
  momentum_decay_NAdam: 4e-4  # Only for NAdam, otherwise ignored
  momentum_SGD: 0.99  # Only for SGD, otherwise ignored
  weight_decay_SGD: 1e-8  # Only for SGD, otherwise ignored
  checkpoint_save_step: 1  # Save checkpoint every N epochs
  tensor_board: false  # Whether to use TensorBoard for PD training
  tensor_board_comment: ""  # If using TensorBoard, a comment suffix
  train_at_inference: true  # Whether to put the model in training mode during inference (e.g., for MC dropout)
  ensemble_size: 1 # Number of models to train
  scan_type: "HF" # Scan type for PD training (HF or FF) -- make sure it matches the aggregated data
  passthrough_count: 10 #  The number of passthroughs to use when creating outputs for the model (for deterministic models 1, for probabilistic you'll probably want >1)
  is_bayesian: false # Whether the model is BBB
  beta_BBB: 1e-2 # Only for BBB, constant factor for the KL divergence term (â‰  1 for tempered Bayesian)

  swag_enabled: true
  swag_model_version: 'MK7_SWAG_TEST' # The model version to load
  swag_start_model_version: 'MK7_01' # The model version to start from for SWA
  swag_start_checkpoint_epoch: 50 # The checkpoint epoch to load from that model
  swag_burn_in_epochs: 0 # Epochs to fine-tune before starting SWA (can be 0)
  swag_swa_epochs: 1 # Number of epochs to perform SWA
  swag_lr: 1e-4 # Constant learning rate for SWA epochs
  swag_cov_mat_rank: 1 # Rank of the SWAG covariance matrix
    
  # Ignore -- not used for PD training (just have to be defined)
  input_type: null

# ID training settings
ID_settings:
  training: true
  training_app: "train_app_MK6_numpy.TrainingApp"
  epochs: 50
  learning_rate: 1e-3
  network_name: "IResNetBBB"
  network_kwargs: {'prior_pi' : 0.5, 'prior_mu' : 0.0, 'prior_sigma1' : 1e-1, 'prior_sigma2' : 1e-3}
  model_version: "MK7_BBB_pi0.5_mu_0.0_sigma1_1e-1_sigma2_1e-3"
  batch_size: 8
  optimizer: "NAdam"
  num_workers: 0
  shuffle: true
  grad_clip: true
  grad_max: 0.01
  betas_NAdam: (0.9, 0.999)
  momentum_decay_NAdam: 4e-4
  momentum_SGD: 0.99
  weight_decay_SGD: 1e-8
  checkpoint_save_step: 1
  tensor_board: false
  tensor_board_comment: ""
  train_at_inference: true
  ensemble_size: 1
  scan_type: "HF"
  input_type: "MK7_MCDROPOUT_30_pct_NEW" # Input type for ID training, i.e., PL or some trained & reconstructed PD model name
  input_type_match_ensemble: true # When training an ensemble, whether the input type should have an extra identifier (e.g., _01) which matches the ID model name
  passthrough_count: 50
  is_bayesian: true
  beta_BBB: 1e-4

  swag_enabled: false
  swag_model_version: 'MK7_SWAG_TEST' # The model version to load
  swag_start_model_version: 'MK7_01' # The model version to start from for SWA
  swag_start_checkpoint_epoch: 50 # The checkpoint epoch to load from that model
  swag_burn_in_epochs: 0 # Epochs to fine-tune before starting SWA (can be 0)
  swag_swa_epochs: 1 # Number of epochs to perform SWA
  swag_lr: 1e-4 # Constant learning rate for SWA epochs
  swag_cov_mat_rank: 1 # Rank of the SWAG covariance matrix
  swag_momentum: 0.0
  swag_weight_decay: 1e-4