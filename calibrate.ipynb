{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "This initial block sets up the environment, defines file paths, and specifies the validation scans and model versions to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.paths import Directories, Files\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "if torch.cuda.is_available():\n",
    "    CUDA_DEVICE = torch.device(\"cuda:0\")\n",
    "    print(f\"CUDA is available. Using device: {CUDA_DEVICE}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your PyTorch installation. Using CPU instead.\")\n",
    "    CUDA_DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "PHASE = '7'\n",
    "DATA_VERSION = '13'\n",
    "\n",
    "\n",
    "# Base directory\n",
    "WORK_ROOT = \"D:/NoahSilverberg/ngCBCT\"\n",
    "\n",
    "# NSG_CBCT Path where the raw matlab data is stored\n",
    "NSG_CBCT_PATH = \"D:/MitchellYu/NSG_CBCT\"\n",
    "\n",
    "# Directory with all files specific to this phase/data version\n",
    "PHASE_DATAVER_DIR = os.path.join(\n",
    "    WORK_ROOT, f\"phase{PHASE}\", f\"DS{DATA_VERSION}\"\n",
    ")\n",
    "\n",
    "DIRECTORIES = Directories(\n",
    "    # mat_projections_dir=os.path.join(NSG_CBCT_PATH, \"data/prj/HF/mat\"),\n",
    "    # pt_projections_dir=os.path.join(WORK_ROOT, \"prj_pt\"),\n",
    "    # projections_aggregate_dir=os.path.join(PHASE_DATAVER_DIR, \"aggregates\", \"projections\"),\n",
    "    # projections_model_dir=os.path.join(PHASE_DATAVER_DIR, \"models\", \"projections\"),\n",
    "    # projections_results_dir=os.path.join(PHASE_DATAVER_DIR, \"results\", \"projections\"),\n",
    "    # projections_gated_dir=os.path.join(WORK_ROOT, \"gated\", \"prj_mat\"),\n",
    "    reconstructions_dir=os.path.join(PHASE_DATAVER_DIR, \"reconstructions\"),\n",
    "    reconstructions_gated_dir=os.path.join(WORK_ROOT, \"gated\", \"fdk_recon\"),\n",
    "    # reconstructions_dir=os.path.join(\"H:\\\\\", \"Public\", \"Noah\", \"reconstructions\"),\n",
    "    # reconstructions_gated_dir=os.path.join(\"H:\\\\\", \"Public\", \"Noah\", \"gated\", \"fdk_recon\"),\n",
    "    # images_aggregate_dir=os.path.join(PHASE_DATAVER_DIR, \"aggregates\", \"images\"),\n",
    "    # images_model_dir=os.path.join(PHASE_DATAVER_DIR, \"models\", \"images\"),\n",
    "    # images_results_dir=os.path.join(PHASE_DATAVER_DIR, \"results\", \"images\"),\n",
    ")\n",
    "\n",
    "FILES = Files(DIRECTORIES)\n",
    "\n",
    "# VAL_SCANS = [('02', '01'), ('02', '02'), ('16', '01'), ('16', '02'), ('22', '01'), ('22', '02')]\n",
    "VAL_SCANS = [('08', '01'), ('10', '01'), ('14', '01'), ('14', '02'), ('15', '01'), ('20', '01')]\n",
    "\n",
    "SCAN_TYPE = 'HF'\n",
    "MODEL_VERSIONS = ['MK7_MCDROPOUT']\n",
    "PASSTHROUGH_COUNT = 50 # use None for multiple models -- only use this for one model for MC dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd7fb7",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "This block gathers the file paths for the model's reconstructions (50 passes for MC Dropout) and the corresponding ground truth reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4132e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_paths_dict = {}\n",
    "recon_names_dict = {}\n",
    "gt_paths_dict = {}\n",
    "\n",
    "for patient, scan in VAL_SCANS:\n",
    "    print(f\"Processing patient {patient}, scan {scan}\")\n",
    "    recon_paths = []\n",
    "    recon_names = []\n",
    "    if PASSTHROUGH_COUNT is None:\n",
    "        for model_version in MODEL_VERSIONS:\n",
    "            recon_path = FILES.get_recon_filepath(model_version, patient, scan, SCAN_TYPE, gated=False)\n",
    "            recon_paths.append(recon_path)\n",
    "            recon_names.append(model_version)\n",
    "    else:\n",
    "        for i in range(PASSTHROUGH_COUNT):\n",
    "            recon_path = FILES.get_recon_filepath(MODEL_VERSIONS[0], patient, scan, SCAN_TYPE, gated=False, passthrough_num=i)\n",
    "            recon_paths.append(recon_path)\n",
    "            recon_names.append(f\"Passthrough {i+1}\")\n",
    "\n",
    "    gt_path = FILES.get_recon_filepath('fdk', patient, scan, SCAN_TYPE, gated=True)\n",
    "    gt_paths_dict[(patient, scan)] = gt_path\n",
    "\n",
    "    recon_paths_dict[(patient, scan)] = recon_paths\n",
    "    recon_names_dict[(patient, scan)] = recon_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a6e21d",
   "metadata": {},
   "source": [
    "## Analysis Loop\n",
    "The main loop iterates through each scan. For each scan, it loads all 50 reconstruction passes, calculates the mean (prediction) and standard deviation (uncertainty), and then performs several analyses to evaluate the quality of the uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now go through each patient and scan, load the reconstructions\n",
    "# calclate the mean and std (pixel-wise) and the error from GT\n",
    "for (patient, scan), recon_paths in recon_paths_dict.items():\n",
    "    print(f\"\\nProcessing reconstructions for patient {patient}, scan {scan}\")\n",
    "    \n",
    "    # Load ground truth\n",
    "    gt_path = gt_paths_dict[(patient, scan)]\n",
    "    gt_recon = torch.load(gt_path).cpu().numpy()\n",
    "    \n",
    "    # Initialize lists to hold reconstructions\n",
    "    reconstructions = []\n",
    "    \n",
    "    for recon_path in recon_paths:\n",
    "        recon = torch.load(recon_path).cpu().numpy()\n",
    "        reconstructions.append(recon)\n",
    "\n",
    "    print(f\"Loaded {len(reconstructions)} reconstructions for patient {patient}, scan {scan}\")\n",
    "    \n",
    "    # Convert to numpy array for easier manipulation\n",
    "    reconstructions = np.array(reconstructions)\n",
    "    \n",
    "    # --- Basic Calculations ---\n",
    "    # Calculate mean and std across the first axis (across models or passthroughs)\n",
    "    mean_recon = np.mean(reconstructions, axis=0)\n",
    "    std_recon = np.std(reconstructions, axis=0)\n",
    "    \n",
    "    # Calculate error from ground truth\n",
    "    error = mean_recon - gt_recon\n",
    "    \n",
    "    # To avoid memory issues, let's work with flattened arrays from here on\n",
    "    # and select a random subset of the data for some plots to keep them readable.\n",
    "    # We'll also only consider voxels inside a radius of 225px from the center (in the last 2 dims)\n",
    "    # (note scans are 200x512x512)\n",
    "    # Efficiently create a circular mask in the last two dimensions\n",
    "    z, y, x = gt_recon.shape\n",
    "    yy, xx = np.ogrid[:y, :x]\n",
    "    center = np.array([y // 2, x // 2])\n",
    "    radius = 225\n",
    "    dist_from_center = np.sqrt((yy - center[0])**2 + (xx - center[1])**2)\n",
    "    slice_mask = dist_from_center <= radius\n",
    "    mask = np.broadcast_to(slice_mask, gt_recon.shape)\n",
    "    \n",
    "    flat_std = std_recon[mask].flatten()\n",
    "    flat_error = error[mask].flatten()\n",
    "    flat_abs_error = np.abs(flat_error)\n",
    "    flat_squared_error = flat_error**2\n",
    "\n",
    "    # Clean up memory\n",
    "    del reconstructions, mean_recon, std_recon, error, gt_recon\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Analysis 1: Standardized Error Histogram (Your Original Analysis) ---\n",
    "    # This shows the distribution of errors normalized by the raw, uncalibrated uncertainty.\n",
    "    # As you noted, the coverage is poor, indicating the raw std dev is too small.\n",
    "    standardized_error = flat_error / (flat_std + 1e-8)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(standardized_error, bins=100, color='gray', alpha=0.7, range=(-5, 5))\n",
    "    plt.title(f'UNCALIBRATED Standardized Error Histogram for Patient {patient}, Scan {scan}')\n",
    "    plt.xlabel('Standardized Error (Error / Predicted Std Dev)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "    percent_within_1 = np.mean(np.abs(standardized_error) < 1) * 100\n",
    "    percent_within_2 = np.mean(np.abs(standardized_error) < 2) * 100\n",
    "    percent_within_3 = np.mean(np.abs(standardized_error) < 3) * 100\n",
    "    print(f\"Patient {patient}, Scan {scan} - UNCALIBRATED Coverage:\")\n",
    "    print(f\"  Percent with abs standardized error < 1: {percent_within_1:.2f}% (Expected for Gaussian: ~68%)\")\n",
    "    print(f\"  Percent with abs standardized error < 2: {percent_within_2:.2f}% (Expected for Gaussian: ~95%)\")\n",
    "    print(f\"  Percent with abs standardized error < 3: {percent_within_3:.2f}% (Expected for Gaussian: ~99.7%)\")\n",
    "\n",
    "    # --- Analysis 2 & 3: Plots for Hyperparameter Tuning (Relative Uncertainty) ---\n",
    "    # These plots from the previous step are excellent for comparing different dropout rates.\n",
    "    # A model with a stronger correlation (tighter diagonal on the 2D hist) and a faster-\n",
    "    # dropping sparsification curve is likely better, regardless of the absolute scale.\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.hist2d(flat_std, flat_abs_error, bins=50, cmap='inferno')\n",
    "    plt.colorbar(label='Voxel Count')\n",
    "    plt.title(f'Predicted Uncertainty vs. Actual Error for Patient {patient}, Scan {scan}')\n",
    "    plt.xlabel('Predicted Standard Deviation (Uncertainty)')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Analysis 3: Calibration Plot (Reliability Diagram) ---\n",
    "    # This plot assesses how well the *magnitude* of the predicted uncertainty corresponds\n",
    "    # to the *magnitude* of the actual error. Voxels are binned by their predicted uncertainty.\n",
    "    # For each bin, we plot the average predicted uncertainty against the actual error (RMSE).\n",
    "    # A perfectly calibrated model would fall on the y=x line.\n",
    "    num_bins = 20\n",
    "    bin_limits = np.linspace(np.min(flat_std), np.max(flat_std), num_bins + 1)\n",
    "    rmse_per_bin = np.zeros(num_bins)\n",
    "    mean_std_per_bin = np.zeros(num_bins)\n",
    "    \n",
    "    for i in range(num_bins):\n",
    "        lower_bound = bin_limits[i]\n",
    "        upper_bound = bin_limits[i+1]\n",
    "        mask_bin = (flat_std >= lower_bound) & (flat_std < upper_bound)\n",
    "        if np.sum(mask_bin) > 0:\n",
    "            rmse_per_bin[i] = np.sqrt(np.mean(flat_squared_error[mask_bin]))\n",
    "            mean_std_per_bin[i] = np.mean(flat_std[mask_bin])\n",
    "    \n",
    "    # Filter out empty bins\n",
    "    valid_bins = mean_std_per_bin > 0\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(mean_std_per_bin[valid_bins], rmse_per_bin[valid_bins], 'o-', label='Model Calibration', color='royalblue')\n",
    "    # Plot the ideal y=x line for perfect calibration\n",
    "    lims = [min(np.min(mean_std_per_bin[valid_bins]), np.min(rmse_per_bin[valid_bins])), max(np.max(mean_std_per_bin[valid_bins]), np.max(rmse_per_bin[valid_bins]))]\n",
    "    plt.plot(lims, lims, 'k--', label='Perfect Calibration (y=x)')\n",
    "    plt.title(f'Calibration Plot for Patient {patient}, Scan {scan}')\n",
    "    plt.xlabel('Average Predicted Standard Deviation (per bin)')\n",
    "    plt.ylabel('Root Mean Squared Error (per bin)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    \n",
    "    sorted_indices = np.argsort(flat_std)\n",
    "    sorted_squared_error = flat_squared_error[sorted_indices]\n",
    "    fractions_removed = np.linspace(0, 0.5, 51)\n",
    "    mse_remaining = np.zeros_like(fractions_removed)\n",
    "    for i, frac in enumerate(fractions_removed):\n",
    "        num_to_keep = int((1 - frac) * len(sorted_squared_error))\n",
    "        mse_remaining[i] = np.mean(sorted_squared_error[:num_to_keep]) if num_to_keep > 0 else 0\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fractions_removed * 100, mse_remaining, 'o-', color='crimson')\n",
    "    plt.title(f'Sparsification Plot for Patient {patient}, Scan {scan}')\n",
    "    plt.xlabel('Percent of Most Uncertain Voxels Removed (%)')\n",
    "    plt.ylabel('Mean Squared Error (on remaining voxels)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Analysis 4: Post-Hoc Uncertainty Calibration ---\n",
    "    # Here, we learn a mapping from the raw predicted uncertainty to the actual observed error.\n",
    "    # This fixes the scaling issue and gives us a meaningful absolute uncertainty.\n",
    "    \n",
    "    # Method 1: Linear Calibration\n",
    "    # Fits a line: y = a*x + b, mapping raw std dev to absolute error.\n",
    "    p_linear = np.polyfit(flat_std, flat_abs_error, 1)\n",
    "    calibrated_std_linear = np.polyval(p_linear, flat_std)\n",
    "    # Ensure non-negative uncertainty\n",
    "    calibrated_std_linear[calibrated_std_linear < 0] = 0\n",
    "    \n",
    "    # Method 2: Isotonic Regression\n",
    "    # A more powerful, non-parametric approach. We map variance to squared error\n",
    "    # as this relationship is more likely to be monotonically increasing.\n",
    "    ir = IsotonicRegression(out_of_bounds='clip')\n",
    "    # Use a subset for fitting to speed up the process, as the relationship is stable.\n",
    "    # This is a common practice when dealing with millions of points.\n",
    "    fit_indices = np.random.choice(len(flat_std), size=min(100000, len(flat_std)), replace=False)\n",
    "    # The model maps predicted variance (std^2) to observed squared error (error^2)\n",
    "    calibrated_var_isotonic = ir.fit_transform(flat_std[fit_indices]**2, flat_squared_error[fit_indices])\n",
    "    # Apply the mapping to all data points\n",
    "    calibrated_var_isotonic_full = ir.predict(flat_std**2)\n",
    "    calibrated_std_isotonic = np.sqrt(calibrated_var_isotonic_full)\n",
    "\n",
    "    print(\"\\n--- Calibration Results ---\")\n",
    "\n",
    "    # --- Visualize the Calibrated Results ---\n",
    "    \n",
    "    # Plot new Calibration Plots for both methods to show the improvement.\n",
    "    # The points should now lie much closer to the y=x line.\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    fig.suptitle(f'Calibration Plots for Patient {patient}, Scan {scan}')\n",
    "    \n",
    "    # Linear Calibration Plot\n",
    "    num_bins = 20\n",
    "    bin_limits_lin = np.linspace(np.min(calibrated_std_linear), np.max(calibrated_std_linear), num_bins + 1)\n",
    "    rmse_per_bin_lin = np.zeros(num_bins)\n",
    "    mean_std_per_bin_lin = np.zeros(num_bins)\n",
    "    for i in range(num_bins):\n",
    "        mask_bin = (calibrated_std_linear >= bin_limits_lin[i]) & (calibrated_std_linear < bin_limits_lin[i+1])\n",
    "        if np.sum(mask_bin) > 0:\n",
    "            rmse_per_bin_lin[i] = np.sqrt(np.mean(flat_squared_error[mask_bin]))\n",
    "            mean_std_per_bin_lin[i] = np.mean(calibrated_std_linear[mask_bin])\n",
    "    valid_bins_lin = mean_std_per_bin_lin > 0\n",
    "    ax1.plot(mean_std_per_bin_lin[valid_bins_lin], rmse_per_bin_lin[valid_bins_lin], 'o-', label='Linear Calibration', color='green')\n",
    "    lims1 = [0, max(np.max(mean_std_per_bin_lin[valid_bins_lin]), np.max(rmse_per_bin_lin[valid_bins_lin]))]\n",
    "    ax1.plot(lims1, lims1, 'k--', label='Perfect Calibration')\n",
    "    ax1.set_title('After Linear Calibration')\n",
    "    ax1.set_xlabel('Average Predicted Standard Deviation')\n",
    "    ax1.set_ylabel('Root Mean Squared Error')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax1.axis('equal')\n",
    "    \n",
    "    # Isotonic Calibration Plot\n",
    "    bin_limits_iso = np.linspace(np.min(calibrated_std_isotonic), np.max(calibrated_std_isotonic), num_bins + 1)\n",
    "    rmse_per_bin_iso = np.zeros(num_bins)\n",
    "    mean_std_per_bin_iso = np.zeros(num_bins)\n",
    "    for i in range(num_bins):\n",
    "        mask_bin = (calibrated_std_isotonic >= bin_limits_iso[i]) & (calibrated_std_isotonic < bin_limits_iso[i+1])\n",
    "        if np.sum(mask_bin) > 0:\n",
    "            rmse_per_bin_iso[i] = np.sqrt(np.mean(flat_squared_error[mask_bin]))\n",
    "            mean_std_per_bin_iso[i] = np.mean(calibrated_std_isotonic[mask_bin])\n",
    "    valid_bins_iso = mean_std_per_bin_iso > 0\n",
    "    ax2.plot(mean_std_per_bin_iso[valid_bins_iso], rmse_per_bin_iso[valid_bins_iso], 'o-', label='Isotonic Calibration', color='purple')\n",
    "    lims2 = [0, max(np.max(mean_std_per_bin_iso[valid_bins_iso]), np.max(rmse_per_bin_iso[valid_bins_iso]))]\n",
    "    ax2.plot(lims2, lims2, 'k--', label='Perfect Calibration')\n",
    "    ax2.set_title('After Isotonic Regression')\n",
    "    ax2.set_xlabel('Average Predicted Standard Deviation')\n",
    "    ax2.set_ylabel('Root Mean Squared Error')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax2.axis('equal')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # --- Recalculate Coverage Statistics with Calibrated Uncertainty ---\n",
    "    # Now that the uncertainty is scaled correctly, the coverage should be much closer to ideal.\n",
    "    calibrated_standardized_error_linear = flat_error / (calibrated_std_linear + 1e-8)\n",
    "    calibrated_standardized_error_isotonic = flat_error / (calibrated_std_isotonic + 1e-8)\n",
    "    \n",
    "    percent_within_1_linear = np.mean(np.abs(calibrated_standardized_error_linear) < 1) * 100\n",
    "    percent_within_1_isotonic = np.mean(np.abs(calibrated_standardized_error_isotonic) < 1) * 100\n",
    "\n",
    "    print(f\"Patient {patient}, Scan {scan} - CALIBRATED Coverage:\")\n",
    "    print(f\"  [Linear] Percent with abs standardized error < 1: {percent_within_1_linear:.2f}%\")\n",
    "    print(f\"  [Isotonic] Percent with abs standardized error < 1: {percent_within_1_isotonic:.2f}% (Ideal is ~68%)\")\n",
    "    \n",
    "    # Clean up memory before next loop iteration\n",
    "    del flat_std, flat_error, flat_abs_error, flat_squared_error, standardized_error\n",
    "    del calibrated_std_linear, calibrated_std_isotonic, calibrated_var_isotonic_full\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
