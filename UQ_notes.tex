\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor=blue, linkcolor=blue, citecolor=red}
\graphicspath{{Figures/}}
\usepackage[shortlabels]{enumitem}
%\renewcommand\thesubsection{\Alph{subsection}} % For alphabetized subsections

\newcommand{\s}{\scalebox{0.75}[1.0]{$-$}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\mf}{\mathfrak}
\newcommand{\mc}{\mathcal}
\renewcommand{\b}{\mathbf}
\newcommand{\bs}{\boldsymbol}
\newcommand{\uv}[1]{\b{\hat{#1}}}
\newcommand{\uvs}[1]{\bs{\hat{#1}}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\renewcommand{\H}{\mathbb{H}}
\newcommand{\Zplus}{\mathbb{Z}^+}

\newcommand{\f}{\frac}

\newcommand{\ket}[1]{\left| #1\right\rangle}
\newcommand{\bra}[1]{\left\langle #1\right|}
\newcommand{\braket}[2]{\left\langle #1\mid #2\right\rangle}
\newcommand{\vket}[1]{| #1\rangle}
\newcommand{\vbra}[1]{\langle #1|}
\newcommand{\vbraket}[2]{\langle #1\mid #2\rangle}
\newcommand{\CNOT}{\mathrm{CNOT}}

\newcommand{\comp}[1]{#1^\mathsf{c}}
\newcommand{\clos}[1]{\overline{#1}}
\newcommand{\0}{\varnothing}

\newcommand{\llangle}{\langle\langle}
\newcommand{\rrangle}{\rangle\rangle}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem*{lemma}{Lemma}
\newtheorem*{exercise}{Exercise}
\newtheorem*{answer}{Answer}
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]

\newcommand{\ad}{\mathrm{ad}}
\newcommand{\Ad}{\mathrm{Ad}}
\renewcommand{\O}{\mathrm{O}}
\newcommand{\SO}{\mathrm{SO}}
\newcommand{\U}{\mathrm{U}}
\newcommand{\SU}{\mathrm{SU}}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\SL}{\mathrm{SL}}
\renewcommand{\o}{\mf{o}}
\newcommand{\so}{\mf{so}}
\renewcommand{\u}{\mf{u}}
\newcommand{\su}{\mf{su}}
\newcommand{\gl}{\mf{gl}}
\renewcommand{\sl}{\mf{sl}}

\renewcommand{\d}{\mathrm{d}}
\newcommand{\D}{\mathrm{D}}
\makeatletter
\newcommand{\pd}[1]{%
  \@ifnextchar\bgroup{\pd@twoargs{#1}}{\pd@onearg{#1}}%
}
\newcommand{\pd@onearg}[1]{
  \frac{\partial}{\partial #1}
}
\newcommand{\pd@twoargs}[2]{
  \frac{\partial #1}{\partial #2}
}
\newcommand{\dd}[1]{%
  \@ifnextchar\bgroup{\dd@twoargs{#1}}{\dd@onearg{#1}}%
}
\newcommand{\dd@onearg}[1]{
  \frac{\mathrm{d}}{\mathrm{d} #1}
}
\newcommand{\dd@twoargs}[2]{
  \frac{\mathrm{d} #1}{\mathrm{d} #2}
}
\makeatother

\title{Uncertainty Quantification Notes}
\author{Noah Silverberg}
\date{}

\begin{document}

\maketitle

\section{Evidential Regression}

Idea adapted from \cite{sensoy2018evidentialdeeplearningquantify}, which was UQ for classification networks. Then some papers extended it to regression, such as \cite{amini2020deep}.

\subsection{Background/Theory}

\subsubsection{Model Output and Interpretation}

Let's say we want a normal distribution as our output for the brightness of a single pixel $y$, i.e.,
$$p(y \mid x) = \mathcal{N}(y \mid \mu, \sigma^2)$$
and we want to place a Gaussian prior over $\mu$ (we'd ideally like pixel values to be in the range $[0, 1]$, but this will be a good approximation) and an inverse gamma prior over $\sigma^2$ (we need to ensure $\sigma^2 > 0$):
$$\mu \sim \mathcal{N}(\gamma, \sigma^2/\nu), \quad \sigma^2 \sim \Gamma^{-1}(\alpha, \beta)$$
Then we can write the joint prior over the outputs as:
$$p(\mu, \sigma^2 \mid \gamma, \nu, \alpha, \beta) = \mathcal{N}(\mu \mid \gamma, \sigma^2/\nu) \cdot \Gamma^{-1}(\sigma^2 \mid \alpha, \beta)= \text{N-}\Gamma^{-1}(\mu, \sigma^2 \mid \gamma, \nu, \alpha, \beta)$$
where $\text{N-}\Gamma^{-1}$ is the normal-inverse-gamma distribution.

So we can write the distribution over the pixel brightness as:
\begin{align*}
    &p(y \mid \gamma, \nu, \alpha, \beta) \\
    &= \int_0^\infty \int_{-\infty}^\infty p(y \mid \mu, \sigma^2) p(\mu, \sigma^2 \mid \gamma, \nu, \alpha, \beta)\ \d\mu\,\d\sigma^2 \\
    &= \int_0^\infty \int_{-\infty}^\infty \mathcal{N}(y \mid \mu, \sigma^2) \text{N-}\Gamma^{-1}(\mu, \sigma^2 \mid \gamma, \nu, \alpha, \beta)\ \d\mu\,\d\sigma^2 \\
    &= \int_0^\infty \int_{-\infty}^\infty \mathcal{N}(y \mid \mu, \sigma^2) \cdot \mathcal{N}(\mu \mid \gamma, \sigma^2/\nu) \cdot \Gamma^{-1}(\sigma^2 \mid \alpha, \beta)\ \d\mu\,\d\sigma^2 \\
    &= \int_0^\infty \Gamma^{-1}(\sigma^2 \mid \alpha, \beta) \left( \int_{-\infty}^\infty \mathcal{N}(y \mid \mu, \sigma^2) \cdot \mathcal{N}(\mu \mid \gamma, \sigma^2/\nu)\ \d\mu \right) \d\sigma^2
\end{align*}

The inner integral is a pain to derive, but it can be shown that:
$$\int_{-\infty}^\infty \mathcal{N}(y \mid \mu, \sigma^2) \cdot \mathcal{N}(\mu \mid \gamma, \sigma^2/\nu)\ \d\mu = \mathcal{N}(y \mid \gamma, (1 + 1/\nu)\sigma^2)$$
So we can write the distribution over the pixel brightness as:
$$p(y \mid \gamma, \nu, \alpha, \beta) = \int_0^\infty \Gamma^{-1}(\sigma^2 \mid \alpha, \beta) \cdot \mathcal{N}(y \mid \gamma, (1 + 1/\nu)\sigma^2)\ \d\sigma^2$$
This is also annoying to derive, but it can be shown that
$$p(y \mid \gamma, \nu, \alpha, \beta) = \text{Student-}t_{2\alpha} \left( y \mid \mathrm{loc} = \gamma,\,\mathrm{scale}^2 = \f{\beta(\nu + 1)}{\alpha \nu} \right)$$
where $\text{Student-}t_{2\alpha}$ is the Student's $t$ distribution with $2\alpha$ degrees of freedom.

So we get a prediction along with measures of both aleatoric and epistemic uncertainty \cite{amini2020deep}:
$$\underbrace{\mathbb{E}[\mu] = \gamma}_{\text{prediction}}, \quad \underbrace{\mathbb{E}[\sigma^2] = \f{\beta}{\alpha - 1}}_{\text{aleatoric}}, \quad \underbrace{\mathrm{Var}[\mu] = \f{\beta}{\nu(\alpha - 1)}}_{\text{epistemic}}$$

\subsubsection{Conjugate Priors}

Let's say we get a bunch of samples $y_1, y_2, \ldots, y_n$, and we want to update our beliefs about $\mu$ and $\sigma^2$ given this new data. By Bayes' theorem:
\begin{align*}
    p(\mu, \sigma^2 \mid y_1, y_2, \ldots, y_n) &\propto p(\mu, \sigma^2) \prod_{i=1}^n p(y_i \mid \mu, \sigma^2) \\
    &=\text{N-}\Gamma^{-1}(\mu, \sigma^2 \mid \gamma, \nu, \alpha, \beta) \cdot \prod_{i=1}^n \mathcal{N}(y_i \mid \mu, \sigma^2) \\
    &=\Gamma^{-1}(\sigma^2 \mid \alpha, \beta) \cdot \mathcal{N}(\mu \mid \gamma, \sigma^2/\nu) \cdot \prod_{i=1}^n \mathcal{N}(y_i \mid \mu, \sigma^2)
\end{align*}
Note:
$$\prod_{i=1}^n \mathcal{N}(y_i \mid \mu, \sigma^2) = \f{1}{(2\pi\sigma^2)^{n/2}} \exp\left(-\f{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu)^2\right)$$
and
$$\Gamma^{-1}(\sigma^2 \mid \alpha, \beta) = \f{\beta^\alpha}{\Gamma(\alpha)} (\sigma^2)^{-\alpha - 1} \exp\left(-\f{\beta}{\sigma^2}\right)$$
and
$$\mathcal{N}(\mu \mid \gamma, \sigma^2/\nu) = \f{1}{\sqrt{2\pi\sigma^2/\nu}} \exp\left(-\f{\nu}{2\sigma^2}(\mu - \gamma)^2\right)$$
So we can write (omitting intermediate steps):
$$p(\mu, \sigma^2 \mid \b{y}) = \mathcal{N}(\mu \mid \tilde{\gamma}, \sigma^2 / \tilde{\nu}) \cdot \Gamma^{-1}(\sigma^2 \mid \tilde{\alpha}, \tilde{\beta}) = \text{N-}\Gamma^{-1}(\mu, \sigma^2 \mid \tilde{\gamma}, \tilde{\nu}, \tilde{\alpha}, \tilde{\beta})$$
where
\begin{align*}
    \tilde{\gamma} &= \f{\nu\gamma + \sum_{i=1}^n y_i}{\nu + n} \\
    \tilde{\nu} &= \nu + n \\
    \tilde{\alpha} &= \alpha + \f{n}{2} \\
    \tilde{\beta} &= \beta + \f{1}{2}\sum_{i=1}^n (y_i - \bar{y})^2 + \f{\nu n}{2(\nu + n)}\left(\gamma - \bar{y}\right)^2
\end{align*}
So the posterior is also a normal-inverse-gamma distribution, which is nice because we can use the same model architecture for training and inference. This is called a conjugate prior.

\subsection{Training}

We need the model to now output the parameters of the prior distribution, i.e., $\gamma$, $\nu$, $\alpha$, and $\beta$, at each pixel. We will want our loss to be the negative log-likelihood, which is:
$$\mathcal{L}_\mathrm{NLL}(\gamma, \nu, \alpha, \beta \mid y_1, y_2, \ldots, y_n) = -\log p(y_1, y_2, \ldots, y_n \mid \gamma, \nu, \alpha, \beta)$$
This can be computed as:
\begin{align*}
    &\mathcal{L}_\mathrm{NLL}(\gamma, \nu, \alpha, \beta \mid y) \\
    &= \f{1}{2}\log\left(\f{\pi}{\nu}\right) - \alpha\log(\Omega) + \left(\alpha + \f{1}{2}\right)\log((y - \gamma)^2\nu + \Omega) + \log\left(\f{\Gamma(\alpha)}{\Gamma\left(\alpha + \f{1}{2}\right)}\right)
\end{align*}
where $\Omega = 2\beta(1 + \nu)$. The model may become overconfident by driving $\alpha,\nu \to \infty$, so we can add a regularization term to the loss:
$$\mathcal{L}_\mathrm{reg}(\gamma, \nu, \alpha, \beta \mid y) = |y - \gamma| \cdot (2\nu + \alpha)$$
to penalize high confidence when the prediction is far from the mean.

So the total loss is:
$$\mathcal{L}(\gamma, \nu, \alpha, \beta \mid y) = \mathcal{L}_\mathrm{NLL}(\gamma, \nu, \alpha, \beta \mid y) + \lambda\mathcal{L}_\mathrm{reg}(\gamma, \nu, \alpha, \beta \mid y)$$

\subsection{Pros/Cons}

\subsubsection{Pros}

\begin{itemize}
  \item Requires little change from our current DDCNN architecuture. The only differences are (1) we need to change the loss function, and (2) we need to output 4 channels instead of 1.
  \item We only need to do one forward pass to get the prediction and uncertainty estimates.
  \item We only need to train one model.
  \item Provides a measure of aleatoric and epistemic uncertainty \cite{amini2020deep}.
\end{itemize}

\subsubsection{Cons}

\begin{itemize}
  \item We don't necessarily know if the current size of the DDCNN would be sufficient to learn 4 parameters per pixel, it might require a larger model.
  \item It isn't a super popular method (although there are some papers that use it).
  \item It is not Bayesian, but I don't think that really matters to us.
  \item It doesn't necessarily know how to handle out-of-distribution data (this is kind of important actually since the uncertainty values we get from it will only really be valid if the data is similar to the training data).
\end{itemize}

\subsection{Application to Dual-Domain CBCT Pipeline}

% Place this block after your Pros/Cons list under Evidential Regression.

\subsubsection{Setup}
We denote the projection-domain network (PD) by $f_\text{PD}(x; \theta_\text{PD})$, which maps undersampled/interpolated projections $x$ to ``gated" projections. And we will write the reconstructed image as $\mathrm{FDK}\big(f_\text{PD}(x;\theta_\text{PD})\big)$. The image-domain network (ID) is $f_\text{ID}(I; \theta_\text{ID})$, which will output evidential parameters $(\gamma,\nu,\alpha,\beta)$ per pixel to characterize $p(y \mid \gamma,\nu,\alpha,\beta)$.

\subsubsection{Training}
In the PD we will use a model with MC dropout. (Luckily we already have this from the MC dropout training, so we can just use this as-is.) Note that if we were to use a deterministic PD model during training, the epistemic PD uncertainty would never be propagated through to the ID (since it is never even quantified in the first place). Note also that we can't use an evidential head in the PD,  since we can't analytically propagate the outputted uncertainties through FDK.

So our only hope for quantifying and propagating PD uncertainty is to do some sort of Monte Carlo sampling through the FDK step -- there doesn't appear to be any way around this. \textit{However}, fortunately this is only true of the training. Consider the following. If we have an evidential ID network, during training the ID network will see variations in its inputs, which are combined true aleatoric uncertainty and PD epistemic uncertainty. However, as far as the ID network is concerned, it doesn't ``know" that these variations include PD epistemic uncertainty -- it simply sees this as uncertainty in its input data. So it will learn the ``aleatoric" uncertainty, which is actually the combined true aleatoric uncertainty plus the PD epistemic uncertainty. Combining this with the ID model uncertainty, we get the total uncertainty of the dual-domain pipeline, including aleatoric uncertainty. [[TODO -- can we remove the true aleatoric uncertainty somehow?]]

During inference, then, we don't really need to do MC sampling in the PD, since a single pass through the PD$\to$FDK$\to$ID will already include the true aleatoric uncertainty, the PD epistemic uncertainty, and the ID epistemic uncertainty (since this was all learned by the ID evidential head during training). So our MC dropout in the PD only needs to be done during training, thankfully. This allows us to still do a single forward pass during inference to get both our clean reconstructed image and our uncertainty map.

More concretely:
\begin{enumerate}[1.]
  \item \textbf{PD sampling:} For each training projection input $x_i$, sample $K$ variants of PD outputs by applying MC-dropout at training time: 
    \[
      s_i^{(k)} = f_\text{PD}(x_i; \hat\theta_\text{PD}, \text{mask}^{(k)}), \quad k=1,\dots,K.
    \]
  \item \textbf{FDK reconstructions:} For each sampled sinogram $s_i^{(k)}$, compute $I_i^{(k)} = \mathrm{FDK}\bigl(s_i^{(k)}\bigr)$. This yields $K$ image inputs per scan.
  \item \textbf{ID training set:} Form the dataset $\{(I_i^{(k)}, y_i^\text{truth}) : i=1,\dots,N,\;k=1,\dots,K\}$. The ID evidential network sees varied inputs mapping to the same ground truth. Again, these variations in the inputs are due to both true aleatoric uncertainty and also PD epistemic uncertainty. For example if we train for 50 epochs, we might set $K=10$ and use each image 5 times. Ideally we would have $K=50$ (if time/storage allows) so we never re-use training data, since this would give our ID the best chance of handling the underlying PD model distribution. Note that this is actually quite nice for ID training -- ideally, for all models we would have an infinite stream of training data, so we could only use each sample once (since this would provide an unbiased estimate of the gradient during optimization). However, normally we don't have enough data for that, so we have to use repeated samples. Although this performs well empirically, we technically speaking are biased since the samples aren't independent. By being able to sample infinitely from the PD model (effectively, although technically speaking we only have a finite number of possible Bernoulli masks) for our ID training data, we get one step closer to the ideal case of infinite training data from the underlying distribution. However, to be fair we aren't \textit{fully} at the ideal case yet, since we are still biased in that we only are adding PD epistemic uncertainty to a fixed set of training scans -- we don't also get infinite samples from the underlying scan distribution.  But the training strategy is still closer than reusing the same training data for 50 epochs.
  \item \textbf{Loss:}
There are a few loss functions we will need to try. The one discussed earlier from \cite{amini2020deep} is the ``basic" one. However, there are some theoretical issues with this, such as the ``evidence contraction" problem [[TODO -- cite papers]]. There are various loss terms that have been proposed to handle this. We also would probably want to still include some normal loss term like $L^1$ (or smooth $L^1$ as we are currently using). We will need to do lots of hyperparameter tuning to control the strength of each of these terms in the loss function. We might also want to consider  having these hyperparameters change throughout the course of training (e.g., we might want the $L^1$ to dominate at the start to get us in a region of the loss landscape where the reconstruction is good, and then let the evidential terms increase their influence later in training to fine tune the uncertainties without harming the reconstruction accuracy too much).
\end{enumerate}

\subsubsection{Inference}
At inference, run:
\[
  s^\ast = f_\text{PD}(x^\ast; \hat\theta_\text{PD}) \quad\to\quad I^\ast = \mathrm{FDK}(s^\ast) \quad\to\quad (\gamma,\nu,\alpha,\beta) = f_\text{ID}(I^\ast; \hat\theta_\text{ID}).
\]
The output predictive variance 
\[
  \underbrace{\mathbb{E}[\sigma^2]}_{\text{aleatoric + PD epistemic}} \;+\; \underbrace{\mathrm{Var}[\mu]}_{\text{ID epistemic}}
\]
approximates the total uncertainty learned from PD sampling during training. We do not decompose PD epistemic explicitly, but its effect is baked into the learned aleatoric term. 

\section{MC Dropout}

[[TODO -- general idea is that this requires minimal changes to DDCNN architecture (just need to turn on dropout during training and inference), but the main issue is that it takes multiple forward passes to get the uncertainty estimates, which is not practical in a clinical setting. This does seem to have good performance empirically, though.]]

\subsection{Dual-Domain MC Dropout Training and Inference}

We train the PD CNN with dropout, and propagate PD uncertainty into the ID by sampling the dropout/Bernoulli masks. At inference we do multiple forward passes to estimate uncertainty.

\subsubsection{PD Training}
Train the projection-domain network $f_\text{PD}(x;\theta_\text{PD})$ with dropout in the usual way on training projections. This yields a model that, when run with dropout at inference, produces approximate posterior samples \cite{gal2016dropoutbayesianapproximationrepresenting}.

\subsubsection{ID Training}
Since we want the ID to see PD-induced variability:
\begin{enumerate}[1.]
  \item \textbf{PD sampling:} For each training projection input $x_i$, perform $K$ forward passes with dropout active:
    \[
      s_i^{(k)} = f_\text{PD}(x_i;\hat\theta_\text{PD},\text{mask}^{(k)}),\quad k=1,\dots,K.
    \]
    (this is identical to the evidential strategy proposed earlier)
  \item \textbf{FDK reconstructions:} For each $s_i^{(k)}$, compute 
    \[
      I_i^{(k)} = \mathrm{FDK}\bigl(s_i^{(k)}\bigr).
    \]
    This yields $K$ image inputs per scan.
  \item \textbf{ID training set:} Form $\{(I_i^{(k)},y_i^\text{truth})\}$ and train the ID network $f_\text{ID}(I;\theta_\text{ID})$ (e.g.\ standard regression loss or with an uncertainty head). The ID thus learns to handle input variations arising from PD dropout.
\end{enumerate}

\subsubsection{Inference}
At test time, for each new input $x^\ast$:
\begin{enumerate}[1.]
  \item Perform $M$ forward passes through PD with dropout active:
    \[
      s^{(m)} = f_\text{PD}(x^\ast;\hat\theta_\text{PD},\text{mask}^{(m)}), \quad m=1,\dots,M.
    \]
  \item For each $s^{(m)}$, compute $I^{(m)} = \mathrm{FDK}(s^{(m)})$, then run ID to obtain predictions $\hat y^{(m)} = f_\text{ID}(I^{(m)};\hat\theta_\text{ID})$.
  \item Aggregate $\{\hat y^{(m)}\}$ (e.g.\ compute mean and variance) as final output and uncertainty estimate.
\end{enumerate}
This yields total epistemic uncertainty from PD dropout and ID model uncertainty, since we have dropout on for both domains at inference.  


\section{Bayesian By Backprop (BBB)}

[[TODO -- general idea is that this requires a lot of changes to the DDCNN architecuture, but it is fully Bayesian. Similar to MC Dropout, though, we need to do multiple forward passes to get the uncertainty estimates, which is not practical in a clinical setting. Also it doesn't seem to have super good performance from what I've read.]]

\subsection{Dual-Domain BBB Training and Inference}

This is entirely analogous to the MC dropout training and inference case -- we treat them the same.


\section{Deep Ensembles}

[[TODO -- this requires no changes at all which is really nice. But it suffers again from the same issue as MCD and BBB which is multiple forward passes. However this emprirically does have good results.]]

\subsection{Dual-Domain Deep Ensembles}

We train multiple PD and ID models independently. At inference we perform multiple forward passes across the ensemble to estimate total uncertainty.

\subsubsection{PD and ID Paired Training}
\begin{enumerate}[1.]
  \item \textbf{PD ensemble:} Train $M$ PD networks $\{f_{\text{PD},i}(x;\theta_{\text{PD},i})\}_{i=1}^M$ from different random initializations (and optionally different shuffles/augmentations).
  \item \textbf{ID paired:} For each PD$_i$, generate sinogram outputs on all training inputs:
    \[
      s_{i,j} = f_{\text{PD},i}(x_j;\hat\theta_{\text{PD},i}),\quad j=1,\dots,N.
    \]
    Compute $I_{i,j} = \mathrm{FDK}(s_{i,j})$. Train ID$_i$ on $\{(I_{i,j},y_j^\text{truth})\}$.
\end{enumerate}
Note that we do \textit{not} combine the results from the different PD models since then we would not actually have an ensemble anymore, i.e., the PD component of each ``ensemble" component would have overlap since they would contain outputs from the sane models, and therefore we would not get a true ensemble.

\subsubsection{Inference}
For a test input $x^\ast$:
\begin{enumerate}[1.]
  \item For each ensemble member $i=1,\dots,M$, run
    \[
      s_i^\ast = f_{\text{PD},i}(x^\ast;\hat\theta_{\text{PD},i}),\quad I_i^\ast = \mathrm{FDK}(s_i^\ast),\quad \hat y_i^\ast = f_{\text{ID},i}(I_i^\ast;\hat\theta_{\text{ID},i}).
    \]
  \item Aggregate $\{\hat y_i^\ast\}$ (mean, variance) to obtain final prediction and uncertainty.
\end{enumerate}
This captures total epistemic uncertainty across PD and ID ensembles.


% Create bibliography with sensoy paper
\bibliographystyle{plain}
\bibliography{references}

\end{document}
