{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "This initial block sets up the environment, defines file paths, and specifies the validation scans and model versions to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.paths import Directories, Files\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "if torch.cuda.is_available():\n",
    "    CUDA_DEVICE = torch.device(\"cuda:0\")\n",
    "    print(f\"CUDA is available. Using device: {CUDA_DEVICE}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your PyTorch installation. Using CPU instead.\")\n",
    "    CUDA_DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "PHASE = '7'\n",
    "DATA_VERSION = '13'\n",
    "\n",
    "\n",
    "# Base directory\n",
    "WORK_ROOT = \"D:/NoahSilverberg/ngCBCT\"\n",
    "\n",
    "# NSG_CBCT Path where the raw matlab data is stored\n",
    "NSG_CBCT_PATH = \"D:/MitchellYu/NSG_CBCT\"\n",
    "\n",
    "# Directory with all files specific to this phase/data version\n",
    "PHASE_DATAVER_DIR = os.path.join(\n",
    "    WORK_ROOT, f\"phase{PHASE}\", f\"DS{DATA_VERSION}\"\n",
    ")\n",
    "\n",
    "DIRECTORIES = Directories(\n",
    "    # mat_projections_dir=os.path.join(NSG_CBCT_PATH, \"data/prj/HF/mat\"),\n",
    "    # pt_projections_dir=os.path.join(WORK_ROOT, \"prj_pt\"),\n",
    "    # projections_aggregate_dir=os.path.join(PHASE_DATAVER_DIR, \"aggregates\", \"projections\"),\n",
    "    # projections_model_dir=os.path.join(PHASE_DATAVER_DIR, \"models\", \"projections\"),\n",
    "    # projections_results_dir=os.path.join(PHASE_DATAVER_DIR, \"results\", \"projections\"),\n",
    "    # projections_gated_dir=os.path.join(WORK_ROOT, \"gated\", \"prj_mat\"),\n",
    "    reconstructions_dir=os.path.join(PHASE_DATAVER_DIR, \"reconstructions\"),\n",
    "    reconstructions_gated_dir=os.path.join(WORK_ROOT, \"gated\", \"fdk_recon\"),\n",
    "    # reconstructions_dir=os.path.join(\"H:\\\\\", \"Public\", \"Noah\", \"reconstructions\"),\n",
    "    # reconstructions_gated_dir=os.path.join(\"H:\\\\\", \"Public\", \"Noah\", \"gated\", \"fdk_recon\"),\n",
    "    # images_aggregate_dir=os.path.join(PHASE_DATAVER_DIR, \"aggregates\", \"images\"),\n",
    "    # images_model_dir=os.path.join(PHASE_DATAVER_DIR, \"models\", \"images\"),\n",
    "    # images_results_dir=os.path.join(PHASE_DATAVER_DIR, \"results\", \"images\"),\n",
    ")\n",
    "\n",
    "FILES = Files(DIRECTORIES)\n",
    "\n",
    "VAL_SCANS = [('02', '01') , ('02', '02'), ('16', '01'), ('16', '02'), ('22', '01')] # , ('22', '02')]\n",
    "# VAL_SCANS = [('08', '01'), ('10', '01'), ('14', '01'), ('14', '02'), ('15', '01'), ('20', '01')]\n",
    "\n",
    "SCAN_TYPE = 'HF'\n",
    "MODEL_VERSIONS = ['MK7_MCDROPOUT_30_pct']\n",
    "PASSTHROUGH_COUNT = 30 # use None for multiple models -- only use this for one model for MC dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd7fb7",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "This block gathers the file paths for the model's reconstructions (50 passes for MC Dropout) and the corresponding ground truth reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4132e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_paths_dict = {}\n",
    "recon_names_dict = {}\n",
    "gt_paths_dict = {}\n",
    "\n",
    "for patient, scan in VAL_SCANS:\n",
    "    print(f\"Processing patient {patient}, scan {scan}\")\n",
    "    recon_paths = []\n",
    "    recon_names = []\n",
    "    if PASSTHROUGH_COUNT is None:\n",
    "        for model_version in MODEL_VERSIONS:\n",
    "            recon_path = FILES.get_recon_filepath(model_version, patient, scan, SCAN_TYPE, gated=False)\n",
    "            recon_paths.append(recon_path)\n",
    "            recon_names.append(model_version)\n",
    "    else:\n",
    "        for i in range(PASSTHROUGH_COUNT):\n",
    "            recon_path = FILES.get_recon_filepath(MODEL_VERSIONS[0], patient, scan, SCAN_TYPE, gated=False, passthrough_num=i)\n",
    "            recon_paths.append(recon_path)\n",
    "            recon_names.append(f\"Passthrough {i+1}\")\n",
    "\n",
    "    gt_path = FILES.get_recon_filepath('fdk', patient, scan, SCAN_TYPE, gated=True)\n",
    "    gt_paths_dict[(patient, scan)] = gt_path\n",
    "\n",
    "    recon_paths_dict[(patient, scan)] = recon_paths\n",
    "    recon_names_dict[(patient, scan)] = recon_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a6e21d",
   "metadata": {},
   "source": [
    "## Analysis Loop\n",
    "The main loop iterates through each scan. For each scan, it loads all 50 reconstruction passes, calculates the mean (prediction) and standard deviation (uncertainty), and then performs several analyses to evaluate the quality of the uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now go through each patient and scan, load the reconstructions\n",
    "# calclate the mean and std (pixel-wise) and the error from GT\n",
    "for (patient, scan), recon_paths in recon_paths_dict.items():\n",
    "    print(f\"\\nProcessing reconstructions for patient {patient}, scan {scan}\")\n",
    "    \n",
    "    # --- MODIFICATION: Apply clip-and-scale to Ground Truth ---\n",
    "    gt_path = gt_paths_dict[(patient, scan)]\n",
    "    gt_recon_raw = torch.load(gt_path).cpu().numpy()\n",
    "    gt_tensor = torch.from_numpy(gt_recon_raw)\n",
    "    gt_processed_tensor = torch.permute(torch.clip(gt_tensor, 0, 0.04) * 25.0, (0, 2, 1))\n",
    "    gt_recon = gt_processed_tensor.cpu().numpy()\n",
    "    del gt_recon_raw, gt_tensor, gt_processed_tensor\n",
    "    \n",
    "    # Initialize lists to hold reconstructions\n",
    "    reconstructions = []\n",
    "    \n",
    "    for recon_path in recon_paths:\n",
    "        recon_raw = torch.load(recon_path).cpu().numpy()\n",
    "        \n",
    "        # --- MODIFICATION: Apply clip-and-scale to each Reconstruction ---\n",
    "        recon_tensor = torch.from_numpy(recon_raw)\n",
    "        recon_processed_tensor = torch.permute(torch.clip(recon_tensor, 0, 0.04) * 25.0, (0, 2, 1))\n",
    "        recon = recon_processed_tensor.cpu().numpy()\n",
    "        reconstructions.append(recon)\n",
    "\n",
    "    print(f\"Loaded and processed {len(reconstructions)} reconstructions for patient {patient}, scan {scan}\")\n",
    "    \n",
    "    # Convert to numpy array for easier manipulation\n",
    "    reconstructions = np.array(reconstructions)\n",
    "    \n",
    "    # --- Basic Calculations ---\n",
    "    # Calculate mean and std across the first axis (across models or passthroughs)\n",
    "    mean_recon = np.mean(reconstructions, axis=0)\n",
    "    std_recon = np.std(reconstructions, axis=0)\n",
    "\n",
    "    # Plot the 100th slice of the mean reconstruction and GT and uncertainty and abs error\n",
    "    slice_index = 100\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(mean_recon[slice_index], cmap='gray')\n",
    "    plt.title(f'Mean Reconstruction - Patient {patient}, Scan {scan}')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(gt_recon[slice_index], cmap='gray')\n",
    "    plt.title(f'Ground Truth Reconstruction - Patient {patient}, Scan {scan}')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(std_recon[slice_index], cmap='inferno')\n",
    "    plt.title(f'Standard Deviation of Reconstructions - Patient {patient}, Scan {scan}')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(np.abs(mean_recon[slice_index] - gt_recon[slice_index]), cmap='viridis')\n",
    "    plt.title(f'Absolute Error - Patient {patient}, Scan {scan}')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate error from ground truth\n",
    "    error = mean_recon - gt_recon\n",
    "    \n",
    "    # Efficiently create a circular mask in the last two dimensions\n",
    "    z, y, x = gt_recon.shape\n",
    "    yy, xx = np.ogrid[:y, :x]\n",
    "    center = np.array([y // 2, x // 2])\n",
    "    radius = 225\n",
    "    dist_from_center = np.sqrt((yy - center[0])**2 + (xx - center[1])**2)\n",
    "    slice_mask = dist_from_center <= radius\n",
    "    mask = np.broadcast_to(slice_mask, gt_recon.shape)\n",
    "    \n",
    "    flat_std = std_recon[mask].flatten()\n",
    "    flat_error = error[mask].flatten()\n",
    "    flat_abs_error = np.abs(flat_error)\n",
    "    flat_squared_error = flat_error**2\n",
    "\n",
    "    # Clean up memory\n",
    "    del reconstructions, mean_recon, std_recon, error, gt_recon\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Analysis 1: Standardized Error Histogram (Your Original Analysis) ---\n",
    "    standardized_error = flat_error / (flat_std + 1e-8)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(standardized_error, bins=100, color='gray', alpha=0.7, range=(-30, 30))\n",
    "    plt.title(f'UNCALIBRATED Standardized Error Histogram for Patient {patient}, Scan {scan}')\n",
    "    plt.xlabel('Standardized Error (Error / Predicted Std Dev)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "    percent_within_1 = np.mean(np.abs(standardized_error) < 1) * 100\n",
    "    percent_within_2 = np.mean(np.abs(standardized_error) < 2) * 100\n",
    "    percent_within_3 = np.mean(np.abs(standardized_error) < 3) * 100\n",
    "    print(f\"Patient {patient}, Scan {scan} - UNCALIBRATED Coverage:\")\n",
    "    print(f\"  Percent with abs standardized error < 1: {percent_within_1:.2f}% (Expected for Gaussian: ~68%)\")\n",
    "    print(f\"  Percent with abs standardized error < 2: {percent_within_2:.2f}% (Expected for Gaussian: ~95%)\")\n",
    "    print(f\"  Percent with abs standardized error < 3: {percent_within_3:.2f}% (Expected for Gaussian: ~99.7%)\")\n",
    "\n",
    "    # --- Analysis 2 & 3: Plots for Hyperparameter Tuning (Visual) ---\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.hist2d(flat_std, flat_abs_error, bins=50, cmap='inferno', range=[[0, 0.0005], [0, 0.005]])\n",
    "    plt.xlim(0, 0.0005)\n",
    "    plt.ylim(0, 0.005)\n",
    "    plt.colorbar(label='Voxel Count')\n",
    "    plt.title(f'Predicted Uncertainty vs. Actual Error for Patient {patient}, Scan {scan}')\n",
    "    plt.xlabel('Predicted Standard Deviation (Uncertainty)')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "    \n",
    "    sorted_indices = np.argsort(flat_std)\n",
    "    sorted_squared_error = flat_squared_error[sorted_indices]\n",
    "    fractions_removed = np.linspace(0, 0.5, 51)\n",
    "    mse_remaining = np.zeros_like(fractions_removed)\n",
    "    for i, frac in enumerate(fractions_removed):\n",
    "        num_to_keep = int((1 - frac) * len(sorted_squared_error))\n",
    "        mse_remaining[i] = np.mean(sorted_squared_error[:num_to_keep]) if num_to_keep > 0 else 0\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fractions_removed * 100, mse_remaining, 'o-', color='crimson')\n",
    "    plt.title(f'Sparsification Plot for Patient {patient}, Scan {scan}')\n",
    "    plt.xlabel('Percent of Most Uncertain Voxels Removed (%)')\n",
    "    plt.ylabel('Mean Squared Error (on remaining voxels)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Analysis 4: Quantitative Tuning Metrics (New Section) ---\n",
    "    # These single numbers help decide which dropout rate is better by focusing on correlation\n",
    "    # and ranking, ignoring the bad scaling of the raw uncertainty.\n",
    "    \n",
    "    # Spearman's Rank Correlation: Measures if high uncertainty ranks correspond to high error ranks. Higher is better.\n",
    "    from scipy.stats import spearmanr\n",
    "    spearman_corr, _ = spearmanr(flat_std, flat_abs_error)\n",
    "    \n",
    "    # Area Under Sparsification Curve (AUSC): Quantifies the sparsification plot. Lower is better.\n",
    "    ausc = np.trapz(mse_remaining, fractions_removed)\n",
    "\n",
    "    print(f\"\\n--- Quantitative Tuning Metrics for Patient {patient}, Scan {scan} ---\")\n",
    "    print(f\"  Spearman's Rank Correlation: {spearman_corr:.4f} (Higher is better)\")\n",
    "    print(f\"  Area Under Sparsification Curve (AUSC): {ausc:.6e} (Lower is better)\")\n",
    "    \n",
    "    # --- Analysis 5: Post-Hoc Uncertainty Calibration (Updated Section) ---\n",
    "    print(\"\\n--- Calibration Results ---\")\n",
    "\n",
    "    # Method 1: Linear Calibration\n",
    "    p_linear = np.polyfit(flat_std, flat_abs_error, 1)\n",
    "    calibrated_std_linear = np.polyval(p_linear, flat_std)\n",
    "    calibrated_std_linear[calibrated_std_linear < 0] = 0\n",
    "\n",
    "    # Method 2: Binned (Isotonic-like) Calibration (No sklearn needed)\n",
    "    # This method learns the average error for different levels of predicted uncertainty.\n",
    "    num_bins_calib = 100\n",
    "    # Sort data by predicted std to make binning efficient\n",
    "    sort_indices_calib = np.argsort(flat_std)\n",
    "    sorted_std_calib = flat_std[sort_indices_calib]\n",
    "    sorted_sq_err_calib = flat_squared_error[sort_indices_calib]\n",
    "    \n",
    "    # Create bins with roughly equal numbers of points\n",
    "    bin_boundaries = np.quantile(sorted_std_calib, np.linspace(0, 1, num_bins_calib + 1))\n",
    "    \n",
    "    calibrated_std_binned = np.zeros_like(flat_std)\n",
    "    \n",
    "    for i in range(num_bins_calib):\n",
    "        lower = bin_boundaries[i]\n",
    "        upper = bin_boundaries[i+1]\n",
    "        # Find all voxels that fall into this uncertainty bin\n",
    "        in_bin = (flat_std >= lower) & (flat_std <= upper)\n",
    "        if np.any(in_bin):\n",
    "            # Calculate the true average error (RMSE) for all voxels in this bin\n",
    "            true_rmse_in_bin = np.sqrt(np.mean(flat_squared_error[in_bin]))\n",
    "            # Assign this true RMSE as the calibrated uncertainty for all those voxels\n",
    "            calibrated_std_binned[in_bin] = true_rmse_in_bin\n",
    "\n",
    "    # --- Visualize the Calibrated Results ---\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    fig.suptitle(f'Calibration Plots for Patient {patient}, Scan {scan}')\n",
    "\n",
    "    num_bins_plot = 100 # Can use fewer bins for a cleaner plot\n",
    "\n",
    "    # Linear Calibration Plot (using Quantile Bins for plotting)\n",
    "    bin_limits_lin = np.quantile(calibrated_std_linear, np.linspace(0, 1, num_bins_plot + 1))\n",
    "    rmse_per_bin_lin = [np.sqrt(np.mean(flat_squared_error[(calibrated_std_linear >= ll) & (calibrated_std_linear < ul)])) if np.any((calibrated_std_linear >= ll) & (calibrated_std_linear < ul)) else 0 for ll, ul in zip(bin_limits_lin[:-1], bin_limits_lin[1:])]\n",
    "    mean_std_per_bin_lin = [np.mean(calibrated_std_linear[(calibrated_std_linear >= ll) & (calibrated_std_linear < ul)]) if np.any((calibrated_std_linear >= ll) & (calibrated_std_linear < ul)) else 0 for ll, ul in zip(bin_limits_lin[:-1], bin_limits_lin[1:])]\n",
    "    ax1.plot(mean_std_per_bin_lin, rmse_per_bin_lin, 'o-', label='Linear Calibration', color='green')\n",
    "    lims1 = [0, max(np.max(mean_std_per_bin_lin), np.max(rmse_per_bin_lin))]\n",
    "    ax1.plot(lims1, lims1, 'k--', label='Perfect Calibration')\n",
    "    ax1.set_title('After Linear Calibration'); ax1.set_xlabel('Avg. Pred. Std'); ax1.set_ylabel('RMSE'); ax1.legend(); ax1.grid(True, alpha=0.5); ax1.axis('equal');\n",
    "\n",
    "    # Binned Calibration Plot (using Quantile Bins for plotting)\n",
    "    bin_limits_binned = np.quantile(calibrated_std_binned, np.linspace(0, 1, num_bins_plot + 1))\n",
    "    rmse_per_bin_binned = [np.sqrt(np.mean(flat_squared_error[(calibrated_std_binned >= ll) & (calibrated_std_binned < ul)])) if np.any((calibrated_std_binned >= ll) & (calibrated_std_binned < ul)) else 0 for ll, ul in zip(bin_limits_binned[:-1], bin_limits_binned[1:])]\n",
    "    mean_std_per_bin_binned = [np.mean(calibrated_std_binned[(calibrated_std_binned >= ll) & (calibrated_std_binned < ul)]) if np.any((calibrated_std_binned >= ll) & (calibrated_std_binned < ul)) else 0 for ll, ul in zip(bin_limits_binned[:-1], bin_limits_binned[1:])]\n",
    "    ax2.plot(mean_std_per_bin_binned, rmse_per_bin_binned, 'o-', label='Binned Calibration', color='purple')\n",
    "    lims2 = [0, max(np.max(mean_std_per_bin_binned), np.max(rmse_per_bin_binned))]\n",
    "    ax2.plot(lims2, lims2, 'k--', label='Perfect Calibration')\n",
    "    ax2.set_title('After Binned (Isotonic-like) Calibration'); ax2.set_xlabel('Avg. Pred. Std'); ax2.set_ylabel('RMSE'); ax2.legend(); ax2.grid(True, alpha=0.5); ax2.axis('equal');\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]); plt.show()\n",
    "\n",
    "    # --- Final Metrics After Calibration ---\n",
    "    # Now that the uncertainty is scaled correctly, we can calculate meaningful absolute metrics.\n",
    "    calibrated_standardized_error_linear = flat_error / (calibrated_std_linear + 1e-8)\n",
    "    percent_within_1_linear = np.mean(np.abs(calibrated_standardized_error_linear) < 1) * 100\n",
    "    \n",
    "    calibrated_standardized_error_binned = flat_error / (calibrated_std_binned + 1e-8)\n",
    "    percent_within_1_binned = np.mean(np.abs(calibrated_standardized_error_binned) < 1) * 100\n",
    "\n",
    "    # Calibrated Negative Log-Likelihood (NLL)\n",
    "    # This is a key metric. A lower NLL indicates a better overall model (accuracy + uncertainty).\n",
    "    # Use the calibrated variance (std^2) for this calculation.\n",
    "    nll_linear = np.mean(0.5 * flat_squared_error / (calibrated_std_linear**2 + 1e-8) + 0.5 * np.log(calibrated_std_linear**2 + 1e-8))\n",
    "    nll_binned = np.mean(0.5 * flat_squared_error / (calibrated_std_binned**2 + 1e-8) + 0.5 * np.log(calibrated_std_binned**2 + 1e-8))\n",
    "\n",
    "    print(f\"--- Final Calibrated Metrics for Patient {patient}, Scan {scan} ---\")\n",
    "    print(f\"  Coverage (Ideal is ~68%):\")\n",
    "    print(f\"    [Linear]   Percent of errors within 1 calibrated sigma: {percent_within_1_linear:.2f}%\")\n",
    "    print(f\"    [Binned]   Percent of errors within 1 calibrated sigma: {percent_within_1_binned:.2f}%\")\n",
    "    print(f\"  Calibrated Negative Log-Likelihood (Lower is better):\")\n",
    "    print(f\"    [Linear]   NLL: {nll_linear:.4f}\")\n",
    "    print(f\"    [Binned]   NLL: {nll_binned:.4f}\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    del flat_std, flat_error, flat_abs_error, flat_squared_error, standardized_error\n",
    "    del calibrated_std_linear, calibrated_std_binned\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a629e",
   "metadata": {},
   "source": [
    "# Analysis 2: Calibration then look at hold-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70cd1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "# --- 1. Create a Train/Test Split for Calibration ---\n",
    "# We will randomly split the validation scans into two halves.\n",
    "# We'll learn the calibration map on the first half and evaluate it on the second.\n",
    "\n",
    "# Shuffle the list of validation scans to ensure a random split\n",
    "random.shuffle(VAL_SCANS)\n",
    "split_point = len(VAL_SCANS) // 2\n",
    "calibration_scans = VAL_SCANS[:split_point]\n",
    "evaluation_scans = VAL_SCANS[split_point:]\n",
    "\n",
    "print(f\"Total scans: {len(VAL_SCANS)}\")\n",
    "print(f\"Calibration Set (for learning the map): {calibration_scans}\")\n",
    "print(f\"Evaluation Set (for testing the map): {evaluation_scans}\")\n",
    "\n",
    "\n",
    "# --- 2. Learn the Calibration Map from the Calibration Set ---\n",
    "# We will aggregate the raw uncertainty and error from all scans in the calibration set.\n",
    "\n",
    "all_calib_std = []\n",
    "all_calib_abs_error = []\n",
    "all_calib_squared_error = []\n",
    "\n",
    "print(\"\\n--- Aggregating data from calibration set... ---\")\n",
    "for (patient, scan) in calibration_scans:\n",
    "    print(f\"  Loading data for calibration from: Patient {patient}, Scan {scan}\")\n",
    "    \n",
    "    # Load ground truth and reconstructions\n",
    "    gt_path = gt_paths_dict[(patient, scan)]\n",
    "    gt_recon = torch.load(gt_path).cpu().numpy()\n",
    "    \n",
    "    reconstructions = []\n",
    "    recon_paths = recon_paths_dict[(patient, scan)]\n",
    "    for recon_path in recon_paths:\n",
    "        reconstructions.append(torch.load(recon_path).cpu().numpy())\n",
    "    \n",
    "    reconstructions = np.array(reconstructions)\n",
    "    \n",
    "    # Calculate basic stats\n",
    "    mean_recon = np.mean(reconstructions, axis=0)\n",
    "    std_recon = np.std(reconstructions, axis=0)\n",
    "    error = mean_recon - gt_recon\n",
    "    \n",
    "    # Apply mask\n",
    "    z, y, x = gt_recon.shape\n",
    "    yy, xx = np.ogrid[:y, :x]\n",
    "    center = np.array([y // 2, x // 2])\n",
    "    radius = 225\n",
    "    dist_from_center = np.sqrt((yy - center[0])**2 + (xx - center[1])**2)\n",
    "    slice_mask = dist_from_center <= radius\n",
    "    mask = np.broadcast_to(slice_mask, gt_recon.shape)\n",
    "    \n",
    "    # Append flattened arrays to our master lists\n",
    "    all_calib_std.append(std_recon[mask].flatten())\n",
    "    all_calib_abs_error.append(np.abs(error[mask].flatten()))\n",
    "    all_calib_squared_error.append((error[mask].flatten())**2)\n",
    "\n",
    "    # Clean up\n",
    "    del reconstructions, mean_recon, std_recon, error, gt_recon\n",
    "    gc.collect()\n",
    "\n",
    "# Concatenate all data into single large arrays\n",
    "calib_flat_std = np.concatenate(all_calib_std)\n",
    "calib_flat_abs_error = np.concatenate(all_calib_abs_error)\n",
    "calib_flat_squared_error = np.concatenate(all_calib_squared_error)\n",
    "\n",
    "print(\"\\n--- Training calibration models... ---\")\n",
    "\n",
    "# Train Method 1: Linear Calibration Model\n",
    "# Fits a line: y = a*x + b, mapping raw std dev to absolute error.\n",
    "linear_calib_coeffs = np.polyfit(calib_flat_std, calib_flat_abs_error, 1)\n",
    "print(f\"Learned Linear Calibration: y = {linear_calib_coeffs[0]:.2f}x + {linear_calib_coeffs[1]:.4f}\")\n",
    "\n",
    "\n",
    "# Train Method 2: Binned (Isotonic-like) Calibration Model\n",
    "# This model learns the average error for different levels of predicted uncertainty.\n",
    "num_bins_calib = 100\n",
    "bin_boundaries = np.quantile(calib_flat_std, np.linspace(0, 1, num_bins_calib + 1))\n",
    "binned_calib_rmse_map = np.zeros(num_bins_calib)\n",
    "\n",
    "for i in range(num_bins_calib):\n",
    "    lower = bin_boundaries[i]\n",
    "    upper = bin_boundaries[i+1]\n",
    "    in_bin = (calib_flat_std >= lower) & (calib_flat_std <= upper)\n",
    "    if np.any(in_bin):\n",
    "        binned_calib_rmse_map[i] = np.sqrt(np.mean(calib_flat_squared_error[in_bin]))\n",
    "\n",
    "# The binned calibration model is now stored in `bin_boundaries` and `binned_calib_rmse_map`\n",
    "print(\"Learned Binned Calibration map with 100 bins.\")\n",
    "\n",
    "# Clean up calibration data\n",
    "del all_calib_std, all_calib_abs_error, all_calib_squared_error\n",
    "del calib_flat_std, calib_flat_abs_error, calib_flat_squared_error\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# --- 3. Evaluate the Calibration on the Hold-Out Evaluation Set ---\n",
    "\n",
    "print(\"\\n--- Evaluating calibration on hold-out evaluation set... ---\")\n",
    "for (patient, scan) in evaluation_scans:\n",
    "    print(f\"\\n--- Results for Patient {patient}, Scan {scan} (Evaluation Set) ---\")\n",
    "    \n",
    "    # Load ground truth and reconstructions for the evaluation scan\n",
    "    gt_path = gt_paths_dict[(patient, scan)]\n",
    "    gt_recon = torch.load(gt_path).cpu().numpy()\n",
    "    \n",
    "    reconstructions = []\n",
    "    recon_paths = recon_paths_dict[(patient, scan)]\n",
    "    for recon_path in recon_paths:\n",
    "        reconstructions.append(torch.load(recon_path).cpu().numpy())\n",
    "        \n",
    "    reconstructions = np.array(reconstructions)\n",
    "    \n",
    "    # Calculate basic stats\n",
    "    mean_recon = np.mean(reconstructions, axis=0)\n",
    "    std_recon = np.std(reconstructions, axis=0)\n",
    "    error = mean_recon - gt_recon\n",
    "    \n",
    "    # Apply mask\n",
    "    z, y, x = gt_recon.shape\n",
    "    yy, xx = np.ogrid[:y, :x]\n",
    "    center = np.array([y // 2, x // 2])\n",
    "    radius = 225\n",
    "    dist_from_center = np.sqrt((yy - center[0])**2 + (xx - center[1])**2)\n",
    "    slice_mask = dist_from_center <= radius\n",
    "    mask = np.broadcast_to(slice_mask, gt_recon.shape)\n",
    "    \n",
    "    eval_flat_std = std_recon[mask].flatten()\n",
    "    eval_flat_error = error[mask].flatten()\n",
    "    eval_flat_squared_error = error[mask]**2\n",
    "\n",
    "    # --- Apply the PREVIOUSLY LEARNED calibration maps ---\n",
    "    \n",
    "    # Apply Linear map\n",
    "    calibrated_std_linear = np.polyval(linear_calib_coeffs, eval_flat_std)\n",
    "    calibrated_std_linear[calibrated_std_linear < 0] = 0\n",
    "\n",
    "    # Apply Binned map\n",
    "    # Find which bin each data point belongs to\n",
    "    bin_indices = np.searchsorted(bin_boundaries, eval_flat_std, side='right') - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, num_bins_calib - 1) # Handle edge cases\n",
    "    calibrated_std_binned = binned_calib_rmse_map[bin_indices]\n",
    "    \n",
    "    # --- Visualize and Quantify the Calibrated Results ---\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    fig.suptitle(f'POST-CALIBRATION Plots for Patient {patient}, Scan {scan} (Evaluation Set)')\n",
    "    \n",
    "    # Linear Calibration Plot\n",
    "    num_bins_plot = 20\n",
    "    bin_limits_lin = np.quantile(calibrated_std_linear, np.linspace(0, 1, num_bins_plot + 1))\n",
    "    rmse_per_bin_lin = [np.sqrt(np.mean(eval_flat_squared_error[(calibrated_std_linear >= ll) & (calibrated_std_linear < ul)])) if np.any((calibrated_std_linear >= ll) & (calibrated_std_linear < ul)) else 0 for ll, ul in zip(bin_limits_lin[:-1], bin_limits_lin[1:])]\n",
    "    mean_std_per_bin_lin = [np.mean(calibrated_std_linear[(calibrated_std_linear >= ll) & (calibrated_std_linear < ul)]) if np.any((calibrated_std_linear >= ll) & (calibrated_std_linear < ul)) else 0 for ll, ul in zip(bin_limits_lin[:-1], bin_limits_lin[1:])]\n",
    "    ax1.plot(mean_std_per_bin_lin, rmse_per_bin_lin, 'o-', label='Linear Calibration', color='green')\n",
    "    lims1 = [0, max(np.max(mean_std_per_bin_lin), np.max(rmse_per_bin_lin))]\n",
    "    ax1.plot(lims1, lims1, 'k--', label='Perfect Calibration')\n",
    "    ax1.set_title('After Linear Calibration'); ax1.set_xlabel('Avg. Pred. Std'); ax1.set_ylabel('RMSE'); ax1.legend(); ax1.grid(True, alpha=0.5); ax1.axis('equal');\n",
    "\n",
    "    # Binned Calibration Plot\n",
    "    bin_limits_binned = np.quantile(calibrated_std_binned, np.linspace(0, 1, num_bins_plot + 1))\n",
    "    rmse_per_bin_binned = [np.sqrt(np.mean(eval_flat_squared_error[(calibrated_std_binned >= ll) & (calibrated_std_binned < ul)])) if np.any((calibrated_std_binned >= ll) & (calibrated_std_binned < ul)) else 0 for ll, ul in zip(bin_limits_binned[:-1], bin_limits_binned[1:])]\n",
    "    mean_std_per_bin_binned = [np.mean(calibrated_std_binned[(calibrated_std_binned >= ll) & (calibrated_std_binned < ul)]) if np.any((calibrated_std_binned >= ll) & (calibrated_std_binned < ul)) else 0 for ll, ul in zip(bin_limits_binned[:-1], bin_limits_binned[1:])]\n",
    "    ax2.plot(mean_std_per_bin_binned, rmse_per_bin_binned, 'o-', label='Binned Calibration', color='purple')\n",
    "    lims2 = [0, max(np.max(mean_std_per_bin_binned), np.max(rmse_per_bin_binned))]\n",
    "    ax2.plot(lims2, lims2, 'k--', label='Perfect Calibration')\n",
    "    ax2.set_title('After Binned (Isotonic-like) Calibration'); ax2.set_xlabel('Avg. Pred. Std'); ax2.set_ylabel('RMSE'); ax2.legend(); ax2.grid(True, alpha=0.5); ax2.axis('equal');\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]); plt.show()\n",
    "\n",
    "    # Final Calibrated Metrics\n",
    "    calibrated_standardized_error_linear = eval_flat_error / (calibrated_std_linear + 1e-8)\n",
    "    percent_within_1_linear = np.mean(np.abs(calibrated_standardized_error_linear) < 1) * 100\n",
    "    \n",
    "    calibrated_standardized_error_binned = eval_flat_error / (calibrated_std_binned + 1e-8)\n",
    "    percent_within_1_binned = np.mean(np.abs(calibrated_standardized_error_binned) < 1) * 100\n",
    "\n",
    "    nll_linear = np.mean(0.5 * eval_flat_squared_error / (calibrated_std_linear**2 + 1e-8) + 0.5 * np.log(calibrated_std_linear**2 + 1e-8))\n",
    "    nll_binned = np.mean(0.5 * eval_flat_squared_error / (calibrated_std_binned**2 + 1e-8) + 0.5 * np.log(calibrated_std_binned**2 + 1e-8))\n",
    "\n",
    "    print(f\"  Coverage on Evaluation Set (Ideal is ~68%):\")\n",
    "    print(f\"    [Linear]   Percent of errors within 1 calibrated sigma: {percent_within_1_linear:.2f}%\")\n",
    "    print(f\"    [Binned]   Percent of errors within 1 calibrated sigma: {percent_within_1_binned:.2f}%\")\n",
    "    print(f\"  Calibrated Negative Log-Likelihood on Evaluation Set (Lower is better):\")\n",
    "    print(f\"    [Linear]   NLL: {nll_linear:.4f}\")\n",
    "    print(f\"    [Binned]   NLL: {nll_binned:.4f}\")\n",
    "    \n",
    "    del reconstructions, mean_recon, std_recon, error, gt_recon, eval_flat_std, eval_flat_error, eval_flat_squared_error\n",
    "    del calibrated_std_linear, calibrated_std_binned\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
