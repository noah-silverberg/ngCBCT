{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85cca8e",
   "metadata": {},
   "source": [
    "# 0. Imports & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0476a6",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd091fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# We set up CUDA first to ensure it is configured correctly\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "CUDA_DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.proj import load_projection_mat, reformat_sinogram, interpolate_projections, pad_and_reshape, divide_sinogram\n",
    "from pipeline.aggregate_prj import aggregate_saved_projections\n",
    "# from .aggregate_ct import aggregate_saved_volumes\n",
    "# from pipeline.apply_model import apply_model_to_projections, load_model\n",
    "# from .infer3d import inference_3d\n",
    "from pipeline.utils import ensure_dir\n",
    "import torch\n",
    "import scipy.io\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yaml\n",
    "import importlib\n",
    "\n",
    "# TODO run FDK via: FFrecon_reconFDK(input_mat, output_mat); in file \"FFrecon_fullFDK.m\"\n",
    "# TODO add some kind of logging for the hyperparameters used in the each run\n",
    "# TODO go through the training code and make sure it is consistent with new pipeline\n",
    "# TODO add input verification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d9f9e",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging mode\n",
    "DEBUG = True\n",
    "\n",
    "# Phase of the project (all data, models, etc. will be saved under this phase)\n",
    "PHASE = \"7\"\n",
    "\n",
    "# If this data version already exists in this phase, it will be loaded\n",
    "# Otherwise it will be created using whatever the most updated data creation script is\n",
    "DATA_VERSION = '12.2'\n",
    "\n",
    "# Scans to use for training, val, and testing\n",
    "SCANS = [\n",
    "    # (patient_id, scan_id, scan_type, sample)\n",
    "    # e.g., (\"13\", \"08\", \"HF\", \"TRAIN\")\n",
    "    # NOTE: scan_type can only be \"HF\" or \"FF\"\n",
    "    #       and sample can only be \"TRAIN\", \"VALIDATION\", or \"TEST\"\n",
    "    #       and the patient_id and scan_id should be strings, NOT ints\n",
    "    (\"01\", \"01\", \"HF\", \"TRAIN\"),\n",
    "    (\"02\", \"01\", \"FF\", \"TRAIN\"),\n",
    "]\n",
    "\n",
    "# List of yaml files that contain configurations for the pipeline\n",
    "# Each file should contain the paramters for a specific model/ensemble\n",
    "CONFIG_FILES = [\n",
    "    \"config.yaml\",\n",
    "]\n",
    "\n",
    "# Base directory\n",
    "WORK_ROOT = os.path.abspath(\"./TESTING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa6e84",
   "metadata": {},
   "source": [
    "### Some immediate variable definitions and setting changes based on the configuration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38798306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show info messages if DEBUG mode is enabled\n",
    "if DEBUG:\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    logging.debug(\"DEBUG mode is enabled. Detailed logs will be shown.\")\n",
    "else:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logging.info(\"DEBUG mode is disabled. Only essential logs will be shown.\")\n",
    "\n",
    "# Directories derived from bases\n",
    "PHASE_DATAVER_DIR = os.path.join(\n",
    "    WORK_ROOT, f\"phase{PHASE}\", f\"DS{DATA_VERSION}\"\n",
    ")  # everything should go inside this directory\n",
    "MODEL_DIR = os.path.join(PHASE_DATAVER_DIR, \"model\")  # for trained models\n",
    "RESULT_DIR = os.path.join(PHASE_DATAVER_DIR, \"result\")  # for outputs of CNN\n",
    "PROJ_DIR = os.path.join(\n",
    "    PHASE_DATAVER_DIR, \"proj_data\"\n",
    ")  # for input data (gated and non-stop gated projections)\n",
    "AGG_DIR = os.path.join(\n",
    "    PHASE_DATAVER_DIR, \"agg\"\n",
    ")  # for aggregated data (for PD and ID training)\n",
    "\n",
    "# Make the folders if they don't already exist\n",
    "ensure_dir(PHASE_DATAVER_DIR)\n",
    "ensure_dir(MODEL_DIR)\n",
    "ensure_dir(RESULT_DIR)\n",
    "ensure_dir(PROJ_DIR)\n",
    "ensure_dir(AGG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b49a2c",
   "metadata": {},
   "source": [
    "# 1. Data Preparation: projection interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccc147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only process the data if it doesn't already exist\n",
    "if len(os.listdir(PROJ_DIR)) > 0:\n",
    "    logging.info(f\"Projection data for phase {PHASE} data version {DATA_VERSION} already exists in {PROJ_DIR}. Skipping processing.\")\n",
    "else:\n",
    "    logging.info(\"Starting to process projection data...\")\n",
    "    for patient, scan, scan_type, sample in SCANS:\n",
    "        # Load the projection data from the matlab files\n",
    "        odd_index, angles, prj = load_projection_mat(patient, scan, scan_type, WORK_ROOT)\n",
    "\n",
    "        # Log shapes of loaded data\n",
    "        logging.debug(f'Processing patient {patient}, scan {scan}, type {scan_type}, sample {sample}')\n",
    "        logging.debug(f'Loaded odd_index shape: {odd_index.shape}')\n",
    "        logging.debug(f'Loaded angles shape: {angles.shape}')\n",
    "        logging.debug(f'Loaded projection shape: {prj.shape}')\n",
    "\n",
    "        # Flip and permute to get it in the right format\n",
    "        prj_gcbct, angles1 = reformat_sinogram(prj, angles)\n",
    "\n",
    "        # Log shapes after reformatting\n",
    "        logging.debug(f'Reformatted projection shape: {prj_gcbct.shape}')\n",
    "\n",
    "        # Simulate ngCBCT projections\n",
    "        prj_ngcbct_li = interpolate_projections(prj_gcbct, odd_index)\n",
    "\n",
    "        # Log shapes after interpolation\n",
    "        logging.debug(f'Interpolated ngCBCT projection shape: {prj_ngcbct_li.shape}')\n",
    "\n",
    "        # Split the projections into two halves so they are good dimensions for the CNN\n",
    "        combined_gcbct = divide_sinogram(pad_and_reshape(prj_gcbct), v_dim=512 if scan_type == \"HF\" else 256)\n",
    "        combined_ngcbct = divide_sinogram(pad_and_reshape(prj_ngcbct_li), v_dim=512 if scan_type == \"HF\" else 256)\n",
    "\n",
    "        # Log shapes after dividing sinograms\n",
    "        logging.debug(f'Combined gCBCT shape: {combined_gcbct.shape}')\n",
    "        logging.debug(f'Combined ngCBCT shape: {combined_ngcbct.shape}')\n",
    "\n",
    "        # Ensure the output directories exist\n",
    "        g_dir = os.path.join(PROJ_DIR, 'gated')\n",
    "        ng_dir = os.path.join(PROJ_DIR, 'ng')\n",
    "        ensure_dir(g_dir)\n",
    "        ensure_dir(ng_dir)\n",
    "\n",
    "        logging.debug(f'Saving projections...')\n",
    "        \n",
    "        # Save the projections\n",
    "        # NOTE: These need to have the same name since later we will aggregate them, and we just sort by the name\n",
    "        torch.save(combined_gcbct, os.path.join(g_dir, f'{scan_type}_p{patient}_{scan}_{sample}.pt')) # e.g., HF_p01_01_TRAIN.pt\n",
    "        torch.save(combined_ngcbct, os.path.join(ng_dir, f'{scan_type}_p{patient}_{scan}_{sample}.pt'))\n",
    "\n",
    "        logging.debug(f'Done with patient {patient}, scan {scan}, type {scan_type}, sample {sample}\\n')\n",
    "\n",
    "    logging.info(\"All projections saved successfully.\")\n",
    "    logging.info(\"Gated projections saved in: %s\", g_dir)\n",
    "    logging.info(\"Non-gated projections saved in: %s\", ng_dir)\n",
    "\n",
    "    # Free up memory\n",
    "    del odd_index, angles, prj, prj_gcbct, angles1, prj_ngcbct_li, combined_gcbct, combined_ngcbct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23247d",
   "metadata": {},
   "source": [
    "### DEBUG: Sample projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b21297",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # Pick the first HF scan and first FF scan\n",
    "    for patient, scan, scan_type, sample in SCANS:\n",
    "        if scan_type == \"HF\":\n",
    "            hf_scan = (patient, scan, scan_type, sample)\n",
    "            break\n",
    "    for patient, scan, scan_type, sample in SCANS:\n",
    "        if scan_type == \"FF\":\n",
    "            ff_scan = (patient, scan, scan_type, sample)\n",
    "            break\n",
    "\n",
    "    g_dir = os.path.join(PROJ_DIR, 'gated')\n",
    "    ng_dir = os.path.join(PROJ_DIR, 'ng')\n",
    "\n",
    "    # Display the first HF scan\n",
    "    # Show the gated and nonstop-gated on subplots\n",
    "    hf_patient, hf_scan_num, hf_scan_type, hf_sample = hf_scan\n",
    "    hf_gated_prj = torch.load(os.path.join(g_dir, f'{hf_scan_type}_p{hf_patient}_{hf_scan_num}_{hf_sample}.pt'))\n",
    "    hf_ng_prj = torch.load(os.path.join(ng_dir, f'{hf_scan_type}_p{hf_patient}_{hf_scan_num}_{hf_sample}.pt'))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(hf_gated_prj[0, 0, :, :].cpu().numpy(), cmap='gray')\n",
    "    plt.title(f'Gated Projection - {hf_scan_type} p{hf_patient}_{hf_scan_num}_{hf_sample}')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(hf_ng_prj[0, 0, :, :].cpu().numpy(), cmap='gray')\n",
    "    plt.title(f'Nonstop-Gated Projection - {hf_scan_type} p{hf_patient}_{hf_scan_num}_{hf_sample}')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Repeat for FF scan\n",
    "    ff_patient, ff_scan_num, ff_scan_type, ff_sample = ff_scan\n",
    "    ff_gated_prj = torch.load(os.path.join(g_dir, f'{ff_scan_type}_p{ff_patient}_{ff_scan_num}_{ff_sample}.pt'))\n",
    "    ff_ng_prj = torch.load(os.path.join(ng_dir, f'{ff_scan_type}_p{ff_patient}_{ff_scan_num}_{ff_sample}.pt'))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(ff_gated_prj[0, 0, :, :].cpu().numpy(), cmap='gray')\n",
    "    plt.title(f'Gated Projection - {ff_scan_type} p{ff_patient}_{ff_scan_num}_{ff_sample}')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(ff_ng_prj[0, 0, :, :].cpu().numpy(), cmap='gray')\n",
    "    plt.title(f'Nonstop-Gated Projection - {ff_scan_type} p{ff_patient}_{ff_scan_num}_{ff_sample}')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Free up memory\n",
    "    del hf_gated_prj, hf_ng_prj, ff_gated_prj, ff_ng_prj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e695f",
   "metadata": {},
   "source": [
    "# 2. Aggregate projections for train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d3403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only aggregate projections if they don't already exist\n",
    "if len([f for f in os.listdir(AGG_DIR) if f.startswith(\"PROJ\")]) > 0:\n",
    "    logging.info(f\"Aggregated projection data for phase {PHASE} data version {DATA_VERSION} already exists in {AGG_DIR}. Skipping aggregation.\")\n",
    "else:\n",
    "    logging.info(\"Starting to aggregate projection data...\")\n",
    "    # Aggregate and save projection data sets\n",
    "    for scan_type in ['HF', 'FF']:\n",
    "        for sample in ['TRAIN', 'VALIDATION', 'TEST']:\n",
    "            prj_gcbct, prj_ngcbct = aggregate_saved_projections(scan_type, sample, PROJ_DIR)\n",
    "            np.save(os.path.join(AGG_DIR, f\"PROJ_gated_{scan_type}_{sample}.npy\"), prj_gcbct.cpu().numpy()) # e.g., PROJ_gated_HF_TRAIN.npy\n",
    "            np.save(os.path.join(AGG_DIR, f\"PROJ_ng_{scan_type}_{sample}.npy\"), prj_ngcbct.cpu().numpy())\n",
    "\n",
    "    # Free up memory\n",
    "    del prj_gcbct, prj_ngcbct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae3e79",
   "metadata": {},
   "source": [
    "# 3. Training PD CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daead9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for config_file in CONFIG_FILES:\n",
    "    # Load the yaml configuration file\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    logging.debug(f\"Loaded configuration from {config_file}: {config}\")\n",
    "\n",
    "    # Skip this config if the user has set PD_training to False\n",
    "    if not config['PD_settings']['training']:\n",
    "        logging.info(f\"Skipping PD training for {config_file} as PD training is set to False.\")\n",
    "        continue\n",
    "\n",
    "    # Get the training application\n",
    "    module_name, class_name = config['training_app'].rsplit('.', 1)\n",
    "    module = importlib.import_module(\"pipeline.\" + module_name)\n",
    "    cls = getattr(module, class_name)\n",
    "\n",
    "    logging.debug(f\"Loaded class {class_name} from module {module_name}\")\n",
    "\n",
    "    # Get the ensemble size, and loop through it\n",
    "    ensemble_size = config['PD_settings']['ensemble_size']\n",
    "    for i in range(ensemble_size):\n",
    "        # If we are training an ensemble, we add an identifier to the model version\n",
    "        if ensemble_size > 1:\n",
    "            config['PD_settings']['model_version'] += f\"_{i + 1:02}\"  # e.g., \"v1_01\"\n",
    "\n",
    "        # Instantiate with the loaded configuration\n",
    "        instance = cls(config, \"PROJ\", DEBUG, MODEL_DIR, AGG_DIR)\n",
    "\n",
    "        logging.info(f\"Starting training for {i + 1}-th model with configuration: {config_file}\")\n",
    "\n",
    "        # Run the training\n",
    "        instance.main()\n",
    "\n",
    "    # Free up memory\n",
    "    del module, cls, instance, config, module_name, class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef2c2c",
   "metadata": {},
   "source": [
    "# 4. Apply PD model to all nonstop-gated sinograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28452019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained PD model onto the GPU\n",
    "PD_model = load_model(PD_network_name, PD_model_name, device=torch.device(CUDA_DEVICE))\n",
    "\n",
    "for patient, scan, scan_type, sample in SCANS:\n",
    "    # Get the matlab dicts for the ground truth and CNN projections\n",
    "    g_mat, cnn_mat = apply_model_to_projections(patient, scan, scan_type, sample, PD_model)\n",
    "\n",
    "    # Save the ground truth and CNN projections\n",
    "    scipy.io.savemat(os.path.join(RESULT_DIR, f'PROJ_gated_{scan_type}_p{patient}_{scan}_{sample}.mat'), g_mat) # e.g., PROJ_gated_HF_p01_01_TRAIN.mat\n",
    "    scipy.io.savemat(os.path.join(RESULT_DIR, f'PROJ_ng_{scan_type}_p{patient}_{scan}_{sample}.mat'), cnn_mat)\n",
    "\n",
    "# Free up memory\n",
    "del PD_model, g_mat, cnn_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feaa971",
   "metadata": {},
   "source": [
    "# 5. TODO: FDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4029008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09b672",
   "metadata": {},
   "source": [
    "# 6. Aggregate CT volumes for train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for aggregated data saving\n",
    "vol_agg_dir = os.path.join(DATA_DIR, \"agg\", \"volumes\")\n",
    "ensure_dir(vol_agg_dir)\n",
    "\n",
    "# Aggregate and save volume data sets\n",
    "for scan_type in ['HF', 'FF']:\n",
    "    for sample in ['train', 'validation', 'test']:\n",
    "        vol_gcbct, vol_ngcbct = aggregate_saved_volumes(scan_type, sample)\n",
    "        torch.save(vol_gcbct, os.path.join(vol_agg_dir, f\"{scan_type}_{sample}_gated.pt\"))\n",
    "        torch.save(vol_ngcbct, os.path.join(vol_agg_dir, f\"{scan_type}_{sample}_ng.pt\"))\n",
    "\n",
    "# Free up memory\n",
    "del vol_gcbct, vol_ngcbct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e933115",
   "metadata": {},
   "source": [
    "# 7. Train ID CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b35e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f74da1",
   "metadata": {},
   "source": [
    "# 8. Inference on test scans for full 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = inference_3d(patient_id, scan_id, 'HF', data_version, model_name, 'tumor_location_panc.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
